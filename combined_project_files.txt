Project File Combination
Generated on: 2024-10-09 14:02:29.652217

Total files: 38
File type counts:
  .py: 11
  .md: 1
  .html: 9
  .css: 1
  .js: 1

File Structure:
../
    requirements.txt
    .env
    .gitignore
    docker-compose.yml
    combined_project_files.txt
    .dockerignore
    entrypoint.sh
    Dockerfile
    mongo-init/
    app/
        database_setup.py
        start_docker.sh
        database_processing.log
        app.py
        readme.md
        chunk_utils.py
        requirements.txt
        generate_password.py
        secret_key.txt
        config.json
        models.py
        json_validator.py
        data_processing.py
        .env
        routes.log
        generate_unique_terms.log
        generate_unique_terms.py
        routes.py
        test_mongo_connection.py
        json_validator_multi.py
        templates/
            index.html
            search-terms.html
            settings.html
            error.html
            login.html
            base.html
            database-info.html
            document-detail.html
            document-list.html
        static/
            style.css
            script.js

********************************************************************************

File: app/database_setup.py
********************************************************************************

# database_setup.py

from pymongo import MongoClient, ASCENDING, DESCENDING, UpdateOne
from bson import ObjectId
import logging
import os
from dotenv import load_dotenv
import pymongo
from pymongo import UpdateOne

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
# Create a logger
logger = logging.getLogger('DatabaseSetupLogger')
logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)  # Show only warnings and above in console

file_handler = logging.FileHandler('database_setup.log')
file_handler.setLevel(logging.DEBUG)  # Capture all debug and higher level logs in file

# Create formatter
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# Set formatter for handlers
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# Add handlers to the logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)

# =======================
# Database Functions
# =======================

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        print("this is the main sequence")
        mongo_uri = os.environ.get('MONGO_URI') # this SHOULD read from .env file but sometimes does not.
        #mongo_uri= "mongodb://admin:secret@mongodb:27017/admin" # this is correct and needs the /admin for the administrative db. REMOVE before prduction
        print(mongo_uri)

        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=1000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']


def clean_unique_terms_collection(db):
    """Remove documents with null term, field, or type."""
    result = db['unique_terms'].delete_many({
        "$or": [
            {"term": None},
            {"field": None},
            {"type": None}
        ]
    })
    logger.info(f"Removed {result.deleted_count} documents with null term, field, or type.")



def initialize_database(client):
    db = get_db(client)
    # Create collections if they don't exist
    for collection_name in ['documents', 'unique_terms', 'field_structure']:
        if collection_name not in db.list_collection_names():
            db.create_collection(collection_name)
            logger.info(f"Created collection: {collection_name}")
    
  
    # Clean unique_terms collection
    clean_unique_terms_collection(db)


    # Create necessary indexes
    documents = db['documents']
    existing_indexes = documents.index_information()
    if 'file_hash_1' not in existing_indexes:
        documents.create_index([("file_hash", 1)], unique=True)
        logger.info("Created unique index on 'file_hash' in 'documents' collection.")
    
    unique_terms = db['unique_terms']
    # Unique compound index on term, field, and type
    unique_terms.create_index(
        [("term", ASCENDING), ("field", ASCENDING), ("type", ASCENDING)],
        unique=True,
        name="unique_term_field_type"
    )
    logger.info("Created unique compound index on 'term', 'field', and 'type' in 'unique_terms' collection.")
    
    # Compound index on field, type, and frequency for efficient sorting
    unique_terms.create_index(
        [("field", ASCENDING), ("type", ASCENDING), ("frequency", DESCENDING)],
        name="field_type_frequency_idx"
    )
    logger.info("Created compound index on 'field', 'type', and 'frequency' in 'unique_terms' collection.")
    
    field_structure = db['field_structure']
    field_structure.create_index([("field", 1)], unique=True)
    logger.info("Created unique index on 'field' in 'field_structure' collection.")
    
    logger.info("Database initialized with required collections and indexes.")


def insert_document(db, document):
    """Insert a document into the 'documents' collection."""
    try:
        documents = db['documents']
        documents.insert_one(document)
        logger.info("Inserted document into 'documents' collection.")
    except Exception as e:
        logger.error(f"Error inserting document: {e}")
        raise e
    
    
def discover_fields(document):
    """
    Recursively discover fields in a document.
    :param document: The document to analyze
    :return: A dictionary representing the field structure
    """
    structure = {}
    for key, value in document.items():
        if isinstance(value, dict):
            structure[key] = discover_fields(value)
        elif isinstance(value, list):
            if value:
                if isinstance(value[0], dict):
                    structure[key] = [discover_fields(value[0])]
                else:
                    structure[key] = [type(value[0]).__name__]
            else:
                structure[key] = []
        else:
            structure[key] = type(value).__name__
    return structure

def merge_structures(existing, new):
    """
    Merge two field structures.
    :param existing: The existing field structure
    :param new: The new field structure to merge
    :return: The merged field structure
    """
    for key, value in new.items():
        if key not in existing:
            existing[key] = value
        elif isinstance(value, dict) and isinstance(existing[key], dict):
            merge_structures(existing[key], value)
        elif isinstance(value, list) and isinstance(existing[key], list):
            if value and existing[key]:
                if isinstance(value[0], dict) and isinstance(existing[key][0], dict):
                    merge_structures(existing[key][0], value[0])
    return existing

def update_field_structure(db, document):
    """
    Update the field structure based on a new document.
    :param db: Database instance
    :param document: The new document to analyze
    """
    field_structure_collection = db['field_structure']
    new_structure = discover_fields(document)
    merged_structure = {}

    # Attempt to retrieve the existing structure
    existing_structure = field_structure_collection.find_one({"_id": "current_structure"})

    if existing_structure:
        # Merge the new structure with the existing one
        merged_structure = merge_structures(existing_structure['structure'], new_structure)
    else:
        # If no existing structure, use the new structure
        merged_structure = new_structure

    # Perform an upsert operation to update or insert the structure
    field_structure_collection.update_one(
        {"_id": "current_structure"},
        {"$set": {"structure": merged_structure}},
        upsert=True
    )


def is_file_ingested(db, file_hash):
    """Check if a file has already been ingested based on its hash."""
    if not file_hash:
        return False
    try:
        documents = db['documents']
        ingested = documents.find_one({'file_hash': file_hash}) is not None
        logger.debug(f"File ingestion check for hash {file_hash}: {ingested}")
        return ingested
    except Exception as e:
        logger.error(f"Error checking ingestion status for hash {file_hash}: {e}")
        return False
def save_unique_terms(db, unique_terms_counter):
    """
    Save the unique terms counter to the database as individual documents.
    :param db: Database instance
    :param unique_terms_counter: Nested dictionary {field: {type: Counter()}}
    """
    unique_terms_collection = db['unique_terms']
    
    # Log the unique_terms_counter content
    logger.debug(f"Unique terms counter before saving: {unique_terms_counter}")
    
    # Prepare bulk operations
    operations = []
    for field, types in unique_terms_counter.items():
        if not field:
            logger.warning("Encountered a null or empty field. Skipping.")
            continue
        for term_type, counter in types.items():
            if not term_type:
                logger.warning("Encountered a null or empty type. Skipping.")
                continue
            for term, freq in counter.items():
                if not term:
                    logger.warning("Encountered a null or empty term. Skipping.")
                    continue
                operations.append(
                    pymongo.UpdateOne(
                        {"term": term, "field": field, "type": term_type},
                        {"$set": {"frequency": freq}},
                        upsert=True
                    )
                )
    
    logger.debug(f"Prepared {len(operations)} bulk operations for unique_terms.")
    
    if operations:
        try:
            result = unique_terms_collection.bulk_write(operations, ordered=False)
            logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} unique terms.")
        except Exception as e:
            logger.error(f"Error bulk upserting unique terms: {e}")
            raise e
    else:
        logger.warning("No unique terms to upsert.")






def update_document(db, document_id, update_data):
    """
    Update a document's information.
    :param db: Database instance
    :param document_id: The ObjectId of the document to update
    :param update_data: A dictionary containing the fields to update
    :return: The number of documents modified
    """
    try:
        documents = db['documents']
        result = documents.update_one({"_id": ObjectId(document_id)}, {"$set": update_data})
        if result.matched_count == 0:
            logger.warning(f"No document found with _id: {document_id}")
        else:
            logger.info(f"Updated document with _id: {document_id}")
        return result.modified_count
    except Exception as e:
        logger.error(f"Error updating document {document_id}: {e}")
        raise e

def delete_document(db, document_id):
    """
    Delete a document from the database.
    :param db: Database instance
    :param document_id: The ObjectId of the document to delete
    :return: The number of documents deleted
    """
    try:
        documents = db['documents']
        result = documents.delete_one({"_id": ObjectId(document_id)})
        if result.deleted_count == 0:
            logger.warning(f"No document found with _id: {document_id}")
        else:
            logger.info(f"Deleted document with _id: {document_id}")
        return result.deleted_count
    except Exception as e:
        logger.error(f"Error deleting document {document_id}: {e}")
        raise e
    
def get_field_structure(db):
    """
    Get the current field structure.
    :param db: Database instance
    :return: The current field structure
    """
    field_structure_collection = db['field_structure']
    structure = field_structure_collection.find_one({"_id": "current_structure"})
    return structure['structure'] if structure else {}


def find_document_by_id(db, document_id):
    """
    Find a document by its ObjectId.
    :param db: Database instance
    :param document_id: The ObjectId of the document
    :return: The document, or None if not found
    """
    try:
        documents = db['documents']
        document = documents.find_one({"_id": ObjectId(document_id)})
        if document:
            logger.info(f"Found document with _id: {document_id}")
            return document
        else:
            logger.warning(f"No document found with _id: {document_id}")
            return None
    except Exception as e:
        logger.error(f"Error finding document {document_id}: {e}")
        raise e

def get_collections(db):
    """Retrieve and return references to the required collections."""
    try:
        documents = db['documents']
        unique_terms_collection = db['unique_terms']
        field_structure_collection = db['field_structure']
        return documents, unique_terms_collection, field_structure_collection
    except Exception as e:
        logger.error(f"Error getting collections: {e}")
        raise

# =======================
# Main Execution (Optional)
# =======================
if __name__ == "__main__":
    client = get_client()  # Get the MongoDB client
    initialize_database(client)  # Initialize the database
    logger.info("Database setup module executed directly.")


********************************************************************************

File: app/app.py
********************************************************************************

# File: app.py
# Path: railroad_documents_project/app.py

import os
import json
from flask import Flask
from flask_caching import Cache
from flask_session import Session
from database_setup import (
    get_client,
    get_db,
    get_collections,
    insert_document,
    update_document,
    delete_document,
    get_field_structure,
    find_document_by_id,
    update_field_structure,
    save_unique_terms
)


import logging
from logging.handlers import RotatingFileHandler

app = Flask(__name__)



# Setup console logging
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# Add the console handler to the app's logger
app.logger.addHandler(console_handler)

# # Setup file-based logging
# if not app.debug:
#     file_handler = RotatingFileHandler('logs/app.log', maxBytes=10240, backupCount=10)
#     file_handler.setLevel(logging.DEBUG)
#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#     file_handler.setFormatter(formatter)
#     app.logger.addHandler(file_handler)

app.logger.setLevel(logging.DEBUG)

# Load configuration
config_path = os.path.join(os.path.dirname(__file__), 'config.json')

try:
    with open(config_path) as config_file:
        config = json.load(config_file)
    app.config['UI_CONFIG'] = config
except FileNotFoundError:
    app.logger.error(f"Configuration file not found at {config_path}.")
    config = {}
except json.JSONDecodeError as e:
    app.logger.error(f"Error decoding JSON from {config_path}: {e}")
    config = {}


# Add config to app config
app.config['UI_CONFIG'] = config

# Print out the template folder path for debugging
print(f"Template folder path: {app.template_folder}")

# Session configuration
app.config['SESSION_TYPE'] = 'filesystem'
app.config['SESSION_PERMANENT'] = False
app.config['SESSION_USE_SIGNER'] = True
app.config['SESSION_KEY_PREFIX'] = 'historical_document_reader'

def get_secret_key():
    secret_file = os.path.join(app.root_path, 'secret_key.txt')
    if os.path.exists(secret_file):
        with open(secret_file, 'r') as f:
            return f.read().strip()
    else:
        import secrets
        generated_key = secrets.token_hex(16)
        with open(secret_file, 'w') as f:
            f.write(generated_key)
        return generated_key

# Set the secret key
app.secret_key = get_secret_key()

# Initialize extensions
cache = Cache(config={'CACHE_TYPE': 'simple'})
cache.init_app(app)
Session(app)

def load_config():
    """
    Load the configuration from the JSON file.
    This function is called on each request to allow for dynamic UI configuration.
    """
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')
    with open(config_path) as config_file:
        return json.load(config_file)

@app.context_processor
def inject_ui_config():
    """
    Inject the UI configuration and field structure into all templates.
    This allows for dynamic UI customization without needing to pass the config to each template.
    """
    app.config['UI_CONFIG'] = load_config()
    client = get_client()
    db = get_db(client)
    field_struct = get_field_structure(db)
    return dict(ui_config=app.config['UI_CONFIG'], field_structure=field_struct)

# Import routes after initializing app to avoid circular imports
from routes import *

if __name__ == '__main__':
    debug_mode = os.environ.get('FLASK_DEBUG', '0') == '1'
    app.run(debug=debug_mode, host='0.0.0.0', port=5000)


********************************************************************************

File: app/readme.md
********************************************************************************

# Historical Document Reader

## Description
The Historical Document Reader is a Flask-based web application designed to manage, search, and display historical documents stored in a MongoDB database. It provides an intuitive interface for researchers and historians to access and analyze digitized historical records.

## Repository
The project is hosted on GitHub at:
https://github.com/proflouishyman/nosql_reader

## Features

- Document Management: Data ingestion from JSON files, dynamic field structure discovery
- Search Functionality: Advanced search with multiple fields and logical operators
- Document Viewing: Detailed view with image zoom and pan capabilities
- Data Analysis: Search terms analysis with word and phrase frequency
- User Interface: Customizable UI settings, responsive design
- Data Export: Export search results to CSV
- Security: Basic authentication system with CAPTCHA

## To Do

to access the shell
docker compose exec -it flask_app /bin/bash


for mongodb
docker compose exec -it mongodb /bin/bash
mongosh mongodb://admin:secret@localhost:27017/admin











0. Shift other PC to Docker
0.1. Create flag for list of hidden fields
1. Convert setup process to a part of settings or a new page. It should be able to add to the DB
2. Create login splash page
3. Need to adjust unique search terms to move from a pickle file to part of the MongoDB
4. Address weirdness of base file and index.html. It is unseemly 
5. Reorganize routes.py
6. Add cross-referencing of named entities
7. Restore adding export from list
8. Add "Select all" for export
9. Clean file description to remove extension in title
10. Create a way to do a search and then add that result to the DB
11. Backup and restore database
12. Remove blank fields from JSON expansion
13. Color code sections of JSON expansion
14. Need to implement a sort for the file results, so that this is in order: 	File	Summary
	RDApp-630550Fox053.jpg.json	The document contains a handwritten signature of an individual named M. Johnson, along with the year 1919.
	RDApp-630550Fox072.jpg.json	This document is a surgeon's first report of an accident for the Baltimore & Ohio Railroad-Relief Department. It details an incident involving an individual named E.S. Fry, a laborer, who resides in All Around O. The report notes injuries sustained: a contusion and a cut lip, with the mention of a broken face due to a tool. The probable duration of disablement is stated to be short. Additionally, it provides a brief account of how the accident occurred, indicating involvement with a train.
	RDApp-630550Fox059.jpg.json	The document appears to be a handwritten note addressed to Mr. Martin from someone requesting approval from Dr. Smith, a company surgeon, regarding a matter likely related to medical or health concerns.
	RDApp-630550Fox014.jpg.json	This document is a correspondence from the Office of General Claim Agent of The Baltimore and Ohio Railroad Company, dated December 8, 1920. It refers to a bill from The Peoples Hospital for services rendered to E. L. Fox, a train rider who was injured at Cuyahoga Falls, Ohio, on October 31, 1920. The bill is being sent to Mr. W. J. Dudley, Superintendent of the Relief Department, for voucher processing. The document indicates that Mr. Fox was a member of the Relief Department at the time of his injury. Additionally, there is a note referencing a letter related to this bill dated 16th of December, 1920.
	RDApp-630550Fox062.jpg.json	This document is a telegram from the Baltimore and Ohio Railroad Company to the Superintendent of City Hospital in Akron, Ohio, dated August 5th, 1921. It refers to a bill concerning an individual named E. L. Fox and requests further communication regarding the matter.


##Notes on how to
1. Inside the util folder is delete_db.py which needs to be run from inside a container in order to delete the database. You will need to delete the database if you change the setup or structure.
   a. docker exec -it flask_app /bin/bash
   b. util/delete_db.py


##NOTES

Tried to optimize the search terms but something is off. i need to look and see what is actually in teh records to make sure they are being put together correctly. i should look back at old code

Also, need to fix images. soemthing is wrong where the pages arent printing >Image not found: {{ image_path }}</p  if it is not found. 

## File Structure

railroad_documents_project/
├── app/
│   ├── app.py
│   ├── requirements.txt
│   ├── routes.py
│   ├── config.json
│   ├── secret_key.txt (optional if using env variables)
│   └── ... (other application files)
├── mongo-init/
│   └── init_script.js (if any initialization scripts)
├── entrypoint.sh
├── Dockerfile
├── docker-compose.yml
├── .env
├── .dockerignore
└── README.md


```
historical_document_reader/
│
├── app.py
├── routes.py
├── models.py
├── database_setup.py
├── data_processing.py
├── json_validator.py
├── json_validator_multi.py
├── generate_password.py
├── requirements.txt
├── config.json
├── secret_key.txt
├── README.md
│
├── static/
│   ├── script.js
│   └── style.css
│
├── templates/
│   ├── base.html
│   ├── index.html
│   ├── document-detail.html
│   ├── document-list.html
│   ├── search-terms.html
│   ├── database-info.html
│   ├── settings.html
│   ├── login.html
│   └── error.html
│
└── archives/
    └── [Your archival files go here]
```

## Installation and Setup

1. Clone the repository:
   ```
   git clone https://github.com/proflouishyman/nosql_reader.git
   ```

2. Navigate to the project directory and install dependencies:
   ```
   cd nosql_reader
   pip install -r requirements.txt
   ```

3. Set up MongoDB:
   - Install MongoDB if you haven't already.
   - Start the MongoDB service.
   - Update the connection string in `database_setup.py` if necessary:
     ```python
     client = MongoClient('mongodb://localhost:27017/')
     ```

4. Initialize the database structure:
   ```
   python database_setup.py
   ```

5. Prepare your JSON data:
   - Ensure your historical document data is in JSON format.
   - Place all archival files in the `archives` subdirectory.
   - If your data is in .txt files, use the JSON validator to convert and validate them:
     ```
     python json_validator.py
     ```

6. Ingest data into the database:
   - Update the `data_directory` path in `data_processing.py` to point to your JSON files in the `archives` subdirectory.
   - Run the data processing script:
     ```
     python data_processing.py
     ```

7. Generate a password for admin access (optional):
   ```
   python generate_password.py
   ```

8. Run the application:
   ```
   python app.py
   ```

## Usage

1. Access the application at `http://localhost:5000`.
2. Log in using the generated admin password.
3. Use the search interface to find documents.
4. View search results and document details.
5. Analyze search terms and view database information.
6. Customize the interface in the Settings page.
7. Export results to CSV as needed.

## Docker Setup

Run `run_docker.sh` to install and set up using Docker. If there are problems, install `dos2unix` and then convert the file. Everything runs better in WSL.

MongoDB connection string for Docker setup:
```python
client = MongoClient('mongodb://admin:secret@localhost:27017/')
```

## Maintenance and Updates

- Add new documents by placing JSON files in the `archives` subdirectory and running `data_processing.py`.
- The system automatically adapts to changes in document structure.
- Regularly backup your MongoDB database.

## Troubleshooting

- Verify JSON format for data processing issues.
- Check MongoDB service and connection string for database issues.
- Consult application logs for error messages.

## Main Python Files

- `app.py`: Main application file, initializes Flask app
- `routes.py`: Contains all route handlers and main functionality
- `database_setup.py`: Manages MongoDB connection and CRUD operations
- `data_processing.py`: Handles data ingestion and processing
- `json_validator.py` and `json_validator_multi.py`: Validate and clean JSON files
- `generate_password.py`: Utility for generating admin password

## Contributing

1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Make changes and commit with descriptive messages.
4. Push changes to your fork.
5. Submit a pull request to the main repository.

## License

[Insert your chosen license here]

## Contact

[Your Name or Organization]
[Contact Information]

********************************************************************************

File: app/chunk_utils.py
********************************************************************************

# chunk_utils.py

from pymongo import MongoClient
import logging

# Setup logger
logger = logging.getLogger('ChunkUtilsLogger')

def save_unique_terms(db, unique_terms_dict, max_chunk_size=1500000):
    """
    Save the unique terms dictionary to the database as multiple documents if it exceeds max size.
    :param db: Database instance
    :param unique_terms_dict: The dictionary containing unique terms
    :param max_chunk_size: The maximum size for each chunk in bytes
    """
    unique_terms_collection = db['unique_terms']
    
    current_chunk = {}
    current_size = 0
    chunk_index = 0

    for key, value in unique_terms_dict.items():
        new_size = len(key) + len(str(value))
        if current_size + new_size > max_chunk_size:
            unique_terms_collection.replace_one(
                {"_id": f"unique_terms_chunk_{chunk_index}"},
                {"terms": current_chunk},
                upsert=True
            )
            chunk_index += 1
            current_chunk = {}
            current_size = 0
        
        current_chunk[key] = value
        current_size += new_size

    if current_chunk:
        unique_terms_collection.replace_one(
            {"_id": f"unique_terms_chunk_{chunk_index}"},
            {"terms": current_chunk},
            upsert=True
        )

    logger.info("Unique terms saved to the database in chunks.")

def retrieve_unique_terms(db):
    """
    Retrieve all unique terms from the database, merging chunks into a single dictionary.
    :param db: Database instance
    :return: Merged unique terms dictionary
    """
    unique_terms_collection = db['unique_terms']
    terms = {}
    
    # Retrieve all unique term chunks
    for doc in unique_terms_collection.find():
        terms.update(doc.get('terms', {}))

    logger.info("Unique terms retrieved from the database.")
    return terms


********************************************************************************

File: app/generate_password.py
********************************************************************************

from werkzeug.security import generate_password_hash

actual_password = 'loulou'
method = 'pbkdf2:sha256:260000'

password_hash = generate_password_hash(actual_password, method=method)
print(password_hash)



********************************************************************************

File: app/models.py
********************************************************************************

# No longer necessary since it is NoSQL db. 

# from app import db, ma

# class OCRText(db.Model):
#     __tablename__ = 'ocr_text'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, nullable=False)
#     text = db.Column(db.Text, nullable=False)

#     summary = db.relationship('Summary', back_populates='ocr_text', uselist=False)
#     named_entities = db.relationship('NamedEntity', back_populates='ocr_text')
#     dates = db.relationship('Date', back_populates='ocr_text')
#     monetary_amounts = db.relationship('MonetaryAmount', back_populates='ocr_text')
#     relationships = db.relationship('Relationship', back_populates='ocr_text')
#     document_metadata = db.relationship('DocumentMetadata', back_populates='ocr_text', uselist=False)
#     translation = db.relationship('Translation', back_populates='ocr_text', uselist=False)
#     file_info = db.relationship('FileInfo', back_populates='ocr_text', uselist=False)

# class Summary(db.Model):
#     __tablename__ = 'summary'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     text = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='summary')

# class NamedEntity(db.Model):
#     __tablename__ = 'named_entities'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     entity = db.Column(db.Text, nullable=False)
#     type = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='named_entities')

# class Date(db.Model):
#     __tablename__ = 'dates'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     date = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='dates')

# class MonetaryAmount(db.Model):
#     __tablename__ = 'monetary_amounts'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     amount = db.Column(db.Text, nullable=False)
#     category = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='monetary_amounts')

# class Relationship(db.Model):
#     __tablename__ = 'relationships'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     entity1 = db.Column(db.Text, nullable=False)
#     relationship = db.Column(db.Text, nullable=False)
#     entity2 = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='relationships')

# class DocumentMetadata(db.Model):
#     __tablename__ = 'metadata'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     document_type = db.Column(db.Text, nullable=False)
#     period = db.Column(db.Text, nullable=False)
#     context = db.Column(db.Text, nullable=False)
#     sentiment = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='document_metadata')

# class Translation(db.Model):
#     __tablename__ = 'translation'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     french_text = db.Column(db.Text, nullable=False)
#     english_translation = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='translation')

# class FileInfo(db.Model):
#     __tablename__ = 'file_info'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     original_filepath = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='file_info')

# # Add Marshmallow schemas if needed
# class OCRTextSchema(ma.SQLAlchemyAutoSchema):
#     class Meta:
#         model = OCRText


********************************************************************************

File: app/json_validator.py
********************************************************************************

import os
import json
import re
import multiprocessing
from tqdm import tqdm

def clean_json(json_text):
    # Remove all control characters
    json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

    # Find the index of the first '{' and the last '}'
    start_index = json_text.find('{')
    end_index = json_text.rfind('}')

    # Extract the clean JSON string
    if start_index != -1 and end_index != -1:
        clean_json_text = json_text[start_index:end_index + 1]
        return clean_json_text
    else:
        raise ValueError("Invalid JSON format: Unable to find '{' or '}'.")

def validate_json_file(file_path):
    filename = os.path.basename(file_path)
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            json_content = file.read()

        # Clean the JSON content before validation
        cleaned_json_content = clean_json(json_content)
        cleaned = cleaned_json_content != json_content

        # Validate the cleaned JSON content
        json_data = json.loads(cleaned_json_content)

        # Define the new file path with .json extension
        base_name, _ = os.path.splitext(file_path)
        new_file_path = f"{base_name}.json"

        # Write the validated and cleaned JSON to the new file
        with open(new_file_path, 'w', encoding='utf-8') as file:
            json.dump(json_data, file, indent=4)

        # Remove the original .txt file
        os.remove(file_path)

        return (filename, True, cleaned)
    except Exception as e:
        return (filename, False, str(e))

def validate_and_replace_json_files(source_dir, num_workers):
    # Collect all .txt file paths recursively
    file_paths = []
    for root, dirs, files in os.walk(source_dir):
        for f in files:
            if f.lower().endswith('.txt'):
                file_paths.append(os.path.join(root, f))
    total_files = len(file_paths)

    print(f"Processing {total_files} .txt files with {num_workers} worker processes...")

    # Initialize counters
    cleaned_count = 0
    replaced_count = 0
    invalid_count = 0

    # Use multiprocessing Pool to process files in parallel
    with multiprocessing.Pool(num_workers) as pool:
        # Use imap_unordered for better performance and integrate with tqdm
        results = []
        for result in tqdm(pool.imap_unordered(validate_json_file, file_paths), total=total_files, desc="Validating JSON files"):
            results.append(result)

    # Process the results
    for filename, is_valid, info in results:
        if is_valid:
            if info:
                cleaned_count += 1
            replaced_count += 1
        else:
            invalid_count += 1

    print(f"\nProcessing complete:")
    print(f"Valid JSON files replaced with .json: {replaced_count}")
    print(f"Files cleaned: {cleaned_count}")
    print(f"Invalid or unreadable files remain as .txt: {invalid_count}")

    print("\nDetailed results:")
    for filename, is_valid, info in results:
        if is_valid:
            if info:
                print(f"{filename} was cleaned and replaced with a .json file.")
            else:
                print(f"{filename} is valid and replaced with a .json file.")
        else:
            print(f"{filename} is invalid or unreadable. Remains as .txt. Error: {info}")

if __name__ == "__main__":
    # Specify the source directory
    source_directory = r"G:\My Drive\2024-2025\coding\rolls_txt"

    # Calculate the number of worker processes (3/4 of available CPUs)
    num_cpus = multiprocessing.cpu_count()
    num_workers = max(1, int(num_cpus * 0.75))

    # Start the validation and replacement process
    validate_and_replace_json_files(source_directory, num_workers)


********************************************************************************

File: app/data_processing.py
********************************************************************************

# data_processing.py

import os
import json
import re
import hashlib
from database_setup import (
    insert_document,
    update_field_structure,
    get_db,
    is_file_ingested,
    get_client,
)
from dotenv import load_dotenv
from pymongo import UpdateOne

from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import time
import logging
import argparse
from collections import Counter
import pymongo

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('DataProcessingLogger')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)

    file_handler = logging.FileHandler('database_processing.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Global Variables
# =======================

root_directory = None
db = None  # Will be initialized in each process

# =======================
# Utility Functions
# =======================

def calculate_file_hash(file_path):
    """Calculate SHA256 hash of a file."""
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"Error calculating hash for {file_path}: {e}")
        return None

def clean_json(json_text):
    """Remove control characters and extract valid JSON content."""
    # Remove control characters
    json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)
    # Attempt to find the first and last curly braces
    start_index = json_text.find('{')
    end_index = json_text.rfind('}')
    if start_index != -1 and end_index != -1 and end_index > start_index:
        json_substring = json_text[start_index:end_index + 1]
        try:
            json.loads(json_substring)
            return json_substring
        except json.JSONDecodeError:
            # Try to fix common JSON issues
            try:
                fixed_json = json_substring.encode('utf-8').decode('unicode_escape', 'ignore')
                json.loads(fixed_json)
                return fixed_json
            except json.JSONDecodeError:
                raise ValueError("Invalid JSON format after cleaning.")
    raise ValueError("Invalid JSON format: Unable to find valid JSON object.")

def load_and_validate_json_file(file_path):
    """Load a JSON file, validate its content, and return it as a dictionary."""
    filename = os.path.basename(file_path)
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            json_content = file.read()
        cleaned_json_content = clean_json(json_content)
        json_data = json.loads(cleaned_json_content)
        json_data.setdefault('filename', filename)
        json_data.setdefault('relative_path', os.path.relpath(file_path, start=root_directory))
        json_data['file_path'] = file_path
        json_data['file_hash'] = calculate_file_hash(file_path)
        return json_data, None
    except json.JSONDecodeError as e:
        error_msg = f"Error decoding JSON in {filename}: {str(e)}"
        return None, error_msg
    except Exception as e:
        error_msg = f"Error processing file {filename}: {str(e)}"
        return None, error_msg

# =======================
# Processing Functions
# =======================

def init_db():
    """Initialize a new MongoDB connection for each process."""
    global db
    try:
        client = get_client()
        db = get_db(client)
        logger.debug("Database connection initialized.")
    except Exception as e:
        logger.exception("Failed to initialize database connection")
        raise e

def process_file(file_path):
    """
    Process a single file and return the result.
    """
    # Initialize database connection for this process
    if db is None:
        init_db()
    filename = os.path.basename(file_path)
    logger.debug(f"Processing file: {filename}")
    result = {'processed': [], 'failed': [], 'skipped': []}

    try:
        file_hash = calculate_file_hash(file_path)
        if is_file_ingested(db, file_hash):
            logger.debug(f"File already ingested: {filename}")
            result['skipped'].append(file_path)
            return result, None

        json_data, error = load_and_validate_json_file(file_path)
        if json_data:
            try:
                update_field_structure(db, json_data)  # Pass 'db' here
                insert_document(db, json_data)         # Pass 'db' here
                logger.debug(f"Processed and inserted document: {filename}")
                result['processed'].append(file_path)
            except Exception as e:
                logger.exception(f"Error processing {filename}")
                result['failed'].append((file_path, str(e)))
        else:
            if error:
                logger.error(error)
                result['failed'].append((file_path, error))

    except Exception as e:
        logger.exception(f"Unexpected error processing {filename}")
        result['failed'].append((file_path, str(e)))

    return result, None

def get_all_files(directory):
    """Recursively get all JSON and TXT files in the given directory and its subdirectories."""
    file_list = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.json', '.txt')):
                file_list.append(os.path.join(root, file))
                logger.debug(f"Found file: {os.path.join(root, file)}")
    return file_list

# =======================
# Sequential Processing
# =======================
def process_directory(directory_path):
    """Process all files in a directory and its subdirectories sequentially for debugging."""
    global root_directory
    root_directory = directory_path

    start_time = time.time()
    files = get_all_files(directory_path)
    total = len(files)

    logger.info(f"Found {total} files to process.")

    if total == 0:
        logger.warning("No files found to process. Exiting.")
        return

    # Initialize results_dict in the main process
    results_dict = {
        'processed': [],
        'failed': [],
        'skipped': []
    }

    with tqdm(total=total, desc="Processing files") as pbar:
        for idx, file_path in enumerate(files, 1):
            result, _ = process_file(file_path)
            for key in ['processed', 'failed', 'skipped']:
                results_dict[key].extend(result.get(key, []))
            pbar.update(1)

    # Initialize database connection (if not already)
    if db is None:
        init_db()
        logger.debug("Initialized main process database connection.")

    logger.info("\nProcessing Summary:")
    logger.info(f"Total files found: {total}")
    logger.info(f"Successfully processed: {len(results_dict['processed'])}")
    logger.info(f"Skipped (already ingested): {len(results_dict['skipped'])}")
    logger.info(f"Failed to process: {len(results_dict['failed'])}")

    if results_dict['failed']:
        logger.info("\nFailed files:")
        for file_path, error in results_dict['failed']:
            logger.error(f"- {file_path}: {error}")

    duration = time.time() - start_time
    logger.info(f"\nTotal processing time: {duration:.2f} seconds.")

# =======================
# Main Execution
# =======================
if __name__ == "__main__":
    print("Starting data_processing.py")
    logger.info("Starting data_processing.py")
    parser = argparse.ArgumentParser(description="Process and validate JSON and TXT files for the railroad documents database.")
    parser.add_argument("data_directory", nargs='?', default='/app/archives',
                        help="Path to the root directory containing JSON and/or text files to process (default: '/app/archives')")
    args = parser.parse_args()
    
    data_directory = args.data_directory
    print(data_directory)
    if not os.path.exists(data_directory):
        logger.error(f"Error: The specified directory does not exist: {data_directory}")
        logger.info(f"Creating directory: {data_directory}")
        try:
            os.makedirs(data_directory)
            logger.info(f"Directory created successfully: {data_directory}")
        except Exception as e:
            logger.exception("Failed to create directory")
            exit(1)

    logger.info(f"Processing directory: {data_directory}")
    print("Don't Forget To Turn On Your Fan!")
    process_directory(data_directory)


********************************************************************************

File: app/generate_unique_terms.py
********************************************************************************

# generate_unique_terms.py

import os
import json
import re
import logging
from collections import Counter
from tqdm import tqdm

from pymongo import UpdateOne, MongoClient

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('GenerateUniqueTermsLogger')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)

    file_handler = logging.FileHandler('generate_unique_terms.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Database Configuration
# =======================

# Load environment variables if using a .env file
from dotenv import load_dotenv
load_dotenv()

def get_client():
    mongo_uri = os.getenv('MONGO_URI', 'mongodb://admin:secret@mongodbt:27017/admin')
    return MongoClient(mongo_uri)

def get_db(client):
    return client['railroad_documents']

def get_collections(db):
    documents = db['documents']
    unique_terms_collection = db['unique_terms']
    return documents, unique_terms_collection

# =======================
# Utility Functions
# =======================

def collect_unique_terms_from_text(text):
    """Collect unique words and phrases from a text string."""
    unique_terms = {'word': Counter(), 'phrase': Counter()}
    if isinstance(text, str):
        words = re.findall(r'\w+', text.lower())
        phrases = [' '.join(pair) for pair in zip(words, words[1:])]
        unique_terms['word'].update(words)
        unique_terms['phrase'].update(phrases)
    return unique_terms

def merge_counters(main_counter, new_counter):
    """Merge two unique terms dictionaries."""
    for term_type in ['word', 'phrase']:
        main_counter[term_type].update(new_counter.get(term_type, Counter()))

def is_text_field(value):
    """Check if the field value is a string."""
    return isinstance(value, str)

# =======================
# Main Processing Function
# =======================

def process_document(document):
    """Extract and count unique terms from a single document by iterating over all string fields."""
    unique_terms = {}
    
    for field, value in document.items():
        if is_text_field(value):
            terms = collect_unique_terms_from_text(value)
            if terms['word'] or terms['phrase']:
                unique_terms.setdefault(field, {'word': Counter(), 'phrase': Counter()})
                merge_counters(unique_terms[field], terms)
    
    return unique_terms

def generate_unique_terms(db):
    """Generate unique terms from documents and populate the unique_terms collection."""
    documents_collection, unique_terms_collection = get_collections(db)
    
    cursor = documents_collection.find({}, {'_id': 0})  # Exclude _id for efficiency
    
    aggregated_unique = {}
    
    logger.info("Starting unique terms generation.")
    
    for doc in tqdm(cursor, desc="Processing documents"):
        doc_unique = process_document(doc)
        for field, terms in doc_unique.items():
            if field not in aggregated_unique:
                aggregated_unique[field] = {'word': Counter(), 'phrase': Counter()}
            merge_counters(aggregated_unique[field], terms)
    
    logger.debug(f"Aggregated unique terms: {aggregated_unique}")
    
    # Prepare bulk operations
    operations = []
    for field, types in aggregated_unique.items():
        for term_type, counter in types.items():
            for term, freq in counter.items():
                operations.append(
                    UpdateOne(
                        {"term": term, "field": field, "type": term_type},
                        {"$inc": {"frequency": freq}},
                        upsert=True
                    )
                )
    
    logger.debug(f"Prepared {len(operations)} bulk operations for unique_terms.")
    
    if operations:
        try:
            result = unique_terms_collection.bulk_write(operations, ordered=False)
            logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} unique terms.")
        except Exception as e:
            logger.error(f"Error bulk upserting unique terms: {e}")
            raise e
    else:
        logger.warning("No unique terms to upsert.")
    
    logger.info("Unique terms generation completed.")

# =======================
# Main Execution
# =======================

if __name__ == "__main__":
    try:
        client = get_client()
        db = get_db(client)
        logger.info("Connected to MongoDB.")
        
        generate_unique_terms(db)
        
    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)


********************************************************************************

File: app/routes.py
********************************************************************************

# File: routes.py
# Path: routes.py

from flask import request, jsonify, render_template, redirect, url_for, flash, session, abort, Response, send_file, Flask
from chunk_utils import save_unique_terms, retrieve_unique_terms
from functools import wraps
from app import app, cache
from database_setup import (
    get_client,
    get_db,
    get_collections,
    find_document_by_id,
    update_document,
    delete_document,
    get_field_structure
)
from bson import ObjectId
from werkzeug.security import generate_password_hash, check_password_hash
import math
import json
import re
import logging
import time
from datetime import datetime, timedelta
import random
import csv
from io import StringIO
from logging.handlers import RotatingFileHandler
import os
import uuid
import pymongo

# Create a logger instance
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# Define a log format
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Create a console handler (optional)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Create a file handler to log to routes.log
log_file_path = os.path.join(os.path.dirname(__file__), 'routes.log')
file_handler = logging.FileHandler(log_file_path)
file_handler.setLevel(logging.DEBUG)  # Set the level for the file handler
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Initialize database connection and collections
client = get_client()
db = get_db(client)
documents, unique_terms_collection, field_structure_collection = get_collections(db)

# Hashed password (generate this using generate_password_hash('your_actual_password'))
ADMIN_PASSWORD_HASH = 'pbkdf2:sha256:260000$uxZ1Fkjt9WQCHwuN$ca37dfb41ebc26b19daf24885ebcd09f607cab85f92dcab13625627fd9ee902a'

# Login attempt tracking
MAX_ATTEMPTS = 5
LOCKOUT_TIME = 15 * 60  # 15 minutes in seconds
login_attempts = {}

def is_locked_out(ip):
    if ip in login_attempts:
        attempts, last_attempt_time = login_attempts[ip]
        if attempts >= MAX_ATTEMPTS:
            if datetime.now() - last_attempt_time < timedelta(seconds=LOCKOUT_TIME):
                return True
            else:
                login_attempts[ip] = (0, datetime.now())
    return False

def update_login_attempts(ip, success):
    if ip in login_attempts:
        attempts, _ = login_attempts[ip]
        if success:
            login_attempts[ip] = (0, datetime.now())
        else:
            login_attempts[ip] = (attempts + 1, datetime.now())
    else:
        login_attempts[ip] = (0, datetime.now()) if success else (1, datetime.now())

# Login required decorator
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'logged_in' not in session:
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

@app.route('/')
# @login_required
def index():
    app.logger.info('Handling request to index')
    num_search_fields = 3  # Number of search fields to display
    field_structure = get_field_structure(db)  # Pass 'db' here
    return render_template('index.html', num_search_fields=num_search_fields, field_structure=field_structure)

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        ip = request.remote_addr

        if is_locked_out(ip):
            flash('Too many failed attempts. Please try again later.')
            return render_template('login.html')

        # Verify CAPTCHA
        user_captcha = request.form.get('captcha')
        correct_captcha = request.form.get('captcha_answer')
        if user_captcha != correct_captcha:
            flash('Incorrect CAPTCHA')
            return redirect(url_for('login'))

        if check_password_hash(ADMIN_PASSWORD_HASH, request.form['password']):
            session['logged_in'] = True
            update_login_attempts(ip, success=True)
            flash('You were successfully logged in')
            next_page = request.args.get('next')
            return redirect(next_page or url_for('index'))
        else:
            update_login_attempts(ip, success=False)
            time.sleep(2)  # Add a delay after failed attempt
            flash('Invalid password')

    # Generate CAPTCHA for GET requests
    captcha_num1 = random.randint(1, 10)
    captcha_num2 = random.randint(1, 10)
    captcha_answer = str(captcha_num1 + captcha_num2)

    return render_template('login.html', captcha_num1=captcha_num1, captcha_num2=captcha_num2, captcha_answer=captcha_answer)

@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    flash('You were logged out')
    return redirect(url_for('index'))

@app.route('/search', methods=['POST'])
# @login_required
def search():
    try:
        data = request.get_json()
        logger.debug(f"Received search request: {data}")

        page = int(data.get('page', 1))
        per_page = int(data.get('per_page', 50))

        query = build_query(data)
        logger.debug(f"Constructed MongoDB query: {query}")

        total_count = documents.count_documents(query)
        search_results = list(documents.find(query).skip((page - 1) * per_page).limit(per_page))

        for doc in search_results:
            doc['_id'] = str(doc['_id'])

        total_pages = math.ceil(total_count / per_page) if per_page else 1

        # Generate unique search ID
        search_id = str(uuid.uuid4())
        # Store the ordered list of document IDs
        ordered_ids = [doc['_id'] for doc in search_results]
        cache.set(f'search_{search_id}', ordered_ids, timeout=3600)  # Expires in 1 hour

        logger.debug(f"Search ID: {search_id}, Found {total_count} documents.")

        return jsonify({
            "search_id": search_id,
            "documents": search_results,
            "total_count": total_count,
            "current_page": page,
            "total_pages": total_pages,
            "per_page": per_page
        })

    except Exception as e:
        logger.error(f"An error occurred during search: {str(e)}", exc_info=True)
        return jsonify({"error": "An internal error occurred"}), 500

def build_query(data):
    query = {}
    criteria_list = []

    logger.debug(f"Building query from search data: {data}")

    for i in range(1, 4):
        field = data.get(f'field{i}')
        search_term = data.get(f'searchTerm{i}')
        operator = data.get(f'operator{i}')

        if field and search_term:
            condition = {}
            if operator == 'NOT':
                condition[field] = {'$not': {'$regex': re.escape(search_term), '$options': 'i'}}
            else:
                condition[field] = {'$regex': re.escape(search_term), '$options': 'i'}
            
            criteria_list.append((operator, condition))
            logger.debug(f"Processed field {field} with search term '{search_term}' and operator '{operator}'")

    if criteria_list:
        and_conditions = []
        or_conditions = []

        for operator, condition in criteria_list:
            if operator == 'AND' or operator == 'NOT':
                and_conditions.append(condition)
            elif operator == 'OR':
                or_conditions.append(condition)

        if and_conditions:
            query['$and'] = and_conditions

        if or_conditions:
            if '$or' not in query:
                query['$or'] = or_conditions
            else:
                query['$or'].extend(or_conditions)

    logger.debug(f"Final query: {query}")
    return query

@app.route('/document/<string:doc_id>')
# @login_required
def document_detail(doc_id):
    
    
    # Hard-coded SHOW_EMPTY variable
    SHOW_EMPTY = False  # Set to True to show empty fields, False to hide them

    # Function to clean the document data
    def clean_data(data):
        empty_values = [None, '', 'N/A', 'null', [], {}, 'None']
        if isinstance(data, dict):
            return {
                k: clean_data(v)
                for k, v in data.items()
                if v not in empty_values and clean_data(v) not in empty_values
            }
        elif isinstance(data, list):
            return [
                clean_data(item)
                for item in data
                if item not in empty_values and clean_data(item) not in empty_values
            ]
        else:
            return data
    
    
    
    
    search_id = request.args.get('search_id')
    if not search_id:
        flash('Missing search context.')
        return redirect(url_for('index'))

    try:
        # Fetch the document by ID
        document = find_document_by_id(db, doc_id)
        if not document:
            abort(404)

        document['_id'] = str(document['_id'])

        # Log the document information for debugging
        logger.debug(f"Retrieved document for ID {doc_id}: {document}")


        # Decide whether to clean the document based on SHOW_EMPTY
        if SHOW_EMPTY:
            document = document
        else:
            # Clean the document to remove empty fields
            document = clean_data(document)
        









        # Retrieve the ordered list from cache
        ordered_ids = cache.get(f'search_{search_id}')
        if not ordered_ids:
            flash('Search context expired. Please perform the search again.')
            return redirect(url_for('index'))

        try:
            current_index = ordered_ids.index(doc_id)
        except ValueError:
            flash('Document not found in the current search results.')
            return redirect(url_for('index'))

        # Determine previous and next IDs based on the search order
        prev_id = ordered_ids[current_index - 1] if current_index > 0 else None
        next_id = ordered_ids[current_index + 1] if current_index < len(ordered_ids) - 1 else None

        # Get the relative path from the document
        relative_path = document.get('relative_path')  # This should contain the relative path to the JSON file

        if relative_path:
            # Construct the image path by removing the '.json' extension
            image_path = relative_path.replace('.json', '')  # 'rolls/rolls/tray_1_roll_5_page3303_img1.png'
            logger.debug(f"Document ID: {doc_id}, Image path: {image_path}")

            # Check if the image file exists
            absolute_image_path = os.path.join('/app/archives', image_path)
            image_exists = os.path.exists(absolute_image_path)

            if not image_exists:
                logger.warning(f"Image not found at: {absolute_image_path}")
        else:
            # Log an error if relative_path is None or not found
            logger.error(f"Error: No relative_path found for document ID: {doc_id}. Document content: {document}")
            image_exists = False
            image_path = None

        # Render the template with all required variables
        return render_template(
            'document-detail.html',
            document=document,
            prev_id=prev_id,
            next_id=next_id,
            search_id=search_id,
            image_path=image_path,  # Pass the constructed image path
            image_exists=image_exists  # Pass the flag indicating if the image exists
        )
    except Exception as e:
        logger.error(f"Error in document_detail: {str(e)}", exc_info=True)
        abort(500)





@app.route('/images/<path:filename>')
def serve_image(filename):
    image_path = os.path.join('/app/archives', filename)
    logger.debug(f"Serving image from: {image_path}")
    if os.path.exists(image_path):
        return send_file(image_path)
    else:
        logger.warning(f"Image not found at: {image_path}")
        abort(404)


def get_top_unique_terms(db, field, term_type, limit=1000, skip=0):
    """
    Retrieve top unique terms sorted by frequency in descending order with pagination.
    :param db: Database instance
    :param field: The field to filter terms by (e.g., 'title', 'description')
    :param term_type: The type of term ('word' or 'phrase')
    :param limit: Number of top terms to retrieve
    :param skip: Number of records to skip for pagination
    :return: List of dictionaries with term and count
    """
    unique_terms_collection = db['unique_terms']
    
    try:
        query = {"field": field, "type": term_type}
        start_time = time.time()
        cursor = unique_terms_collection.find(query, {"_id": 0, "term": 1, "frequency": 1}) \
                                         .sort("frequency", pymongo.DESCENDING) \
                                         .skip(skip) \
                                         .limit(limit)
        terms_list = []
        for doc in cursor:
            key = 'word' if term_type == 'word' else 'phrase'
            terms_list.append({key: doc['term'], 'count': doc['frequency']})
        
        duration = time.time() - start_time
        logger.info(f"Retrieved top {len(terms_list)} {term_type}s in {duration:.4f} seconds for field '{field}'.")
        
        return terms_list
    except Exception as e:
        logger.error(f"Error retrieving unique terms: {e}")
        return []

def get_unique_terms_count(db, field, term_type):
    """
    Get the count of unique terms for a specific field and type.
    :param db: Database instance
    :param field: The field to filter terms by
    :param term_type: The type of term ('word' or 'phrase')
    :return: Integer count of unique terms
    """
    unique_terms_collection = db['unique_terms']
    
    try:
        count = unique_terms_collection.count_documents({"field": field, "type": term_type})
        logger.info(f"Counted {count} unique {term_type}s for field '{field}'.")
        return count
    except Exception as e:
        logger.error(f"Error counting unique terms: {e}")
        return 0

@app.route('/search-terms', methods=['GET'])
def search_terms():
    client = get_client()  # Initialize your MongoDB client
    db = get_db(client)    # Get the database instance

    if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
        # Handle AJAX request
        field = request.args.get('field')
        if not field:
            return jsonify({"error": "No field specified"}), 400

        logger.debug(f"AJAX request for field: {field}")  # Using logger here
        
        # Define term types
        term_types = ['word', 'phrase']
        data = {}
        total_records = 0

        for term_type in term_types:
            page = int(request.args.get('page', 1))
            per_page = int(request.args.get('per_page', 100))
            skip = (page - 1) * per_page
            terms = get_top_unique_terms(db, field, term_type, limit=per_page, skip=skip)
            data[term_type + 's'] = terms
            count = get_unique_terms_count(db, field, term_type)
            data['unique_' + term_type + 's'] = count
            total_records += count

        data['total_records'] = total_records

        return jsonify(data)
    else:
        # Render the HTML template
        field_structure = get_field_structure(db)
        unique_fields = []  # Define if necessary
        return render_template('search-terms.html', field_structure=field_structure, unique_fields=unique_fields)

@app.route('/database-info')
# @login_required
def database_info():
    field_struct = get_field_structure(db)  # Pass 'db' here
    collection_info = []

    def count_documents_with_field(field_path):
        count = documents.count_documents({field_path: {'$exists': True}})
        return count

    def traverse_structure(structure, current_path=''):
        for field, value in structure.items():
            path = f"{current_path}.{field}" if current_path else field
            if isinstance(value, dict):
                traverse_structure(value, current_path=path)
            else:
                count = count_documents_with_field(path)
                collection_info.append({
                    'name': path,
                    'count': count
                })

    traverse_structure(field_struct)

    return render_template('database-info.html', collection_info=collection_info)

@app.route('/settings', methods=['GET', 'POST'])
# @login_required
def settings():
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')

    if request.method == 'POST':
        new_config = request.form.to_dict()

        for key in ['fonts', 'sizes', 'colors', 'spacing']:
            if key in new_config:
                try:
                    new_config[key] = json.loads(new_config[key])
                except json.JSONDecodeError:
                    flash(f"Invalid JSON format for {key}.", 'danger')
                    return redirect(url_for('settings'))

        try:
            with open(config_path, 'w') as config_file:
                json.dump(new_config, config_file, indent=4)
            app.config['UI_CONFIG'] = new_config
            flash('Settings updated successfully', 'success')
        except Exception as e:
            logger.error(f"Error updating settings: {str(e)}")
            flash('Failed to update settings.', 'danger')
        return redirect(url_for('settings'))

    try:
        if os.path.exists(config_path):
            with open(config_path) as config_file:
                config = json.load(config_file)
        else:
            config = {}
    except json.JSONDecodeError:
        config = {}
        flash('Configuration file is corrupted. Using default settings.', 'warning')

    return render_template('settings.html', config=config)

# Consider streaming if it ends up being thousands of documents
@app.route('/export_selected_csv', methods=['POST'])
# @login_required
def export_selected_csv():
    try:
        data = request.get_json()
        document_ids = data.get('document_ids', [])
        if not document_ids:
            return jsonify({"error": "No document IDs provided"}), 400

        # Convert string IDs to ObjectIds, handle invalid IDs
        valid_ids = []
        for doc_id in document_ids:
            try:
                valid_ids.append(ObjectId(doc_id))
            except Exception as e:
                logger.warning(f"Invalid document ID: {doc_id}")

        if not valid_ids:
            return jsonify({"error": "No valid document IDs provided"}), 400

        # Check if any documents exist with the provided IDs
        count = documents.count_documents({"_id": {"$in": valid_ids}})
        if count == 0:
            return jsonify({"error": "No documents found for the provided IDs."}), 404

        # Retrieve the documents
        documents_cursor = documents.find({"_id": {"$in": valid_ids}})

        # Create CSV
        output = StringIO()
        writer = csv.writer(output)
        writer.writerow(['filename', 'OCR', 'original_json'])  # Header row

        for doc in documents_cursor:
            filename = doc.get('filename', 'N/A')
            ocr = doc.get('summary', 'N/A')  # Adjust field as necessary
            original_json = json.dumps(doc, default=str)  # Convert ObjectId to string if necessary
            writer.writerow([filename, ocr, original_json])

        # Prepare CSV for download
        output.seek(0)
        return Response(
            output.getvalue(),
            mimetype='text/csv',
            headers={'Content-Disposition': 'attachment; filename=selected_documents.csv'}
        )

    except Exception as e:
        logger.error(f"Error exporting selected CSV: {str(e)}", exc_info=True)
        return jsonify({"error": "An internal error occurred"}), 500

@app.errorhandler(404)
def not_found_error(error):
    return render_template('error.html', message='Page not found'), 404

@app.errorhandler(500)
def internal_error(error):
    return render_template('error.html', message='An unexpected error has occurred'), 500


********************************************************************************

File: app/test_mongo_connection.py
********************************************************************************

# Date: 2024-10-07
# Purpose: Test connection to MongoDB using PyMongo and print connection details.

import os
from pymongo import MongoClient
from urllib.parse import urlparse
import logging
from dotenv import load_dotenv



def test_connection():
    env_file_path  = '../.env'

    # Load environment variables from .env file
    env_path = load_dotenv(env_file_path)  # Load .env file

    print(os.path.abspath(env_file_path))

    if env_path:
        print(f"Using .env file: {env_path}")
    else:
        print("No .env file found.")

    # Retrieve the MONGO_URI from environment variables
    mongo_uri = os.environ.get('MONGO_URI')
    print(mongo_uri)




    
    # Set a default value for testing if environment variable is not set
    #mongo_uri = "mongodb://admin:secret@mongodb:27017/admin"
    print(f"Using default MONGO_URI: {mongo_uri}")

    print(mongo_uri)
    


    # # Print all environment variables for debugging
    # print("Current Environment Variables:")
    # for key, value in os.environ.items():
    #     print(f"{key}={value}")


    # Parse the MongoDB URI to extract its components
    parsed_uri = urlparse(mongo_uri)
    
    # Extract components with masking for sensitive information
    scheme = parsed_uri.scheme
    username = parsed_uri.username
    password = parsed_uri.password 
    hostname = parsed_uri.hostname
    port = parsed_uri.port
    auth_database = parsed_uri.path[1:] if parsed_uri.path else None  # Remove leading '/'
    query = parsed_uri.query

    # Display the parsed components
    print("🔍 **MongoDB Connection Details:**")
    print(f"• **Scheme:** {scheme}")
    print(f"• **Username:** {username}")
    print(f"• **Password:** {password}")
    print(f"• **Hostname:** {hostname}")
    print(f"• **Port:** {port}")
    print(f"• **Authentication Database:** {auth_database}")
    print(f"• **Options:** {query}\n")

    print(f"MONGO_URI: mongodb://{username}:****@{hostname}:{port}/{auth_database}")

    try:
        # Initialize the MongoDB client with increased timeout for reliability
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        
        # Attempt to ping the MongoDB server to test the connection
        client.admin.command('ping')
        print("✅ Successfully connected to MongoDB.")
    except Exception as e:
        print(f"❌ Failed to connect to MongoDB: {e}")

if __name__ == "__main__":
    print("🚀 Testing MongoDB connection...\n")
    test_connection()


********************************************************************************

File: app/json_validator_multi.py
********************************************************************************

import os
import json
import re
import multiprocessing
from tqdm import tqdm

def clean_json(json_text):
    """
    Cleans the JSON text by removing control characters and extracting the valid JSON portion.
    """
    # Remove all control characters
    json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)

    # Find the index of the first '{' and the last '}'
    start_index = json_text.find('{')
    end_index = json_text.rfind('}')

    # Extract the clean JSON string
    if start_index != -1 and end_index != -1:
        clean_json_text = json_text[start_index:end_index + 1]
        return clean_json_text
    else:
        raise ValueError("Invalid JSON format: Unable to find '{' or '}'.")

def validate_json_file(file_path):
    """
    Validates and cleans a single JSON file.
    Converts valid .txt files to .json and renames invalid ones to .bad.
    """
    filename = os.path.basename(file_path)
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            json_content = file.read()

        # Clean the JSON content before validation
        cleaned_json_content = clean_json(json_content)
        cleaned = cleaned_json_content != json_content

        # Validate the cleaned JSON content
        json_data = json.loads(cleaned_json_content)

        # Define the new file path with .json extension
        base_name, _ = os.path.splitext(file_path)
        new_file_path = f"{base_name}.json"

        # Write the validated and cleaned JSON to the new file
        with open(new_file_path, 'w', encoding='utf-8') as file:
            json.dump(json_data, file, indent=4)

        # Remove the original .txt file
        os.remove(file_path)

        return (filename, True, cleaned)
    except Exception as e:
        # Rename the original .txt file by appending .bad
        bad_file_path = f"{file_path}.bad"
        try:
            os.rename(file_path, bad_file_path)
        except Exception as rename_error:
            return (filename, False, f"Failed to rename to .bad. Original error: {e}; Rename error: {rename_error}")
        return (filename, False, f"File renamed to .bad due to invalid JSON. Error: {e}")

def validate_and_replace_json_files(source_dir, num_workers):
    """
    Traverses the source directory recursively to find and process all .txt files.
    Utilizes multiprocessing for parallel processing.
    """
    # Collect all .txt file paths recursively
    file_paths = []
    print(f"🔍 Starting to walk through the source directory: {source_dir}\n")
    for root, dirs, files in os.walk(source_dir, followlinks=True):
        print(f"📂 Accessing directory: {root}")
        for f in files:
            if f.lower().endswith('.txt'):
                full_path = os.path.join(root, f)
                file_paths.append(full_path)
                print(f"📄 Found .txt file: {full_path}")
    total_files = len(file_paths)

    print(f"\n📊 Processing {total_files} .txt files with {num_workers} worker processes...\n")

    # Initialize counters
    cleaned_count = 0
    replaced_count = 0
    invalid_count = 0

    if total_files == 0:
        print("🚫 No .txt files found. Exiting the script.")
        return

    # Use multiprocessing Pool to process files in parallel
    with multiprocessing.Pool(num_workers) as pool:
        results = []
        for result in tqdm(pool.imap_unordered(validate_json_file, file_paths), total=total_files, desc="Validating JSON files"):
            results.append(result)

    # Process the results
    for filename, is_valid, info in results:
        if is_valid:
            if info:
                cleaned_count += 1
            replaced_count += 1
        else:
            invalid_count += 1

    print(f"\n✅ Processing complete:")
    print(f"✔️  Valid JSON files replaced with .json: {replaced_count}")
    print(f"🧹 Files cleaned: {cleaned_count}")
    print(f"❌ Invalid or unreadable files renamed to .bad: {invalid_count}")

    print("\n📋 Detailed results:")
    for filename, is_valid, info in results:
        if is_valid:
            if info:
                print(f"✅ {filename} was cleaned and replaced with a .json file.")
            else:
                print(f"✅ {filename} is valid and replaced with a .json file.")
        else:
            print(f"❌ {filename} was invalid and renamed to .bad. Error: {info}")

if __name__ == "__main__":
    # Specify the source directory
    source_directory = "/home/lhyman/coding/nosql_reader/archives"

    # Calculate the number of worker processes (3/4 of available CPUs)
    num_cpus = multiprocessing.cpu_count()
    num_workers = max(1, int(num_cpus * 0.75))

    # Start the validation and replacement process
    validate_and_replace_json_files(source_directory, num_workers)


********************************************************************************

File: app/templates/index.html
********************************************************************************

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Historical Document Reader</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Open+Sans&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="container">
        <h1 class="mt-4">Historical Document Reader</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
                {% if 'logged_in' in session %}
                    <li><a href="{{ url_for('logout') }}">Logout</a></li>
                {% else %}
                    <li><a href="{{ url_for('login') }}">Login</a></li>
                {% endif %}
            </ul>
        </nav>

        <form id="searchForm">
            <div id="searchFields">
                {% for i in range(1, num_search_fields + 1) %}
                <div class="search-field">
                    <label for="field{{ i }}">Field:</label>
                    <select name="field{{ i }}" id="field{{ i }}">
                        <option value="" disabled selected>Select a field</option>
                        {% for field in field_structure|dictsort %}
                            <option value="{{ field[0] }}">{{ field[0] }}</option>
                        {% endfor %}
                    </select>
                    <label for="operator{{ i }}">Operator:</label>
                    <select name="operator{{ i }}" id="operator{{ i }}">
                        <option value="AND">AND</option>
                        <option value="OR">OR</option>
                        <option value="NOT">NOT</option>
                    </select>
                    <label for="searchTerm{{ i }}">Search Term:</label>
                    <input type="text" name="searchTerm{{ i }}" id="searchTerm{{ i }}" placeholder="Enter search term">
                </div>
                {% endfor %}
            </div>
            <button type="submit" id="searchButton">Search</button>
            <!-- Export Selected to CSV Button -->
            <button id="exportSelectedCsv" style="display: none; margin-top: 10px;">Export Selected to CSV</button>
        </form>

        <!-- Loading Indicator -->
        <div id="loadingIndicator" style="display: none;">
            <div class="spinner"></div>
            <p>Loading... Please wait.</p>
        </div>

        <!-- Cancel Search Button -->
        <button id="cancelSearch" style="display: none;">Cancel Search</button>

        <!-- Total Results Display -->
        <div id="totalResults" class="mt-4"></div>

        <!-- Results Container -->
        <div id="results" class="mt-4">
            <!-- The table will be dynamically inserted here -->
        </div>

        
    </div>

    <!-- Include jQuery -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <!-- Include your custom script -->
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>


********************************************************************************

File: app/templates/search-terms.html
********************************************************************************

<!-- File: templates/search-terms.html -->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Search Terms</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <!-- Include DataTables CSS -->
    <link rel="stylesheet" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
    <style>
        #loadingIndicator {
            display: none;
            text-align: center;
            margin-top: 20px;
        }
        .spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        nav ul {
            list-style-type: none;
            padding: 0;
            display: flex;
            gap: 15px;
        }
        nav ul li {
            display: inline;
        }
        nav ul li a {
            text-decoration: none;
            color: #3498db;
            font-weight: bold;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .section {
            margin-top: 30px;
        }
        #noDataMessage {
            display: none;
            color: red;
            margin-top: 20px;
            font-weight: bold;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Search Terms</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
            </ul>
        </nav>
        <div style="margin-top: 20px;">
            <label for="fieldSelect">Select Field:</label>
            <select id="fieldSelect">
                <option value="" disabled selected>Select a field</option>
                {% for field in field_structure|dictsort %}
                    {% if field[0] not in unique_fields %}
                        <option value="{{ field[0] }}">{{ field[0] }}</option>
                    {% endif %}
                {% endfor %}
            </select>
        </div>
        <div id="tableInfo" style="margin-top: 10px;">
            <p>Number of unique words: <span id="uniqueWords">0</span></p>
            <p>Number of unique phrases: <span id="uniquePhrases">0</span></p>
            <p>Total number of records: <span id="totalRecords">0</span></p>
        </div>
        <div id="loadingIndicator">
            <div class="spinner"></div>
            <p>Loading data...</p>
        </div>
        <!-- No Data Message -->
        <div id="noDataMessage"></div>
        <div class="section">
            <h2>Words</h2>
            <table id="wordsTable" class="display" style="width:100%; margin-top: 10px;">
                <thead>
                    <tr>
                        <th>Word</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Initially empty; populated via AJAX -->
                </tbody>
            </table>
        </div>
        <div class="section">
            <h2>Phrases</h2>
            <table id="phrasesTable" class="display" style="width:100%; margin-top: 10px;">
                <thead>
                    <tr>
                        <th>Phrase</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Initially empty; populated via AJAX -->
                </tbody>
            </table>
        </div>
    </div>
    <!-- Include jQuery and DataTables JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.js"></script>
    <script>
        $(document).ready(function() {
            var wordsTable = $('#wordsTable').DataTable({
                "pageLength": 25,
                "order": [[ 0, "asc" ]],
                "columns": [
                    { "data": "word" },
                    { "data": "count" }
                ]
            });

            var phrasesTable = $('#phrasesTable').DataTable({
                "pageLength": 25,
                "order": [[ 0, "asc" ]],
                "columns": [
                    { "data": "phrase" },
                    { "data": "count" }
                ]
            });

            function updateTableInfo(data) {
                $('#uniqueWords').text(data.unique_words);
                $('#uniquePhrases').text(data.unique_phrases);
                $('#totalRecords').text(data.total_records);
            }

            $('#fieldSelect').on('change', function() {
                var selectedField = $(this).val();
                if (!selectedField) {
                    wordsTable.clear().draw();
                    phrasesTable.clear().draw();
                    updateTableInfo({ unique_words: 0, unique_phrases: 0, total_records: 0 });
                    $('#noDataMessage').hide();
                    return;
                }

                $('#loadingIndicator').show();
                $('#noDataMessage').hide();
                $('#wordsTableContainer').show();
                $('#phrasesTableContainer').show();

                $.ajax({
                    url: '/search-terms',
                    data: { field: selectedField },
                    success: function(data) {
                        // Check if both words and phrases are empty
                        if (data.words.length === 0 && data.phrases.length === 0) {
                            // Hide the tables
                            $('.section').hide();
                            // Show the no data message
                            $('#noDataMessage').text(data.message || 'No data available for this field.').show();
                        } else {
                            // Show the tables in case they were hidden
                            $('.section').show();
                            // Hide the no data message
                            $('#noDataMessage').hide();

                            // Populate words table
                            wordsTable.clear();
                            data.words.forEach(function(word) {
                                wordsTable.row.add({ word: word.word, count: word.count });
                            });
                            wordsTable.draw();

                            // Populate phrases table
                            phrasesTable.clear();
                            data.phrases.forEach(function(phrase) {
                                phrasesTable.row.add({ phrase: phrase.phrase, count: phrase.count });
                            });
                            phrasesTable.draw();
                        }

                        // Update table info
                        updateTableInfo(data);
                        $('#loadingIndicator').hide();
                    },
                    error: function() {
                        alert('Error fetching data. Please try again.');
                        $('#loadingIndicator').hide();
                    }
                });
            });
        });
    </script>
</body>
</html>


********************************************************************************

File: app/templates/settings.html
********************************************************************************

<!-- File: templates/settings.html -->

{% extends "base.html" %}

{% block content %}
<div class="container">
    <h1>Settings</h1>
    <form method="POST">
        <h2>Fonts</h2>
        <label for="fonts_main">Main Font:</label>
        <input type="text" id="fonts_main" name="fonts[main]" value="{{ config['fonts']['main'] }}"><br>
        <label for="fonts_headers">Headers Font:</label>
        <input type="text" id="fonts_headers" name="fonts[headers]" value="{{ config['fonts']['headers'] }}"><br>

        <h2>Sizes</h2>
        <label for="sizes_base">Base Size:</label>
        <input type="text" id="sizes_base" name="sizes[base]" value="{{ config['sizes']['base'] }}"><br>
        <label for="sizes_h1">H1 Size:</label>
        <input type="text" id="sizes_h1" name="sizes[h1]" value="{{ config['sizes']['h1'] }}"><br>
        <label for="sizes_h2">H2 Size:</label>
        <input type="text" id="sizes_h2" name="sizes[h2]" value="{{ config['sizes']['h2'] }}"><br>
        <label for="sizes_h3">H3 Size:</label>
        <input type="text" id="sizes_h3" name="sizes[h3]" value="{{ config['sizes']['h3'] }}"><br>

        <h2>Colors</h2>
        <label for="colors_primary">Primary Color:</label>
        <input type="color" id="colors_primary" name="colors[primary]" value="{{ config['colors']['primary'] }}"><br>
        <label for="colors_secondary">Secondary Color:</label>
        <input type="color" id="colors_secondary" name="colors[secondary]" value="{{ config['colors']['secondary'] }}"><br>
        <label for="colors_background">Background Color:</label>
        <input type="color" id="colors_background" name="colors[background]" value="{{ config['colors']['background'] }}"><br>
        <label for="colors_text">Text Color:</label>
        <input type="color" id="colors_text" name="colors[text]" value="{{ config['colors']['text'] }}"><br>

        <h2>Spacing</h2>
        <label for="spacing_small">Small Spacing:</label>
        <input type="text" id="spacing_small" name="spacing[small]" value="{{ config['spacing']['small'] }}"><br>
        <label for="spacing_medium">Medium Spacing:</label>
        <input type="text" id="spacing_medium" name="spacing[medium]" value="{{ config['spacing']['medium'] }}"><br>
        <label for="spacing_large">Large Spacing:</label>
        <input type="text" id="spacing_large" name="spacing[large]" value="{{ config['spacing']['large'] }}"><br>

        <button type="submit">Save Settings</button>
    </form>
</div>
<script>
    // Convert nested objects to JSON strings before form submission
    document.querySelector('form').addEventListener('submit', function(e) {
        ['fonts', 'sizes', 'colors', 'spacing'].forEach(function(key) {
            var inputs = document.querySelectorAll(`[name^="${key}\\["]`);
            var obj = {};
            inputs.forEach(function(input) {
                var prop = input.name.match(/\[(.*?)\]/)[1];
                obj[prop] = input.value;
            });
            var hiddenInput = document.createElement('input');
            hiddenInput.type = 'hidden';
            hiddenInput.name = key;
            hiddenInput.value = JSON.stringify(obj);
            this.appendChild(hiddenInput);
        }, this);
    });
</script>
{% endblock %}


********************************************************************************

File: app/templates/error.html
********************************************************************************

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <h1>Error</h1>
    <p>{{ message }}</p>
    <a href="{{ url_for('index') }}">Return to Home</a>
</body>
</html>


********************************************************************************

File: app/templates/login.html
********************************************************************************

<!-- File: templates/login.html -->
<!-- Created: 2024-08-12 11:00 -->

{% extends "base.html" %}

{% block content %}
    <h1>Login</h1>
    
    {% with messages = get_flashed_messages() %}
        {% if messages %}
            <ul class="flashes">
                {% for message in messages %}
                    <li>{{ message }}</li>
                {% endfor %}
            </ul>
        {% endif %}
    {% endwith %}
    
    <form method="post">
        <label for="password">Password:</label>
        <input type="password" id="password" name="password" required>
        <br><br>
        
        <label for="captcha">CAPTCHA: What is {{ captcha_num1 }} + {{ captcha_num2 }}?</label>
        <input type="number" id="captcha" name="captcha" required>
        <input type="hidden" name="captcha_answer" value="{{ captcha_answer }}">
        <br><br>
        
        <input type="submit" value="Login">
    </form>
{% endblock %}

********************************************************************************

File: app/templates/base.html
********************************************************************************

<!-- templates/base.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>{% block title %}Historical Document Reader{% endblock %}</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <!-- Add this in the <head> section of base.html -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-yHnj4MvlYYKxE/6q9ZsH5mJ5RkJyKpjtXHYQVdG1Vl6HjnAsfsj0G9aJv1bQZoKEtZnSmhMWj58ZwFZJe61D1A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    {% block styles %}{% endblock %}
</head>
<body>
    <nav>
        <ul>
            <li><a href="{{ url_for('index') }}">Home</a></li>
            {% if session.get('logged_in') %}
                <li><a href="{{ url_for('logout') }}">Logout</a></li>
            {% else %}
                <li><a href="{{ url_for('login') }}">Login</a></li>
            {% endif %}
            <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
            <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
            <li><a href="{{ url_for('settings') }}">Settings</a></li>
        </ul>
    </nav>
    
    {% with messages = get_flashed_messages() %}
        {% if messages %}
            <ul class="flashes">
                {% for message in messages %}
                    <li>{{ message }}</li>
                {% endfor %}
            </ul>
        {% endif %}
    {% endwith %}
    
    {% block content %}{% endblock %}
    {% block scripts %}{% endblock %}
</body>
</html>


********************************************************************************

File: app/templates/database-info.html
********************************************************************************

<!-- templates/database-info.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Database Information</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Database Information</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
            </ul>
        </nav>
        <table>
            <thead>
                <tr>
                    <th>Field Name</th>
                    <th>Number of Records</th>
                </tr>
            </thead>
            <tbody>
                {% for field in collection_info %}
                <tr>
                    <td>{{ field.name }}</td>
                    <td>{{ field.count }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
</body>
</html>


********************************************************************************

File: app/templates/document-detail.html
********************************************************************************

{% extends "base.html" %}

{% macro render_json(key, value) %}
  <div class="info-section">
    <h2>{{ key|title }}</h2>
    {{ render_value(value) }}
  </div>
{% endmacro %}

{% macro render_value(value) %}
  {% if value is mapping %}
    <table class="info-table">
      {% for subkey, subvalue in value.items() %}
        <tr>
          <th>{{ subkey|title }}</th>
          <td>
            {{ render_value(subvalue) }}
          </td>
        </tr>
      {% endfor %}
    </table>
  {% elif value is iterable and value is not string %}
    <ul class="info-list">
      {% for item in value %}
        <li>
          {% if item is mapping %}
            <div class="nested-mapping">
              {% for itemkey, itemvalue in item.items() %}
                <strong>{{ itemkey|title }}:</strong> {{ render_value(itemvalue) }}<br>
              {% endfor %}
            </div>
          {% else %}
            {{ item }}
          {% endif %}
        </li>
      {% endfor %}
    </ul>
  {% else %}
    {{ value if value is not none else 'N/A' }}
  {% endif %}
{% endmacro %}

{% block styles %}
<style>
  .document-detail .detail-container {
    display: flex;
    gap: 20px;
  }

  .document-detail .info-panel {
    flex: 1;
    overflow-y: auto;
    max-height: calc(100vh - 40px);
  }

  .document-detail .image-panel {
    flex: 1;
    position: sticky;
    top: 20px;
    align-self: flex-start;
  }

  #imageContainer {
    width: 100%;
    height: auto;
    max-height: calc(100vh - 100px);
    overflow: auto;
    cursor: grab;
    position: relative;
  }

  #imageContainer.active {
    cursor: grabbing;
  }

  #documentImage {
    display: block;
    transform-origin: center;
  }

  .zoom-controls,
  .adjustment-controls {
    margin-top: 10px;
  }

  .zoom-controls button,
  .adjustment-controls button {
    margin-right: 5px;
    padding: 8px 12px;
    font-size: 14px;
  }

  .adjustment-controls label {
    margin-right: 10px;
  }

  .adjustment-controls input[type="range"] {
    vertical-align: middle;
    margin-right: 10px;
  }
</style>
{% endblock %}

{% block content %}
<div class="document-detail">
  <div class="detail-container">
    <div class="info-panel">
      <div class="navigation">
        <a href="{{ url_for('index') }}?return_to_search=true" class="nav-button" title="Return to Search Results">
            <span>Search</span>
        </a>
        <a href="{{ prev_id and url_for('document_detail', doc_id=prev_id, search_id=search_id) or '#' }}" class="nav-button {{ 'disabled' if not prev_id else '' }}" title="Previous Result">
            <span>Previous</span>
        </a>
        <a href="{{ url_for('index') }}" class="nav-button" title="Home">
            <span>Home</span>
        </a>
        <a href="{{ next_id and url_for('document_detail', doc_id=next_id, search_id=search_id) or '#' }}" class="nav-button {{ 'disabled' if not next_id else '' }}" title="Next Result">
            <span>Next</span>
        </a>
      </div>

      <h1>{{ document.get('filename', 'Untitled Document') }}</h1>

      <div class="info-container">
        {% for key, value in document.items() %}
          {% if key != '_id' and key != 'filename' %}
            {{ render_json(key, value) }}
          {% endif %}
        {% endfor %}
      </div>
    </div>

    <div class="image-panel">
      {% if image_exists %}
        <div id="imageContainer">
          <img id="documentImage" src="{{ url_for('serve_image', filename=image_path | urlencode) }}" alt="Document Image">
        </div>
        <div class="zoom-controls">
          <button id="rotateLeft">Rotate Left</button>
          <button id="rotateRight">Rotate Right</button>
          <button id="zoomIn">Zoom In</button>
          <button id="zoomOut">Zoom Out</button>
          <button id="resetZoom">Reset</button>
        </div>
        <div class="adjustment-controls">
          <label for="brightnessRange">Brightness:</label>
          <input type="range" id="brightnessRange" min="0.5" max="1.5" step="0.1" value="1">
          <label for="contrastRange">Contrast:</label>
          <input type="range" id="contrastRange" min="0.5" max="2" step="0.1" value="1">
          <label for="saturationRange">Saturation:</label>
          <input type="range" id="saturationRange" min="0" max="3" step="0.1" value="1">
          <label for="hueRange">Hue Rotation:</label>
          <input type="range" id="hueRange" min="0" max="360" step="10" value="0">
          <label for="blurRange">Blur:</label>
          <input type="range" id="blurRange" min="0" max="10" step="1" value="0">
          <button id="sharpenBtn">Sharpen</button>
          <button id="grayscaleBtn">Grayscale</button>
          <button id="sepiaBtn">Sepia</button>
          <button id="invertBtn">Invert Colors</button>
          <button id="flipHorizontalBtn">Flip Horizontal</button>
          <button id="flipVerticalBtn">Flip Vertical</button>
          <button id="resetAllBtn">Reset All</button>
        </div>
        <!-- Include SVG filter definitions for sharpening -->
        <svg width="0" height="0">
          <filter id="sharpen">
            <feConvolveMatrix order="3" kernelMatrix="0 -1 0 -1 5 -1 0 -1 0"></feConvolveMatrix>
          </filter>
        </svg>
      {% else %}
        <div class="placeholder-image">
          <p>Image not found: {{ image_path }}</p>
        </div>
      {% endif %}
    </div>
    
  </div>
</div>
{% endblock %}

{% block scripts %}
<script>
  document.addEventListener('DOMContentLoaded', () => {
    let scale = 1;
    let rotation = 0;
    const ZOOM_STEP = 0.1;
    const MAX_SCALE = 3;
    const MIN_SCALE = 0.5;

    // Variables for filters
    const brightnessRange = document.getElementById('brightnessRange');
    const contrastRange = document.getElementById('contrastRange');
    const saturationRange = document.getElementById('saturationRange');
    const hueRange = document.getElementById('hueRange');
    const blurRange = document.getElementById('blurRange');
    const sharpenBtn = document.getElementById('sharpenBtn');
    const grayscaleBtn = document.getElementById('grayscaleBtn');
    const sepiaBtn = document.getElementById('sepiaBtn');
    const invertBtn = document.getElementById('invertBtn');
    const flipHorizontalBtn = document.getElementById('flipHorizontalBtn');
    const flipVerticalBtn = document.getElementById('flipVerticalBtn');
    const resetAllBtn = document.getElementById('resetAllBtn');

    let isSharpened = false;
    let isGrayscale = false;
    let isSepia = false;
    let isInverted = false;
    let isFlippedHorizontal = false;
    let isFlippedVertical = false;

    const imageContainer = document.getElementById('imageContainer');
    const documentImage = document.getElementById('documentImage');
    const zoomInBtn = document.getElementById('zoomIn');
    const zoomOutBtn = document.getElementById('zoomOut');
    const resetZoomBtn = document.getElementById('resetZoom');
    const rotateLeftBtn = document.getElementById('rotateLeft');
    const rotateRightBtn = document.getElementById('rotateRight');

    function setImageTransform() {
      let transforms = [];

      // Scaling
      transforms.push(`scale(${scale})`);

      // Rotation
      transforms.push(`rotate(${rotation}deg)`);

      // Flipping
      const flipScaleX = isFlippedHorizontal ? -1 : 1;
      const flipScaleY = isFlippedVertical ? -1 : 1;
      transforms.push(`scale(${flipScaleX}, ${flipScaleY})`);

      documentImage.style.transform = transforms.join(' ');
    }

    function updateFilters() {
      let filters = [];

      // Brightness
      const brightnessValue = brightnessRange.value;
      filters.push(`brightness(${brightnessValue})`);

      // Contrast
      const contrastValue = contrastRange.value;
      filters.push(`contrast(${contrastValue})`);

      // Saturation
      const saturationValue = saturationRange.value;
      filters.push(`saturate(${saturationValue})`);

      // Hue Rotation
      const hueValue = hueRange.value;
      filters.push(`hue-rotate(${hueValue}deg)`);

      // Blur
      const blurValue = blurRange.value;
      filters.push(`blur(${blurValue}px)`);

      // Grayscale and Sepia
      if (isGrayscale) {
        filters.push(`grayscale(1)`);
      } else if (isSepia) {
        filters.push(`sepia(1)`);
      }

      // Invert
      if (isInverted) {
        filters.push(`invert(1)`);
      }

      // Apply Sharpen Filter if enabled
      if (isSharpened) {
        documentImage.style.filter = filters.join(' ') + ' url(#sharpen)';
      } else {
        documentImage.style.filter = filters.join(' ');
      }
    }

    function zoomIn() {
      if (scale < MAX_SCALE) {
        scale += ZOOM_STEP;
        setImageTransform();
      }
    }

    function zoomOut() {
      if (scale > MIN_SCALE) {
        scale -= ZOOM_STEP;
        setImageTransform();
      }
    }

    function resetZoom() {
      scale = 1;
      rotation = 0;
      setImageTransform();
    }

    function rotateLeft() {
      rotation -= 90;
      setImageTransform();
    }

    function rotateRight() {
      rotation += 90;
      setImageTransform();
    }

    zoomInBtn.addEventListener('click', zoomIn);
    zoomOutBtn.addEventListener('click', zoomOut);
    resetZoomBtn.addEventListener('click', resetZoom);
    rotateLeftBtn.addEventListener('click', rotateLeft);
    rotateRightBtn.addEventListener('click', rotateRight);

    // Event listeners for filters
    brightnessRange.addEventListener('input', updateFilters);
    contrastRange.addEventListener('input', updateFilters);
    saturationRange.addEventListener('input', updateFilters);
    hueRange.addEventListener('input', updateFilters);
    blurRange.addEventListener('input', updateFilters);

    sharpenBtn.addEventListener('click', () => {
      isSharpened = !isSharpened;
      updateFilters();
      sharpenBtn.textContent = isSharpened ? 'Unsharpen' : 'Sharpen';
    });

    grayscaleBtn.addEventListener('click', () => {
      isGrayscale = !isGrayscale;
      isSepia = false;
      updateFilters();
    });

    sepiaBtn.addEventListener('click', () => {
      isSepia = !isSepia;
      isGrayscale = false;
      updateFilters();
    });

    invertBtn.addEventListener('click', () => {
      isInverted = !isInverted;
      updateFilters();
    });

    flipHorizontalBtn.addEventListener('click', () => {
      isFlippedHorizontal = !isFlippedHorizontal;
      setImageTransform();
    });

    flipVerticalBtn.addEventListener('click', () => {
      isFlippedVertical = !isFlippedVertical;
      setImageTransform();
    });

    resetAllBtn.addEventListener('click', () => {
      // Reset transformations
      scale = 1;
      rotation = 0;
      isFlippedHorizontal = false;
      isFlippedVertical = false;
      setImageTransform();

      // Reset filters
      brightnessRange.value = 1;
      contrastRange.value = 1;
      saturationRange.value = 1;
      hueRange.value = 0;
      blurRange.value = 0;
      isSharpened = false;
      isGrayscale = false;
      isSepia = false;
      isInverted = false;
      updateFilters();

      // Reset button texts
      sharpenBtn.textContent = 'Sharpen';
    });

    // Dragging functionality
    let isDragging = false;
    let startX, startY, scrollLeft, scrollTop;

    imageContainer.addEventListener('mousedown', (e) => {
      isDragging = true;
      imageContainer.classList.add('active');
      startX = e.pageX - imageContainer.offsetLeft;
      startY = e.pageY - imageContainer.offsetTop;
      scrollLeft = imageContainer.scrollLeft;
      scrollTop = imageContainer.scrollTop;
    });

    imageContainer.addEventListener('mouseleave', () => {
      isDragging = false;
      imageContainer.classList.remove('active');
    });

    imageContainer.addEventListener('mouseup', () => {
      isDragging = false;
      imageContainer.classList.remove('active');
    });

    imageContainer.addEventListener('mousemove', (e) => {
      if (!isDragging) return;
      e.preventDefault();
      const x = e.pageX - imageContainer.offsetLeft;
      const y = e.pageY - imageContainer.offsetTop;
      const walkX = x - startX;
      const walkY = y - startY;
      imageContainer.scrollLeft = scrollLeft - walkX;
      imageContainer.scrollTop = scrollTop - walkY;
    });
  });
</script>
{% endblock %}


********************************************************************************

File: app/templates/document-list.html
********************************************************************************

<!-- templates/document-list.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document List</title>
</head>
<body>
    <h1>Document List</h1>

    <ul id="documentList">
    {% for document in documents %}
        <li>
            <a href="{{ url_for('document_detail', doc_id=document['_id']) }}" target="_blank" rel="noopener noreferrer" class="document-link">{{ document.get('filename', 'Untitled Document') }}</a>
        </li>
    {% endfor %}
    </ul>
</body>
</html>


********************************************************************************

File: app/static/style.css
********************************************************************************

/* static/style.css */

/* Base Styles */
body {
    font-family: 'Open Sans', sans-serif;
    font-size: 16px;
    line-height: 1.6;
    background-color: #ffffff;
    color: #333;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1, h2, h3 {
    font-family: 'Montserrat', sans-serif;
    color: #333;
    margin-bottom: 20px;
}

/* Navigation */
nav ul {
    list-style-type: none;
    padding: 0;
    display: flex;
    gap: 15px;
    margin-bottom: 20px;
}

nav ul li {
    display: inline;
}

nav ul li a {
    text-decoration: none;
    color: #007bff;
    font-weight: bold;
}

nav ul li a:hover {
    text-decoration: underline;
}

/* Flash Messages */
.flashes {
    list-style-type: none;
    padding: 0;
    color: red;
}

/* Search Form */
.search-field {
    display: flex;
    gap: 10px;
    margin-bottom: 15px;
    align-items: center;
}

input[type="text"], select {
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 4px;
    font-size: 16px;
}

button {
    padding: 10px 20px;
    background-color: #007bff;
    color: #fff;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s ease;
    font-size: 16px;
}

button:hover {
    background-color: #0056b3;
}

/* Navigation Buttons */
.nav-button {
    display: inline-block;
    padding: 8px 12px;
    margin-right: 5px;
    background-color: #007bff;
    color: #fff;
    text-decoration: none;
    border-radius: 4px;
    transition: background-color 0.3s ease;
    font-size: 14px;
    text-align: center;
}

.nav-button:hover {
    background-color: #0056b3;
}

.nav-button.disabled {
    background-color: #6c757d;
    pointer-events: none;
    cursor: default;
}

/* Results Table */
#results table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 20px;
}

#results th, #results td {
    padding: 12px;
    text-align: left;
    border-bottom: 1px solid #ddd;
}

#results th {
    background-color: #f0f0f0;
    font-weight: bold;
}

#results tbody tr:nth-child(odd) {
    background-color: #f9f9f9;
}

#results tbody tr:nth-child(even) {
    background-color: #ffffff;
}

#results tbody tr:hover {
    background-color: #eaeaea;
}

#results a {
    color: #007bff;
    text-decoration: none;
}

#results a:hover {
    text-decoration: underline;
}

/* Loading Indicator */
#loadingIndicator {
    text-align: center;
    margin-top: 20px;
}

.spinner {
    border: 4px solid #f3f3f3;
    border-top: 4px solid #3498db;
    border-radius: 50%;
    width: 40px;
    height: 40px;
    animation: spin 1s linear infinite;
    margin: 0 auto;
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

/* Additional Styles */
.mt-4 {
    margin-top: 1rem;
}

.ml-auto {
    margin-left: auto;
}

/* Error Messages */
.error {
    color: #dc3545;
    font-weight: bold;
}

/* Document Detail Styles */
.document-detail .detail-container {
    display: flex;
    gap: 20px;
}

.document-detail .info-panel {
    flex: 1;
}

.document-detail .image-panel {
    flex: 1;
    position: relative;
}

#imageContainer {
    overflow: auto;
    max-width: 100%;
    max-height: 600px;
}

#documentImage {
    max-width: 100%;
    max-height: 100%;
    transform-origin: top left;
}

.zoom-controls {
    margin-top: 10px;
}

.zoom-controls button {
    margin-right: 5px;
}

.info-section {
    margin-bottom: 20px;
    border: 1px solid #e0e0e0;
    border-radius: 5px;
    padding: 15px;
}

.info-table, .nested-table {
    width: 100%;
    border-collapse: collapse;
}

.info-table th, .info-table td,
.nested-table th, .nested-table td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
}

.info-table th, .nested-table th {
    background-color: #f2f2f2;
}

.nested-list, .info-list {
    list-style-type: none;
    padding-left: 0;
}

.nested-list li, .info-list li {
    margin-bottom: 10px;
}


********************************************************************************

File: app/static/script.js
********************************************************************************

document.addEventListener('DOMContentLoaded', function() {
    // ===============================
    // Initialization
    // ===============================
    const searchForm = document.getElementById('searchForm');
    const resultsDiv = document.getElementById('results');
    const loadingIndicator = document.getElementById('loadingIndicator');
    const cancelButton = document.getElementById('cancelSearch');
    const totalResultsDiv = document.getElementById('totalResults');
    const exportSelectedCsvButton = document.getElementById('exportSelectedCsv');

    // Add console warnings for missing elements
    if (!searchForm) console.warn('Search form not found');
    if (!resultsDiv) console.warn('Results div not found');
    if (!loadingIndicator) console.warn('Loading indicator not found');
    if (!cancelButton) console.warn('Cancel button not found');
    if (!totalResultsDiv) console.warn('Total results div not found');
    if (!exportSelectedCsvButton) console.warn('Export Selected CSV button not found');

    // Pagination and Search Variables
    let controller;
    let page = 1;
    let totalPages = 1;
    const perPage = 50;  // Fixed number of results per request
    let totalResults = 0;
    let isLoading = false;
    let hasMore = true;
    let currentQuery = {};
    let prefetchedData = null;

    // Selection Management
    let selectedDocuments = new Set();

    // Variable to store current search_id
    let searchId = null;

    // ===============================
    // Utility Functions
    // ===============================

    /**
     * Debounce function to limit the rate at which a function can fire.
     * @param {Function} func - The function to debounce.
     * @param {number} delay - The delay in milliseconds.
     * @returns {Function} - The debounced function.
     */
    function debounce(func, delay) {
        let timeoutId;
        return function(...args) {
            clearTimeout(timeoutId);
            timeoutId = setTimeout(() => func.apply(this, args), delay);
        };
    }

    /**
     * Toggles the selection of a document.
     * @param {string} docId - The ID of the document.
     * @param {boolean} isSelected - Whether the document is selected.
     */
    function toggleDocumentSelection(docId, isSelected) {
        if (isSelected) {
            selectedDocuments.add(docId);
        } else {
            selectedDocuments.delete(docId);
        }
        updateExportButtonVisibility();
        saveSelectedDocuments();
    }

    /**
     * Updates the visibility of the Export Selected button based on selections.
     */
    function updateExportButtonVisibility() {
        if (!exportSelectedCsvButton) return;
        if (selectedDocuments.size > 0) {
            exportSelectedCsvButton.style.display = 'inline-block';
        } else {
            exportSelectedCsvButton.style.display = 'none';
        }
    }

    /**
     * Saves the selected documents to localStorage.
     */
    function saveSelectedDocuments() {
        localStorage.setItem('selectedDocuments', JSON.stringify(Array.from(selectedDocuments)));
    }

    /**
     * Loads the selected documents from localStorage.
     */
    function loadSelectedDocuments() {
        const savedSelectedDocuments = JSON.parse(localStorage.getItem('selectedDocuments') || '[]');
        savedSelectedDocuments.forEach(id => selectedDocuments.add(id));
        updateExportButtonVisibility();
    }

    /**
     * Shows the loading indicator.
     */
    function showLoadingIndicator() {
        if (loadingIndicator && loadingIndicator.style) {
            loadingIndicator.style.display = 'block';
        }
    }

    /**
     * Hides the loading indicator.
     */
    function hideLoadingIndicator() {
        if (loadingIndicator && loadingIndicator.style) {
            loadingIndicator.style.display = 'none';
        }
    }

    /**
     * Resets the search parameters and UI elements.
     */
    function resetSearch() {
        page = 1;
        hasMore = true;
        if (resultsDiv) resultsDiv.innerHTML = '';
        if (totalResultsDiv) totalResultsDiv.textContent = '';
        searchId = null;  // Reset search_id for new search
    }

    /**
     * Gathers search parameters from the form.
     */
    function gatherSearchParameters() {
        const formData = new FormData(searchForm);
        currentQuery = {};
        for (let i = 1; i <= 3; i++) {
            currentQuery[`field${i}`] = formData.get(`field${i}`);
            currentQuery[`operator${i}`] = formData.get(`operator${i}`);
            currentQuery[`searchTerm${i}`] = formData.get(`searchTerm${i}`);
        }
    }

    /**
     * Validates the search parameters.
     * @returns {boolean} True if valid, else false.
     */
    function validateSearchParameters() {
        if (!currentQuery['field1'] || !currentQuery['searchTerm1']) {
            console.error('Please enter a valid search term in the first field.');
            alert('Please enter a valid search term in the first field.');
            return false;
        }
        return true;
    }

    /**
     * Handles the scroll event for infinite scrolling.
     */
    function handleScroll() {
        const scrollPosition = window.innerHeight + window.scrollY;
        const threshold = document.body.offsetHeight - 100;

        if (scrollPosition >= threshold) {
            if (prefetchedData) {
                // Use prefetched data
                appendResults(prefetchedData.documents);
                page += 1;
                totalPages = prefetchedData.total_pages;
                totalResults = prefetchedData.total_count;
                updateTotalResults();

                // Clear prefetched data and prefetch the next page
                prefetchedData = null;
                if (page <= totalPages) {
                    prefetchNextPage();
                } else {
                    hasMore = false;
                }
            } else {
                performSearch();
            }
        } else if (prefetchedData === null && (scrollPosition >= threshold / 2)) {
            // Start prefetching when user scrolls halfway
            prefetchNextPage();
        }
    }

    // ===============================
    // Event Listeners
    // ===============================

    // Handle form submission for search
    if (searchForm) {
        searchForm.addEventListener('submit', function(e) {
            e.preventDefault();
            resetSearch();
            gatherSearchParameters();
            if (validateSearchParameters()) {
                performSearch(true);
            }
        });
    }

    // Handle checkbox changes using event delegation
    if (resultsDiv) {
        resultsDiv.addEventListener('change', function(e) {
            if (e.target && e.target.matches('.select-document')) {
                const docId = e.target.getAttribute('data-doc-id');
                toggleDocumentSelection(docId, e.target.checked);
            }
        });
    }

    // Handle "Select All" functionality
    if (resultsDiv) {
        resultsDiv.addEventListener('change', function(e) {
            if (e.target && e.target.matches('#selectAll')) {
                const checkboxes = resultsDiv.querySelectorAll('.select-document');
                const isChecked = e.target.checked;
                checkboxes.forEach(cb => {
                    cb.checked = isChecked;
                    const docId = cb.getAttribute('data-doc-id');
                    if (isChecked) {
                        selectedDocuments.add(docId);
                    } else {
                        selectedDocuments.delete(docId);
                    }
                });
                updateExportButtonVisibility();
                saveSelectedDocuments();
            }
        });
    }

    // Handle Export Selected to CSV Button Click
    if (exportSelectedCsvButton) {
        exportSelectedCsvButton.addEventListener('click', function() {
            exportSelectedDocuments();
        });
    }

    // Handle Cancel Search Button Click
    if (cancelButton) {
        cancelButton.addEventListener('click', function() {
            cancelSearch();
        });
    }

    // Handle Infinite Scroll with Debounce
    window.addEventListener('scroll', debounce(handleScroll, 200));

    // ===============================
    // Search Functionality
    // ===============================

    /**
     * Performs the search by sending a POST request to the server.
     * @param {boolean} isNewSearch - Indicates if it's a new search.
     */
    function performSearch(isNewSearch = false) {
        if (isLoading || !hasMore) return;
        isLoading = true;

        if (isNewSearch) {
            prefetchedData = null;
            searchId = null;  // Reset search_id for new search
        }

        showLoadingIndicator();
        if (cancelButton) cancelButton.style.display = 'inline-block';

        // Add page and perPage to currentQuery
        currentQuery.page = page;
        currentQuery.per_page = perPage;

        controller = new AbortController();
        const signal = controller.signal;

        fetch('/search', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(currentQuery),
            signal: signal
        })
        .then(response => response.json())
        .then(data => {
            hideLoadingIndicator();
            if (cancelButton) cancelButton.style.display = 'none';
            if (data.documents && data.documents.length > 0) {
                // Store search_id if it's a new search
                if (isNewSearch) {
                    searchId = data.search_id;
                }

                appendResults(data.documents);
                totalPages = data.total_pages;
                totalResults = data.total_count;

                // Prefetch the next page if there are more pages
                if (page < totalPages) {
                    page += 1;
                    prefetchNextPage();
                } else {
                    hasMore = false;
                }
            } else {
                hasMore = false;
                if (isNewSearch && resultsDiv && resultsDiv.innerHTML === '') {
                    resultsDiv.innerHTML = '<p>No results found.</p>';
                }
            }
            updateTotalResults();
            isLoading = false;
        })
        .catch(error => {
            hideLoadingIndicator();
            if (cancelButton) cancelButton.style.display = 'none';
            isLoading = false;
            if (error.name === 'AbortError') {
                console.log('Search was cancelled');
            } else {
                console.error('Error:', error);
                alert('An error occurred during the search. Please try again.');
            }
        });
    }

    /**
     * Appends search results to the resultsDiv.
     * @param {Array} documents - List of document objects.
     */
    function appendResults(documents) {
        if (!resultsDiv) return;

        let table = document.getElementById('resultsTable');
        let tbody;

        // If the table doesn't exist yet, create it
        if (!table) {
            table = document.createElement('table');
            table.id = 'resultsTable';

            // Create table headers with a Select All checkbox
            const thead = document.createElement('thead');
            thead.innerHTML = `
                <tr>
                    <th><input type="checkbox" id="selectAll" /></th>
                    <th>File</th>
                    <th>Summary</th>
                </tr>
            `;
            table.appendChild(thead);

            // Create table body
            tbody = document.createElement('tbody');
            table.appendChild(tbody);

            // Append the table to the results div
            resultsDiv.appendChild(table);
        } else {
            // If the table exists, get its tbody
            tbody = table.querySelector('tbody');
        }

        // Append new rows to the table body
        documents.forEach(doc => {
            const row = document.createElement('tr');
            row.innerHTML = `
                <td><input type="checkbox" class="select-document" data-doc-id="${doc._id}" /></td>
                <td><a href="/document/${doc._id}?search_id=${searchId}">${doc.filename || 'No file name'}</a></td>
                <td>${doc.summary || 'No summary available.'}</td>
            `;
            // If the document is already selected, check the box
            if (selectedDocuments.has(doc._id)) {
                row.querySelector('.select-document').checked = true;
            }
            tbody.appendChild(row);
        });
    }

    /**
     * Updates the total results display.
     */
    function updateTotalResults() {
        if (totalResultsDiv) {
            totalResultsDiv.textContent = `Total results: ${totalResults}`;
        }
    }

    /**
     * Prefetches the next page of results.
     */
    function prefetchNextPage() {
        if (prefetchedData || !hasMore) return;

        const prefetchQuery = { ...currentQuery, page: page };

        fetch('/search', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(prefetchQuery),
        })
        .then(response => response.json())
        .then(data => {
            if (data.documents && data.documents.length > 0) {
                prefetchedData = data;
            } else {
                hasMore = false;
            }
        })
        .catch(error => {
            console.error('Error during prefetching:', error);
        });
    }

    // ===============================
    // Selection Management
    // ===============================

    /**
     * Exports the selected documents to CSV.
     */
    function exportSelectedDocuments() {
        if (selectedDocuments.size === 0) {
            alert('No documents selected.');
            return;
        }

        // Prepare the list of selected document IDs
        const selectedIds = Array.from(selectedDocuments);

        // Show a loading modal or notification
        showExportModal('Exporting selected documents...');

        // Send the list to the server via POST
        fetch('/export_selected_csv', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ document_ids: selectedIds }),
        })
        .then(response => {
            if (response.ok) {
                return response.blob();
            } else {
                return response.json().then(err => { throw err; });
            }
        })
        .then(blob => {
            // Create a link to download the blob
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'selected_documents.csv';
            document.body.appendChild(a);
            a.click();
            a.remove();
            window.URL.revokeObjectURL(url);

            // Hide the export modal and show success message
            hideExportModal();
            showExportModal('Export successful!', 'success');

            // Reset the export button
            exportSelectedCsvButton.disabled = false;
            exportSelectedCsvButton.textContent = 'Export Selected to CSV';

            // Optionally, clear the selectedDocuments set
            selectedDocuments.clear();
            updateExportButtonVisibility();

            // Uncheck all checkboxes
            const checkboxes = resultsDiv.querySelectorAll('.select-document');
            checkboxes.forEach(cb => cb.checked = false);

            // Uncheck "Select All" checkbox
            const selectAllCheckbox = document.getElementById('selectAll');
            if (selectAllCheckbox) {
                selectAllCheckbox.checked = false;
            }

            // Remove success message after a short delay
            setTimeout(() => {
                hideExportModal();
            }, 3000);
        })
        .catch(error => {
            console.error('Error exporting selected documents:', error);
            hideExportModal();
            alert('Error exporting selected documents.');

            // Reset the export button
            exportSelectedCsvButton.disabled = false;
            exportSelectedCsvButton.textContent = 'Export Selected to CSV';
        });
    }

    /**
     * Cancels the ongoing search.
     */
    function cancelSearch() {
        if (controller) {
            controller.abort();
            hideLoadingIndicator();
            isLoading = false;
            hasMore = false;
            if (cancelButton) cancelButton.style.display = 'none';
        }
    }

    // ===============================
    // Export Feedback Mechanism
    // ===============================

    /**
     * Creates and displays an export modal for feedback.
     * @param {string} message - The message to display.
     * @param {string} type - The type of message ('info', 'success', 'error').
     */
    function showExportModal(message, type = 'info') {
        // Check if modal already exists
        let modal = document.getElementById('exportModal');
        if (!modal) {
            modal = document.createElement('div');
            modal.id = 'exportModal';
            modal.innerHTML = `
                <div class="modal-content">
                    <span class="close-button">&times;</span>
                    <p id="exportMessage">${message}</p>
                </div>
            `;
            document.body.appendChild(modal);

            // Style the modal
            const style = document.createElement('style');
            style.textContent = `
                #exportModal {
                    display: block; 
                    position: fixed; 
                    z-index: 1000; 
                    left: 0;
                    top: 0;
                    width: 100%; 
                    height: 100%; 
                    overflow: auto; 
                    background-color: rgba(0,0,0,0.4); 
                }
                .modal-content {
                    background-color: #fefefe;
                    margin: 15% auto; 
                    padding: 20px;
                    border: 1px solid #888;
                    width: 300px; 
                    text-align: center;
                    border-radius: 5px;
                }
                .close-button {
                    color: #aaa;
                    float: right;
                    font-size: 28px;
                    font-weight: bold;
                    cursor: pointer;
                }
                .close-button:hover,
                .close-button:focus {
                    color: black;
                    text-decoration: none;
                }
                .modal-content.success {
                    border-color: #28a745;
                }
                .modal-content.error {
                    border-color: #dc3545;
                }
            `;
            document.head.appendChild(style);

            // Handle close button click
            modal.querySelector('.close-button').addEventListener('click', hideExportModal);
        }

        // Update the message and style based on type
        const exportMessage = modal.querySelector('#exportMessage');
        exportMessage.textContent = message;
        modal.querySelector('.modal-content').className = 'modal-content'; // Reset classes

        if (type === 'success') {
            modal.querySelector('.modal-content').classList.add('success');
        } else if (type === 'error') {
            modal.querySelector('.modal-content').classList.add('error');
        }

        // Display the modal
        modal.style.display = 'block';
    }

    /**
     * Hides the export modal.
     */
    function hideExportModal() {
        const modal = document.getElementById('exportModal');
        if (modal) {
            modal.style.display = 'none';
        }
    }

    // ===============================
    // Initial Load
    // ===============================

    // Load selected documents from localStorage
    loadSelectedDocuments();
});


********************************************************************************

