Project File Combination
Generated on: 2024-10-21 13:06:11.381209

Total files: 65
File type counts:
  .md: 3
  .py: 14
  .html: 9
  .css: 1
  .js: 1

File Structure:
../
    todo.md
    readme.md
    requirements.txt
    restore_db.py
    .env
    .gitignore
    docker-compose.yml
    combined_project_files.txt
    backup_db.py
    .dockerignore
    export_unique_terms.log
    design_document_for_hpc_server_link_files/
        colorschememapping.xml
        filelist.xml
        themedata.thmx
    mongo-init/
    app/
        database_setup.py
        database_processing.log
        app.py
        entity_linking.log
        requirements.in
        ner_processing.prof
        test_db_connection.py
        show_env.py
        export_linked_terms.log
        entity_processing.log
        requirements.txt
        secret_key.txt
        config.json
        generate_unique_terms.prof
        models.py
        show_env.prof
        database_setup.log
        data_processing.prof
        data_processing.py
        .env
        entity_linking.prof
        routes.log
        database_setup.prof
        ner_worker.py
        generate_unique_terms.log
        ner_processing.py
        entrypoint.sh
        generate_unique_terms.py
        routes.py
        export_unique_terms.log
        Dockerfile
        test_spacy.py
        unique_terms_export.csv
        entity_linking.py
        profile_results.txt
        entity_processing_worker.log
        templates/
            index.html
            search-terms.html
            settings.html
            error.html
            login.html
            base.html
            database-info.html
            document-detail.html
            document-list.html
        static/
            style.css
            script.js
    docs/
        design_document.md
        design_document_for_hpc_server_link.pdf
        design_document_for_hpc_server_link.htm
        design_document_for_hpc_server_link.rtf
        ~$sign_document_for_hpc_server_link.htm

********************************************************************************

File: todo.md
********************************************************************************

todo.txt

## To Do


DEBUG PAPER IMAGES

SEARCH BOX FOR UNIQUE TERMs








to access the shell
docker compose exec -it flask_app /bin/bash


for mongodb
docker compose exec -it mongodb /bin/bash
mongosh mongodb://admin:secret@localhost:27017/admin


to drop

db.documents.updateMany(
    { "entities_processed": { "$exists": true } }, // Filter for documents with the field
    { "$unset": { "entities_processed": "" } } // Remove the field
)

see CDIST for faster fuzzy matching


allow fo rthe use of fuzzy matching:
{
    _id: ObjectId('670ff5f0ab41bbc86731de5a'),
    term: '.87',
    document_ids: [
      ObjectId('670ff1ec0cc27d75bc5d2559'),
      ObjectId('670ff1fc0cc27d75bc5d36f5'),
      ObjectId('670ff22a0cc27d75bc5d4e0e'),
      ObjectId('670ff1ed0cc27d75bc5d279e'),
      ObjectId('670ff1e60cc27d75bc5d1bd2')
    ],
    frequency: 15,
    kb_id: null,
    type: 'ORG'
  },



AI linking
AI assited document retrieval (RAG LLM)
Geographic features
REcord linking across datasets
Full ingestion of all teh data
Restricted searches
SQL injection attacks
security


history topics:
reprimands


woman

corrections

crowdsource corrections to the db

next roll image
next folder

shift from remaking a connection to passing a connection (but how does that work with multiprocessing?)
SummaryThe profiling output indicates that your script is heavily involved in I/O operations, particularly through the poll method of select.poll objects. To address this:Reuse the MongoDB client to minimize connection overhead.Optimize batch sizes and database operations to reduce the number of I/O calls.Enhance logging efficiency to prevent excessive I/O wait times.Consider asynchronous programming paradigms if applicable.
PRIORITIES
6. Add cross-referencing of named entities
debug env file and add scrip that prints env variables
STATUS: linking seems to work. unique terms no longer works. need to check or revert or soemthing.

revert to earlier unique items

named entity, reprocessing, and fuzzy matching

1. reprocess and add NER to documents, priomed for fuzzy matching
2. clik to mfuzzy match and open new document


            spacey? spacey vs AI
            how to reconcile different named entities?

            first entry or something?

            toponyms

            forced to see something not looking for

            fuzzy matching

            click on that and open a new tab and show all the files


            head injury, missing pictures

            remove blur

            add contrast


            llama 3.1

            move documents name to .env file







##Feature to do list
0. Shift other PC to Docker

1. Convert setup process to a part of settings or a new page. It should be able to add to the DB
2. Create login splash page

4. Address weirdness of base file and index.html. It is unseemly 


7. Restore adding export from list
8. Add "Select all" for export
9. Clean file description to remove extension in title
10. Create a way to do a search and then add that result to the DB
11. Backup and restore database

13. Color code sections of JSON expansion
14. Need to implement a sort for the file results, so that this is in order: 	File	Summary
	RDApp-630550Fox053.jpg.json	The document contains a handwritten signature of an individual named M. Johnson, along with the year 1919.
	RDApp-630550Fox072.jpg.json	This document is a surgeon's first report of an accident for the Baltimore & Ohio Railroad-Relief Department. It details an incident involving an individual named E.S. Fry, a laborer, who resides in All Around O. The report notes injuries sustained: a contusion and a cut lip, with the mention of a broken face due to a tool. The probable duration of disablement is stated to be short. Additionally, it provides a brief account of how the accident occurred, indicating involvement with a train.
	RDApp-630550Fox059.jpg.json	The document appears to be a handwritten note addressed to Mr. Martin from someone requesting approval from Dr. Smith, a company surgeon, regarding a matter likely related to medical or health concerns.
	RDApp-630550Fox014.jpg.json	This document is a correspondence from the Office of General Claim Agent of The Baltimore and Ohio Railroad Company, dated December 8, 1920. It refers to a bill from The Peoples Hospital for services rendered to E. L. Fox, a train rider who was injured at Cuyahoga Falls, Ohio, on October 31, 1920. The bill is being sent to Mr. W. J. Dudley, Superintendent of the Relief Department, for voucher processing. The document indicates that Mr. Fox was a member of the Relief Department at the time of his injury. Additionally, there is a note referencing a letter related to this bill dated 16th of December, 1920.
	RDApp-630550Fox062.jpg.json	This document is a telegram from the Baltimore and Ohio Railroad Company to the Superintendent of City Hospital in Akron, Ohio, dated August 5th, 1921. It refers to a bill concerning an individual named E. L. Fox and requests further communication regarding the matter.


##Notes on how to
1. Inside the util folder is delete_db.py which needs to be run from inside a container in order to delete the database. You will need to delete the database if you change the setup or structure.
   a. docker exec -it flask_app /bin/bash
   b. util/delete_db.py



********************************************************************************

File: readme.md
********************************************************************************

# Historical Document Reader
v0.1

## Description
The Historical Document Reader is a Flask-based web application designed to manage, search, and display historical documents stored in a MongoDB database. It provides an intuitive interface for researchers and historians to access and analyze digitized historical records.

## Repository
The project is hosted on GitHub at:
https://github.com/proflouishyman/nosql_reader

## Features

- Document Management: Data ingestion from JSON files, dynamic field structure discovery
- Search Functionality: Advanced search with multiple fields and logical operators
- Document Viewing: Detailed view with image zoom, pan, and enhancement capabilities
- Data Analysis: Search terms analysis with word and phrase frequency
- User Interface: Customizable UI settings, responsive design
- Data Export: Export search results to CSV
- Security: Basic authentication system with CAPTCHA
- Docker Support: Containerized application setup

## File Structure

```
railroad_documents_project/
├── app/
│   ├── app.py
│   ├── routes.py
│   ├── models.py
│   ├── database_setup.py
│   ├── data_processing.py
│   ├── json_validator.py
│   ├── json_validator_multi.py
│   ├── generate_password.py
│   ├── chunk_utils.py
│   ├── generate_unique_terms.py
│   ├── test_mongo_connection.py
│   ├── requirements.txt
│   ├── config.json
│   ├── secret_key.txt
│   ├── README.md
│   ├── static/
│   │   ├── script.js
│   │   └── style.css
│   └── templates/
│       ├── base.html
│       ├── index.html
│       ├── document-detail.html
│       ├── document-list.html
│       ├── search-terms.html
│       ├── database-info.html
│       ├── settings.html
│       ├── login.html
│       └── error.html
├── mongo-init/
│   └── init_script.js
├── entrypoint.sh
├── Dockerfile
├── docker-compose.yml
├── .env
├── .dockerignore
└── README.md
└── archives/
    └── [Your document files go here]


```

## Installation and Setup

1. Clone the repository:
   ```
   git clone https://github.com/proflouishyman/nosql_reader.git
   ```

2. Set up the environment:
   - Copy the `.env.example` file to `.env` and update the variables as needed.
   - Ensure Docker and Docker Compose are installed on your system.

3. Build and run the Docker containers:
   ```
   docker-compose up --build
   ```

4. Access the application at `http://localhost:5000`.

## Usage

1. Log in using the generated admin password (see `generate_password.py`).
2. Use the search interface to find documents.
3. View search results and document details.
4. Analyze search terms and view database information.
5. Customize the interface in the Settings page.
6. Export results to CSV as needed.

## Docker Setup

The application is containerized using Docker. The `docker-compose.yml` file defines the services:
- `flask_app`: The main Flask application
- `mongodb`: The MongoDB database
- `mongo-express`: A web-based MongoDB admin interface (optional)

To start the application:
```
docker-compose up
```

To rebuild the containers after making changes:
```
docker-compose up --build
```

## Maintenance and Updates

- Add new documents by placing JSON files in the `app/archives` directory and running `data_processing.py` within the container.
- The system automatically adapts to changes in document structure.
- Use `generate_unique_terms.py` to update the unique terms collection.
- Regularly backup your MongoDB database using MongoDB's backup tools.

## Troubleshooting

- For database connection issues, use `test_mongo_connection.py` to verify the connection.
- Check Docker logs for each service:
  ```
  docker-compose logs flask_app
  docker-compose logs mongodb
  ```
- Verify JSON format using `json_validator.py` or `json_validator_multi.py` for batch processing.
-to access the shell
'''
docker compose exec -it flask_app /bin/bash
'''

-for mongodb
'''
docker compose exec -it mongodb /bin/bash
mongosh mongodb://admin:secret@localhost:27017/admin
'''


## Contributing

1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Make changes and commit with descriptive messages.
4. Push changes to your fork.
5. Submit a pull request to the main repository.

## License

[Insert your chosen license here]

## Contact

[Your Name or Organization]
[Contact Information]

********************************************************************************

File: restore_db.py
********************************************************************************

# restore_db.py
# Date created: 2024-10-19
# Purpose: This script restores the MongoDB database 'railroad_documents' from the most recent backup found in the ../db_backup/ directory.

import os
import subprocess
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

# Get MongoDB credentials from environment variables
username = os.getenv("MONGO_INITDB_ROOT_USERNAME")
password = os.getenv("MONGO_INITDB_ROOT_PASSWORD")
database_name = "railroad_documents"  # Change this if needed
backup_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "db_backup")

# Find the most recent backup directory
def get_latest_backup_dir(backup_root):
    backup_dirs = [d for d in os.listdir(backup_root) if os.path.isdir(os.path.join(backup_root, d))]
    if not backup_dirs:
        print("No backup directories found.")
        return None
    latest_backup = max(backup_dirs, key=lambda d: os.path.getmtime(os.path.join(backup_root, d)))
    return os.path.join(backup_root, latest_backup)

# Get the latest backup directory
latest_backup_dir = get_latest_backup_dir(backup_root)

if latest_backup_dir:
    # Create the mongorestore command
    command = [
        "mongorestore",
        f"--uri=mongodb://{username}:{password}@localhost:27017/{database_name}?authSource=admin",
        latest_backup_dir
    ]

    # Execute the restore command
    try:
        subprocess.run(command, check=True)
        print(f"Database '{database_name}' restored successfully from {latest_backup_dir}.")
    except subprocess.CalledProcessError as e:
        print(f"Restore failed: {e}")
else:
    print("No backup to restore from.")


********************************************************************************

File: backup_db.py
********************************************************************************

# backup_db.py
# Date created: 2024-10-19
# Purpose: This script creates a backup of the MongoDB database 'railroad_documents' using credentials from a .env file and reports the backup size.

import os
import subprocess
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

# Get MongoDB credentials from environment variables
username = os.getenv("MONGO_INITDB_ROOT_USERNAME")
password = os.getenv("MONGO_INITDB_ROOT_PASSWORD")
database_name = "railroad_documents"  # Change this if needed
backup_root = os.path.join(os.path.dirname(os.path.dirname(__file__)), "db_backup")

# Create a timestamped directory
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
backup_dir = os.path.join(backup_root, f"backup_{timestamp}")

# Create the backup directory if it doesn't exist
os.makedirs(backup_dir, exist_ok=True)

# Create the mongodump command
command = [
    "mongodump",
    f"--uri=mongodb://{username}:{password}@localhost:27017/{database_name}?authSource=admin",
    f"--out={backup_dir}"
]

# Execute the backup command
try:
    subprocess.run(command, check=True)
    print(f"Backup successful! Files are stored in: {backup_dir}")

    # Calculate the size of the backup directory
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(backup_dir):
        for file in filenames:
            fp = os.path.join(dirpath, file)
            total_size += os.path.getsize(fp)

    # Convert size to MB
    total_size_mb = total_size / (1024 * 1024)
    print(f"Total backup size: {total_size_mb:.2f} MB")
except subprocess.CalledProcessError as e:
    print(f"Backup failed: {e}")


********************************************************************************

File: app/database_setup.py
********************************************************************************

# database_setup.py

from pymongo import MongoClient, ASCENDING, DESCENDING, UpdateOne
from bson import ObjectId
import logging
import os
from dotenv import load_dotenv
import pymongo
from pymongo import UpdateOne

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
# Create a logger
logger = logging.getLogger('DatabaseSetupLogger')
logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs

# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)  # Show only warnings and above in console

file_handler = logging.FileHandler('database_setup.log')
file_handler.setLevel(logging.DEBUG)  # Capture all debug and higher level logs in file

# Create formatter
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# Set formatter for handlers
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# Add handlers to the logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)

# =======================
# Database Functions
# =======================

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        
        mongo_uri = os.environ.get('MONGO_URI') # this SHOULD read from .env file but sometimes does not.
        #mongo_uri= "mongodb://admin:secret@mongodb:27017/admin" # this is correct and needs the /admin for the administrative db. REMOVE before prduction
        #print(mongo_uri)

        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=1000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']


def clean_unique_terms_collection(db):
    """Remove documents with null term, field, or type."""
    result = db['unique_terms'].delete_many({
        "$or": [
            {"term": None},
            {"field": None},
            {"type": None}
        ]
    })
    logger.info(f"Removed {result.deleted_count} documents with null term, field, or type.")



def initialize_database(client):
    db = get_db(client)
    # Create collections if they don't exist
    for collection_name in ['documents', 'unique_terms', 'field_structure']:
        if collection_name not in db.list_collection_names():
            db.create_collection(collection_name)
            logger.info(f"Created collection: {collection_name}")
    
  
    # Clean unique_terms collection
    clean_unique_terms_collection(db)


    # Create necessary indexes
    documents = db['documents']
    existing_indexes = documents.index_information()
    if 'file_hash_1' not in existing_indexes:
        documents.create_index([("file_hash", 1)], unique=True)
        logger.info("Created unique index on 'file_hash' in 'documents' collection.")
    
    unique_terms = db['unique_terms']
    # Unique compound index on term, field, and type
    unique_terms.create_index(
        [("term", ASCENDING), ("field", ASCENDING), ("type", ASCENDING)],
        unique=True,
        name="unique_term_field_type"
    )
    logger.info("Created unique compound index on 'term', 'field', and 'type' in 'unique_terms' collection.")
    
    # Compound index on field, type, and frequency for efficient sorting
    unique_terms.create_index(
        [("field", ASCENDING), ("type", ASCENDING), ("frequency", DESCENDING)],
        name="field_type_frequency_idx"
    )
    logger.info("Created compound index on 'field', 'type', and 'frequency' in 'unique_terms' collection.")
    
    field_structure = db['field_structure']
    field_structure.create_index([("field", 1)], unique=True)
    logger.info("Created unique index on 'field' in 'field_structure' collection.")


    # Create unique index on 'term' in 'linked_entities' collection
    linked_entities = db['linked_entities']
    existing_le_indexes = linked_entities.index_information()
    if 'term_1' not in existing_le_indexes:
        linked_entities.create_index([("term", ASCENDING)], unique=True)
        logger.info("Created unique index on 'term' in 'linked_entities' collection.")
    
    
    logger.info("Database initialized with required collections and indexes.")


def insert_document(db, document):
    """Insert a document into the 'documents' collection."""
    try:
        documents = db['documents']
        documents.insert_one(document)
        logger.info("Inserted document into 'documents' collection.")
    except Exception as e:
        logger.error(f"Error inserting document: {e}")
        raise e
    
    
def discover_fields(document):
    """
    Recursively discover fields in a document.
    :param document: The document to analyze
    :return: A dictionary representing the field structure
    """
    structure = {}
    for key, value in document.items():
        if isinstance(value, dict):
            structure[key] = discover_fields(value)
        elif isinstance(value, list):
            if value:
                if isinstance(value[0], dict):
                    structure[key] = [discover_fields(value[0])]
                else:
                    structure[key] = [type(value[0]).__name__]
            else:
                structure[key] = []
        else:
            structure[key] = type(value).__name__
    return structure

def merge_structures(existing, new):
    """
    Merge two field structures.
    :param existing: The existing field structure
    :param new: The new field structure to merge
    :return: The merged field structure
    """
    for key, value in new.items():
        if key not in existing:
            existing[key] = value
        elif isinstance(value, dict) and isinstance(existing[key], dict):
            merge_structures(existing[key], value)
        elif isinstance(value, list) and isinstance(existing[key], list):
            if value and existing[key]:
                if isinstance(value[0], dict) and isinstance(existing[key][0], dict):
                    merge_structures(existing[key][0], value[0])
    return existing

def update_field_structure(db, document):
    """
    Update the field structure based on a new document.
    :param db: Database instance
    :param document: The new document to analyze
    """
    field_structure_collection = db['field_structure']
    new_structure = discover_fields(document)
    merged_structure = {}

    # Attempt to retrieve the existing structure
    existing_structure = field_structure_collection.find_one({"_id": "current_structure"})

    if existing_structure:
        # Merge the new structure with the existing one
        merged_structure = merge_structures(existing_structure['structure'], new_structure)
    else:
        # If no existing structure, use the new structure
        merged_structure = new_structure

    # Perform an upsert operation to update or insert the structure
    field_structure_collection.update_one(
        {"_id": "current_structure"},
        {"$set": {"structure": merged_structure}},
        upsert=True
    )


def is_file_ingested(db, file_hash):
    """Check if a file has already been ingested based on its hash."""
    if not file_hash:
        return False
    try:
        documents = db['documents']
        ingested = documents.find_one({'file_hash': file_hash}) is not None
        logger.debug(f"File ingestion check for hash {file_hash}: {ingested}")
        return ingested
    except Exception as e:
        logger.error(f"Error checking ingestion status for hash {file_hash}: {e}")
        return False
def save_unique_terms(db, unique_terms_counter):
    """
    Save the unique terms counter to the database as individual documents.
    :param db: Database instance
    :param unique_terms_counter: Nested dictionary {field: {type: Counter()}}
    """
    unique_terms_collection = db['unique_terms']
    
    # Log the unique_terms_counter content
    logger.debug(f"Unique terms counter before saving: {unique_terms_counter}")
    
    # Prepare bulk operations
    operations = []
    for field, types in unique_terms_counter.items():
        if not field:
            logger.warning("Encountered a null or empty field. Skipping.")
            continue
        for term_type, counter in types.items():
            if not term_type:
                logger.warning("Encountered a null or empty type. Skipping.")
                continue
            for term, freq in counter.items():
                if not term:
                    logger.warning("Encountered a null or empty term. Skipping.")
                    continue
                operations.append(
                    pymongo.UpdateOne(
                        {"term": term, "field": field, "type": term_type},
                        {"$set": {"frequency": freq}},
                        upsert=True
                    )
                )
    
    logger.debug(f"Prepared {len(operations)} bulk operations for unique_terms.")
    
    if operations:
        try:
            result = unique_terms_collection.bulk_write(operations, ordered=False)
            logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} unique terms.")
        except Exception as e:
            logger.error(f"Error bulk upserting unique terms: {e}")
            raise e
    else:
        logger.warning("No unique terms to upsert.")






def update_document(db, document_id, update_data):
    """
    Update a document's information.
    :param db: Database instance
    :param document_id: The ObjectId of the document to update
    :param update_data: A dictionary containing the fields to update
    :return: The number of documents modified
    """
    try:
        documents = db['documents']
        result = documents.update_one({"_id": ObjectId(document_id)}, {"$set": update_data})
        if result.matched_count == 0:
            logger.warning(f"No document found with _id: {document_id}")
        else:
            logger.info(f"Updated document with _id: {document_id}")
        return result.modified_count
    except Exception as e:
        logger.error(f"Error updating document {document_id}: {e}")
        raise e

def delete_document(db, document_id):
    """
    Delete a document from the database.
    :param db: Database instance
    :param document_id: The ObjectId of the document to delete
    :return: The number of documents deleted
    """
    try:
        documents = db['documents']
        result = documents.delete_one({"_id": ObjectId(document_id)})
        if result.deleted_count == 0:
            logger.warning(f"No document found with _id: {document_id}")
        else:
            logger.info(f"Deleted document with _id: {document_id}")
        return result.deleted_count
    except Exception as e:
        logger.error(f"Error deleting document {document_id}: {e}")
        raise e
    
def get_field_structure(db):
    """
    Get the current field structure.
    :param db: Database instance
    :return: The current field structure
    """
    field_structure_collection = db['field_structure']
    structure = field_structure_collection.find_one({"_id": "current_structure"})
    return structure['structure'] if structure else {}


def find_document_by_id(db, document_id):
    """
    Find a document by its ObjectId.
    :param db: Database instance
    :param document_id: The ObjectId of the document
    :return: The document, or None if not found
    """
    try:
        documents = db['documents']
        document = documents.find_one({"_id": ObjectId(document_id)})
        if document:
            logger.info(f"Found document with _id: {document_id}")
            return document
        else:
            logger.warning(f"No document found with _id: {document_id}")
            return None
    except Exception as e:
        logger.error(f"Error finding document {document_id}: {e}")
        raise e

def get_collections(db):
    """Retrieve and return references to the required collections."""
    try:
        documents = db['documents']
        unique_terms_collection = db['unique_terms']
        field_structure_collection = db['field_structure']
        return documents, unique_terms_collection, field_structure_collection
    except Exception as e:
        logger.error(f"Error getting collections: {e}")
        raise

# =======================
# Main Execution (Optional)
# =======================
if __name__ == "__main__":
    client = get_client()  # Get the MongoDB client
    db = get_db(client)
    initialize_database(client)  # Initialize the database
    logger.info("Database setup module executed directly.")


********************************************************************************

File: app/app.py
********************************************************************************

# File: app.py
# Path: railroad_documents_project/app.py

import os
import json
from flask import Flask
from flask_caching import Cache
from flask_session import Session
from database_setup import (
    get_client,
    get_db,
    get_collections,
    insert_document,
    update_document,
    delete_document,
    get_field_structure,
    find_document_by_id,
    update_field_structure,
    save_unique_terms
)


import logging
from logging.handlers import RotatingFileHandler

app = Flask(__name__)



# Setup console logging
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# Add the console handler to the app's logger
app.logger.addHandler(console_handler)

# # Setup file-based logging
# if not app.debug:
#     file_handler = RotatingFileHandler('logs/app.log', maxBytes=10240, backupCount=10)
#     file_handler.setLevel(logging.DEBUG)
#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#     file_handler.setFormatter(formatter)
#     app.logger.addHandler(file_handler)

app.logger.setLevel(logging.DEBUG)

# Load configuration
config_path = os.path.join(os.path.dirname(__file__), 'config.json')

try:
    with open(config_path) as config_file:
        config = json.load(config_file)
    app.config['UI_CONFIG'] = config
except FileNotFoundError:
    app.logger.error(f"Configuration file not found at {config_path}.")
    config = {}
except json.JSONDecodeError as e:
    app.logger.error(f"Error decoding JSON from {config_path}: {e}")
    config = {}


# Add config to app config
app.config['UI_CONFIG'] = config

# Print out the template folder path for debugging
print(f"Template folder path: {app.template_folder}")

# Session configuration
app.config['SESSION_TYPE'] = 'filesystem'
app.config['SESSION_PERMANENT'] = False
app.config['SESSION_USE_SIGNER'] = True
app.config['SESSION_KEY_PREFIX'] = 'historical_document_reader'

def get_secret_key():
    secret_file = os.path.join(app.root_path, 'secret_key.txt')
    if os.path.exists(secret_file):
        with open(secret_file, 'r') as f:
            return f.read().strip()
    else:
        import secrets
        generated_key = secrets.token_hex(16)
        with open(secret_file, 'w') as f:
            f.write(generated_key)
        return generated_key

# Set the secret key
app.secret_key = get_secret_key()

# Initialize extensions
cache = Cache(config={'CACHE_TYPE': 'simple'})
cache.init_app(app)
Session(app)

def load_config():
    """
    Load the configuration from the JSON file.
    This function is called on each request to allow for dynamic UI configuration.
    """
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')
    with open(config_path) as config_file:
        return json.load(config_file)

@app.context_processor
def inject_ui_config():
    """
    Inject the UI configuration and field structure into all templates.
    This allows for dynamic UI customization without needing to pass the config to each template.
    """
    app.config['UI_CONFIG'] = load_config()
    client = get_client()
    db = get_db(client)
    field_struct = get_field_structure(db)
    return dict(ui_config=app.config['UI_CONFIG'], field_structure=field_struct)
#rewrite
# Import routes after initializing app to avoid circular imports
from routes import *

if __name__ == '__main__':
    debug_mode = os.environ.get('FLASK_DEBUG', '0') == '1'
    app.run(debug=debug_mode, host='0.0.0.0', port=5000)


********************************************************************************

File: app/test_db_connection.py
********************************************************************************

# test_db_connection.py

import os
import logging
from pymongo import MongoClient
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Logging Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('TestDBLogger')

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        mongo_uri = os.environ.get('MONGO_URI')
        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        print(mongo_uri)
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']  # Adjust if your database name is different

def main():
    try:
        
        client = get_client()
        db = get_db(client)
        unique_terms_collection = db['unique_terms']

        # Check the total number of documents
        total_docs = unique_terms_collection.count_documents({})
        print(f"Total documents in unique_terms: {total_docs}")

        # Retrieve a sample document
        sample_doc = unique_terms_collection.find_one()
        print("\nSample document from unique_terms collection:")
        print(sample_doc)

        # Retrieve distinct 'type' field values
        type_values = unique_terms_collection.distinct('type')
        print("\nDistinct 'type' values in unique_terms collection:")
        print(type_values)

        # Count documents for each 'type' value
        print("\nDocument count for each 'type' value:")
        for type_value in type_values:
            count = unique_terms_collection.count_documents({'type': type_value})
            print(f"Type '{type_value}': {count} documents")

    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)

if __name__ == "__main__":
    main()


********************************************************************************

File: app/show_env.py
********************************************************************************

# datetime: 2024-10-16
# Purpose: To load and print all environment variables from a .env file using dotenv

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Print all environment variables
for key, value in os.environ.items():
    print(f"{key}: {value}")


********************************************************************************

File: app/models.py
********************************************************************************

# No longer necessary since it is NoSQL db. 

# from app import db, ma

# class OCRText(db.Model):
#     __tablename__ = 'ocr_text'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, nullable=False)
#     text = db.Column(db.Text, nullable=False)

#     summary = db.relationship('Summary', back_populates='ocr_text', uselist=False)
#     named_entities = db.relationship('NamedEntity', back_populates='ocr_text')
#     dates = db.relationship('Date', back_populates='ocr_text')
#     monetary_amounts = db.relationship('MonetaryAmount', back_populates='ocr_text')
#     relationships = db.relationship('Relationship', back_populates='ocr_text')
#     document_metadata = db.relationship('DocumentMetadata', back_populates='ocr_text', uselist=False)
#     translation = db.relationship('Translation', back_populates='ocr_text', uselist=False)
#     file_info = db.relationship('FileInfo', back_populates='ocr_text', uselist=False)

# class Summary(db.Model):
#     __tablename__ = 'summary'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     text = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='summary')

# class NamedEntity(db.Model):
#     __tablename__ = 'named_entities'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     entity = db.Column(db.Text, nullable=False)
#     type = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='named_entities')

# class Date(db.Model):
#     __tablename__ = 'dates'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     date = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='dates')

# class MonetaryAmount(db.Model):
#     __tablename__ = 'monetary_amounts'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     amount = db.Column(db.Text, nullable=False)
#     category = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='monetary_amounts')

# class Relationship(db.Model):
#     __tablename__ = 'relationships'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     entity1 = db.Column(db.Text, nullable=False)
#     relationship = db.Column(db.Text, nullable=False)
#     entity2 = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='relationships')

# class DocumentMetadata(db.Model):
#     __tablename__ = 'metadata'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     document_type = db.Column(db.Text, nullable=False)
#     period = db.Column(db.Text, nullable=False)
#     context = db.Column(db.Text, nullable=False)
#     sentiment = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='document_metadata')

# class Translation(db.Model):
#     __tablename__ = 'translation'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     french_text = db.Column(db.Text, nullable=False)
#     english_translation = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='translation')

# class FileInfo(db.Model):
#     __tablename__ = 'file_info'
#     id = db.Column(db.Text, primary_key=True)
#     file = db.Column(db.Text, db.ForeignKey('ocr_text.id'), nullable=False)
#     original_filepath = db.Column(db.Text, nullable=False)

#     ocr_text = db.relationship('OCRText', back_populates='file_info')

# # Add Marshmallow schemas if needed
# class OCRTextSchema(ma.SQLAlchemyAutoSchema):
#     class Meta:
#         model = OCRText


********************************************************************************

File: app/data_processing.py
********************************************************************************

# data_processing.py

import os
import json
import re
import hashlib
from database_setup import (
    insert_document,
    update_field_structure,
    get_db,
    is_file_ingested,
    get_client,
)
from dotenv import load_dotenv
from pymongo import UpdateOne

from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import time
import logging
import argparse
from collections import Counter
import pymongo

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('DataProcessingLogger')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)

    file_handler = logging.FileHandler('database_processing.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Global Variables
# =======================

root_directory = None
db = None  # Will be initialized in each process

# =======================
# Utility Functions
# =======================

def calculate_file_hash(file_path):
    """Calculate SHA256 hash of a file."""
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except Exception as e:
        logger.error(f"Error calculating hash for {file_path}: {e}")
        return None

def clean_json(json_text):
    """Remove control characters and extract valid JSON content."""
    # Remove control characters
    json_text = re.sub(r'[\x00-\x1F\x7F]', '', json_text)
    # Attempt to find the first and last curly braces
    start_index = json_text.find('{')
    end_index = json_text.rfind('}')
    if start_index != -1 and end_index != -1 and end_index > start_index:
        json_substring = json_text[start_index:end_index + 1]
        try:
            json.loads(json_substring)
            return json_substring
        except json.JSONDecodeError:
            # Try to fix common JSON issues
            try:
                fixed_json = json_substring.encode('utf-8').decode('unicode_escape', 'ignore')
                json.loads(fixed_json)
                return fixed_json
            except json.JSONDecodeError:
                raise ValueError("Invalid JSON format after cleaning.")
    raise ValueError("Invalid JSON format: Unable to find valid JSON object.")

def load_and_validate_json_file(file_path):
    """Load a JSON file, validate its content, and return it as a dictionary."""
    filename = os.path.basename(file_path)
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            json_content = file.read()
        cleaned_json_content = clean_json(json_content)
        json_data = json.loads(cleaned_json_content)
        json_data.setdefault('filename', filename)
        json_data.setdefault('relative_path', os.path.relpath(file_path, start=root_directory))
        json_data['file_path'] = file_path
        json_data['file_hash'] = calculate_file_hash(file_path)
        return json_data, None
    except json.JSONDecodeError as e:
        error_msg = f"Error decoding JSON in {filename}: {str(e)}"
        return None, error_msg
    except Exception as e:
        error_msg = f"Error processing file {filename}: {str(e)}"
        return None, error_msg

# =======================
# Combined Hash Functions
# =======================

def calculate_combined_hash(file_list):
    """Calculate a combined SHA256 hash for all files in the list."""
    sha256_hash = hashlib.sha256()
    for file_path in sorted(file_list):
        file_hash = calculate_file_hash(file_path)
        if file_hash:
            sha256_hash.update(file_hash.encode())
    return sha256_hash.hexdigest()

def get_stored_combined_hash(db):
    """Retrieve the stored combined hash from the metadata collection."""
    metadata = db.metadata.find_one({"type": "combined_hash"})
    return metadata["hash"] if metadata and "hash" in metadata else None

def store_combined_hash(db, combined_hash):
    """Store the combined hash in the metadata collection."""
    db.metadata.update_one(
        {"type": "combined_hash"},
        {"$set": {"hash": combined_hash, "last_updated": time.time()}},
        upsert=True
    )

def total_hash_check(directory_path):
    files = get_all_files(directory_path)
    total = len(files)

    logger.info(f"Found {total} files to process.")

    if total == 0:
        logger.warning("No files found to process. Exiting.")
        return

    # Calculate the combined hash of all files
    logger.info("Calculating combined hash of all files...")
    combined_hash = calculate_combined_hash(files)
    logger.debug(f"Combined Hash: {combined_hash}")



    # Initialize database connection
    init_db()

    # Retrieve the stored combined hash
    stored_hash = get_stored_combined_hash(db)
    logger.debug(f"Stored Combined Hash: {stored_hash}")

    if combined_hash == stored_hash:
        logger.info("No changes detected. Skipping processing.")
        print("No changes detected. Skipping processing.")
        flag = False
        return
    else:
        logger.info("Changes detected. Proceeding with processing.")
        print("Changes detected. Proceeding with processing.")
        flag = True

    return flag 

# =======================
# Processing Functions
# =======================

def init_db():
    """Initialize a new MongoDB connection for each process."""
    global db
    try:
        client = get_client()
        db = get_db(client)
        logger.debug("Database connection initialized.")
    except Exception as e:
        logger.exception("Failed to initialize database connection")
        raise e

def process_file(file_path):
    """
    Process a single file and return the result.
    """
    # Initialize database connection for this process
    if db is None:
        init_db()
    filename = os.path.basename(file_path)
    logger.debug(f"Processing file: {filename}")
    result = {'processed': [], 'failed': [], 'skipped': []}

    try:
        file_hash = calculate_file_hash(file_path)
        if is_file_ingested(db, file_hash):
            logger.debug(f"File already ingested: {filename}")
            result['skipped'].append(file_path)
            return result, None

        json_data, error = load_and_validate_json_file(file_path)
        if json_data:
            try:
                update_field_structure(db, json_data)  # Pass 'db' here
                insert_document(db, json_data)         # Pass 'db' here
                logger.debug(f"Processed and inserted document: {filename}")
                result['processed'].append(file_path)
            except Exception as e:
                logger.exception(f"Error processing {filename}")
                result['failed'].append((file_path, str(e)))
        else:
            if error:
                logger.error(error)
                result['failed'].append((file_path, error))

    except Exception as e:
        logger.exception(f"Unexpected error processing {filename}")
        result['failed'].append((file_path, str(e)))

    return result, None

def get_all_files(directory):
    """Recursively get all JSON and TXT files in the given directory and its subdirectories."""
    file_list = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.json', '.txt')):
                file_list.append(os.path.join(root, file))
                logger.debug(f"Found file: {os.path.join(root, file)}")
    return file_list

# =======================
# Sequential Processing
# =======================
def process_directory(directory_path):
    """Process all files in a directory and its subdirectories sequentially for debugging."""
    global root_directory
    root_directory = directory_path

    start_time = time.time()
    files = get_all_files(directory_path)
    total = len(files)

    logger.info(f"Found {total} files to process.")

    if total == 0:
        logger.warning("No files found to process. Exiting.")
        return

    # Initialize results_dict in the main process
    results_dict = {
        'processed': [],
        'failed': [],
        'skipped': []
    }

    with tqdm(total=total, desc="Processing files") as pbar:
        for idx, file_path in enumerate(files, 1):
            result, _ = process_file(file_path)
            for key in ['processed', 'failed', 'skipped']:
                results_dict[key].extend(result.get(key, []))
            pbar.update(1)

    # Initialize database connection (if not already)
    if db is None:
        init_db()
        logger.debug("Initialized main process database connection.")

    logger.info("\nProcessing Summary:")
    logger.info(f"Total files found: {total}")
    logger.info(f"Successfully processed: {len(results_dict['processed'])}")
    logger.info(f"Skipped (already ingested): {len(results_dict['skipped'])}")
    logger.info(f"Failed to process: {len(results_dict['failed'])}")

    if results_dict['failed']:
        logger.info("\nFailed files:")
        for file_path, error in results_dict['failed']:
            logger.error(f"- {file_path}: {error}")

    duration = time.time() - start_time
    logger.info(f"\nTotal processing time: {duration:.2f} seconds.")

# =======================
# Main Execution
# =======================
if __name__ == "__main__":
    print("Starting data_processing.py")
    logger.info("Starting data_processing.py")
    parser = argparse.ArgumentParser(description="Process and validate JSON and TXT files for the railroad documents database.")
    parser.add_argument("data_directory", nargs='?', default='/app/archives',
                        help="Path to the root directory containing JSON and/or text files to process (default: '/app/archives')")
    args = parser.parse_args()
    
    data_directory = args.data_directory
    print(data_directory)
    if not os.path.exists(data_directory):
        logger.error(f"Error: The specified directory does not exist: {data_directory}")
        logger.info(f"Creating directory: {data_directory}")
        try:
            os.makedirs(data_directory)
            logger.info(f"Directory created successfully: {data_directory}")
        except Exception as e:
            logger.exception("Failed to create directory")
            exit(1)

    #hash check for archive file change
    logger.info("Checking to see if archive has changed")
    archives_file_change = total_hash_check(data_directory) #should return a boolean

    if archives_file_change == True:
        logger.info(f"Processing directory: {data_directory}")
        process_directory(data_directory)

        # After processing, calculate and store the new hash
        new_combined_hash = calculate_combined_hash(get_all_files(data_directory))
        store_combined_hash(db, new_combined_hash)
        logger.info("Updated stored hash after processing.")
    else:
        logger.info("No change in archives")



********************************************************************************

File: app/ner_worker.py
********************************************************************************

# ner_worker.py

import os
import logging
from pymongo import UpdateOne, MongoClient
from dotenv import load_dotenv
import spacy
from collections import defaultdict
from rapidfuzz import process, fuzz
import requests

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('EntityProcessingWorker')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)

    file_handler = logging.FileHandler('entity_processing_worker.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Database Functions
# =======================

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        mongo_uri = os.environ.get('MONGO_URI')
        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']

def get_collections(db):
    """Retrieve and return references to the required collections."""
    try:
        documents_collection = db['documents']
        linked_entities_collection = db['linked_entities']
        return documents_collection, linked_entities_collection
    except Exception as e:
        logger.error(f"Error getting collections: {e}")
        raise e

# =======================
# spaCy Initialization
# =======================
def initialize_spacy():
    """Load spaCy model."""
    try:
        logger.info("Loading spaCy model with optimized settings.")
        # Load spaCy model with only the NER component
        nlp = spacy.load("en_core_web_lg", disable=["tagger", "parser", "lemmatizer"])
        return nlp
    except Exception as e:
        logger.error(f"Error loading spaCy model: {e}")
        raise e

# Global variable for spaCy model in worker processes
nlp = None

def initialize_worker():
    """Initialize spaCy model in worker processes."""
    global nlp
    nlp = initialize_spacy()

# =======================
# Utility Functions
# =======================

def entity_default():
    """Default factory function for defaultdict."""
    return {'frequency': 0, 'document_ids': set(), 'type': ''}

def fuzzy_match(term, reference_terms, threshold=90):
    """
    Perform fuzzy matching to find the best match for a term in reference_terms.
    Returns the matched term if similarity exceeds the threshold, else None.
    """
    try:
        result = process.extractOne(term, reference_terms, scorer=fuzz.token_sort_ratio)
        if result is None:
            logger.debug(f"No fuzzy match found for term '{term}'.")
            return None
        match, score = result
        logger.debug(f"Fuzzy match for term '{term}': '{match}' with score {score}.")
        if score >= threshold:
            return match
        return None
    except Exception as e:
        logger.error(f"Exception during fuzzy matching for term '{term}': {e}")
        return None

# In-memory cache for Wikidata entities
wikidata_cache = {}

def fetch_wikidata_entity(term):
    """
    Fetch Wikidata entity ID for a given term using the Wikidata API.
    Returns the entity ID if found, else None.
    """
    if term in wikidata_cache:
        return wikidata_cache[term]
    try:
        url = "https://www.wikidata.org/w/api.php"
        params = {
            'action': 'wbsearchentities',
            'search': term,
            'language': 'en',
            'format': 'json'
        }
        response = requests.get(url, params=params, timeout=5)
        data = response.json()
        if 'search' in data and len(data['search']) > 0:
            # Return the first matching entity ID
            wikidata_id = data['search'][0]['id']
            wikidata_cache[term] = wikidata_id
            logger.debug(f"Fetched Wikidata ID '{wikidata_id}' for term '{term}'.")
            return wikidata_id
        else:
            wikidata_cache[term] = None
            logger.debug(f"No Wikidata ID found for term '{term}'.")
            return None
    except Exception as e:
        logger.error(f"Error fetching Wikidata entity for term '{term}': {e}")
        return None

# =======================
# Main Processing Function
# =======================

def process_documents_batch(args):
    """
    Process a batch of documents to extract entities.
    This function is intended to be run in a worker process.
    """
    batch_docs, fields_to_process, valid_entity_labels, existing_linked_terms, link_wikidata, fuzzy_threshold = args
    global nlp
    if nlp is None:
        initialize_worker()
    processed_doc_ids = []
    aggregated_entities = defaultdict(entity_default)

    # Initialize database connection in worker
    try:
        client = get_client()
        db = get_db(client)
        documents_collection, _ = get_collections(db)
    except Exception as e:
        logger.error(f"Worker failed to connect to MongoDB: {e}")
        return processed_doc_ids, aggregated_entities

    texts = []
    doc_id_mapping = []
    for doc in batch_docs:
        doc_id = doc['_id']
        combined_text = ''
        for field in fields_to_process:
            text = doc.get(field, '')
            if not text:
                continue
            if isinstance(text, list):
                text = ' '.join(text)
            elif not isinstance(text, str):
                continue
            combined_text += ' ' + text
        if combined_text.strip():
            texts.append(combined_text)
            doc_id_mapping.append(doc_id)
        else:
            logger.warning(f"Document '{doc_id}' has no text in specified fields.")

    if not texts:
        logger.warning("No texts found in the current batch to process.")
        return processed_doc_ids, aggregated_entities

    logger.debug(f"Processing batch of {len(texts)} documents.")
    # Process texts in batch using nlp.pipe()
    try:
        for doc_id, spacy_doc in zip(doc_id_mapping, nlp.pipe(texts, batch_size=100, n_process=1)):
            entities = []
            for ent in spacy_doc.ents:
                if ent.label_ in valid_entity_labels:
                    term = ent.text
                    ent_type = ent.label_
                    term_lower = term.lower()
                    entities.append({'text': term, 'type': ent_type})
                    logger.debug(f"Found entity '{term}' of type '{ent_type}' in document '{doc_id}'.")

                    # Aggregate entities for linking
                    aggregated_entities[(term_lower, ent_type)]['frequency'] += 1
                    aggregated_entities[(term_lower, ent_type)]['document_ids'].add(doc_id)
                    aggregated_entities[(term_lower, ent_type)]['type'] = ent_type

            if entities:
                # Update the document with extracted entities and mark as processed
                try:
                    result = documents_collection.update_one(
                        {'_id': doc_id, 'entities_processed': {'$ne': True}},
                        {'$set': {
                            'extracted_entities': entities,
                            'entities_processed': True  # Mark as processed here
                        }}
                    )
                    if result.modified_count > 0:
                        processed_doc_ids.append(doc_id)
                        logger.debug(f"Updated document '{doc_id}' with extracted entities and marked as processed.")
                    else:
                        logger.debug(f"Document '{doc_id}' was already processed by another worker.")
                except Exception as e:
                    logger.error(f"Failed to update document '{doc_id}': {e}")
            else:
                logger.info(f"No valid entities found in document '{doc_id}'. Marking as processed.")
                # Even if no entities found, mark as processed to avoid reprocessing
                try:
                    result = documents_collection.update_one(
                        {'_id': doc_id, 'entities_processed': {'$ne': True}},
                        {'$set': {'entities_processed': True}}
                    )
                    if result.modified_count > 0:
                        processed_doc_ids.append(doc_id)
                        logger.debug(f"Marked document '{doc_id}' as processed with no entities found.")
                    else:
                        logger.debug(f"Document '{doc_id}' was already processed by another worker.")
                except Exception as e:
                    logger.error(f"Failed to mark document '{doc_id}' as processed: {e}")
    except Exception as e:
        logger.error(f"Error during processing batch: {e}")

    return processed_doc_ids, aggregated_entities


********************************************************************************

File: app/ner_processing.py
********************************************************************************

# ner_processing.py

import os
import logging
from pymongo import UpdateOne, MongoClient
from dotenv import load_dotenv
import spacy
from tqdm import tqdm
import multiprocessing
import json
from collections import defaultdict
from rapidfuzz import process, fuzz
import requests

# Import the worker function from ner_worker.py
from ner_worker import process_documents_batch, initialize_spacy

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('EntityProcessingLogger')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)

    file_handler = logging.FileHandler('entity_processing.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Database Functions
# =======================

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        mongo_uri = os.environ.get('MONGO_URI')
        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']

def get_collections(db):
    """Retrieve and return references to the required collections."""
    try:
        documents_collection = db['documents']
        linked_entities_collection = db['linked_entities']
        return documents_collection, linked_entities_collection
    except Exception as e:
        logger.error(f"Error getting collections: {e}")
        raise e

# =======================
# Utility Functions
# =======================

def chunkify(iterable, chunk_size):
    """
    Split an iterable into chunks of a specified size.
    """
    chunk = []
    for item in iterable:
        chunk.append(item)
        if len(chunk) == chunk_size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk

# In-memory cache for Wikidata entities
wikidata_cache = {}

def entity_default():
    """Default factory function for defaultdict."""
    return {'frequency': 0, 'document_ids': set(), 'type': ''}

def fuzzy_match(term, reference_terms, threshold=90):
    """
    Perform fuzzy matching to find the best match for a term in reference_terms.
    Returns the matched term if similarity exceeds the threshold, else None.
    """
    try:
        result = process.extractOne(term, reference_terms, scorer=fuzz.token_sort_ratio)
        if result is None:
            logger.debug(f"No fuzzy match found for term '{term}'.")
            return None
        match, score = result
        logger.debug(f"Fuzzy match for term '{term}': '{match}' with score {score}.")
        if score >= threshold:
            return match
        return None
    except Exception as e:
        logger.error(f"Exception during fuzzy matching for term '{term}': {e}")
        return None

def fetch_wikidata_entity(term):
    """
    Fetch Wikidata entity ID for a given term using the Wikidata API.
    Returns the entity ID if found, else None.
    """
    if term in wikidata_cache:
        return wikidata_cache[term]
    try:
        url = "https://www.wikidata.org/w/api.php"
        params = {
            'action': 'wbsearchentities',
            'search': term,
            'language': 'en',
            'format': 'json'
        }
        response = requests.get(url, params=params, timeout=5)
        data = response.json()
        if 'search' in data and len(data['search']) > 0:
            # Return the first matching entity ID
            wikidata_id = data['search'][0]['id']
            wikidata_cache[term] = wikidata_id
            logger.debug(f"Fetched Wikidata ID '{wikidata_id}' for term '{term}'.")
            return wikidata_id
        else:
            wikidata_cache[term] = None
            logger.debug(f"No Wikidata ID found for term '{term}'.")
            return None
    except Exception as e:
        logger.error(f"Error fetching Wikidata entity for term '{term}': {e}")
        return None

# =======================
# Main Processing Functions
# =======================

def extract_and_link_entities(documents_collection, linked_entities_collection, fields_to_process, link_wikidata, fuzzy_threshold=90, batch_size=1000, use_multiprocessing=False):
    """
    Extract entities from documents, perform entity linking, and store results.
    """
    # Extended entity types to consider
    valid_entity_labels = [
        'PERSON', 'ORG', 'GPE', 'LOC', 'DATE', 'TIME',
        'MONEY'
    ]

    processed_count = 0

    # Prepare a list of already linked terms for fuzzy matching
    existing_linked_terms = linked_entities_collection.distinct("term")
    logger.debug(f"Fetched {len(existing_linked_terms)} existing linked terms for fuzzy matching.")

    # Query to fetch only unprocessed documents
    query = {"$or": [{"entities_processed": {"$exists": False}}, {"entities_processed": False}]}
    cursor = documents_collection.find(
        query,
        {'_id': 1, **{field: 1 for field in fields_to_process}}
    )

    total_documents = documents_collection.count_documents(query)
    logger.info(f"Total unprocessed documents to process: {total_documents}")

    if total_documents == 0:
        logger.info("No unprocessed documents found. Exiting.")
        return

    aggregated_entities = defaultdict(entity_default)

    if use_multiprocessing:
        logger.info("Multiprocessing is ENABLED.")
        # Prepare batches of documents
        batches = list(chunkify(cursor, batch_size))
        total_batches = len(batches)
        logger.info(f"Total batches to process: {total_batches}")

        # Prepare data for multiprocessing
        batch_args = [(batch, fields_to_process, valid_entity_labels, existing_linked_terms, link_wikidata, fuzzy_threshold) for batch in batches]

        num_processes = multiprocessing.cpu_count() - 1 or 1  # Reserve one core
        logger.info(f"Using {num_processes} worker processes for multiprocessing.")

        with multiprocessing.Pool(processes=num_processes, initializer=initialize_spacy) as pool:
            results = tqdm(pool.imap_unordered(process_documents_batch, batch_args), total=total_batches, desc="Processing batches")
            for processed_doc_ids, batch_aggregated_entities in results:
                # No need to update 'entities_processed' here as workers already did it
                processed_count += len(processed_doc_ids)
                logger.debug(f"Processed {processed_count}/{total_documents} documents so far.")

                # Merge batch results into the main aggregated_entities
                for key, value in batch_aggregated_entities.items():
                    aggregated_entities[key]['frequency'] += value['frequency']
                    aggregated_entities[key]['document_ids'].update(value['document_ids'])
                    aggregated_entities[key]['type'] = value['type']
    else:
        logger.info("Multiprocessing is DISABLED.")
        nlp = initialize_spacy()  # Initialize spaCy once here
        batches = chunkify(cursor, batch_size)
        for batch_docs in tqdm(batches, desc="Processing batches"):
            processed_doc_ids, batch_aggregated_entities = process_documents_batch((batch_docs, fields_to_process, valid_entity_labels, existing_linked_terms, link_wikidata, fuzzy_threshold))
            # No need to update 'entities_processed' here as workers already did it
            processed_count += len(processed_doc_ids)
            logger.debug(f"Processed {processed_count}/{total_documents} documents so far.")

            # Merge batch results into the main aggregated_entities
            for key, value in batch_aggregated_entities.items():
                aggregated_entities[key]['frequency'] += value['frequency']
                aggregated_entities[key]['document_ids'].update(value['document_ids'])
                aggregated_entities[key]['type'] = value['type']

    logger.info(f"Processed {processed_count} documents.")

    # Now, perform entity linking
    link_entities(aggregated_entities, linked_entities_collection, existing_linked_terms, link_wikidata, fuzzy_threshold, batch_size)

def link_entities(aggregated_entities, linked_entities_collection, existing_linked_terms, link_wikidata, fuzzy_threshold, batch_size):
    """
    Link aggregated entities and update the linked_entities collection.
    """
    linked_count = 0
    processed_entities = 0
    total_entities = len(aggregated_entities)
    logger.info(f"Total unique entities to process: {total_entities}")

    operations = []

    for (term_lower, ent_type), data in aggregated_entities.items():
        frequency = data['frequency']
        document_ids = list(data['document_ids'])
        wikidata_id = None

        if link_wikidata:
            wikidata_id = fetch_wikidata_entity(term_lower)

        # If Wikidata linking is disabled or Wikidata ID not found, perform fuzzy matching
        if not wikidata_id:
            fuzzy_match_term = fuzzy_match(term_lower, existing_linked_terms, threshold=fuzzy_threshold)
            if fuzzy_match_term:
                # Fetch the corresponding Wikidata ID for the matched term
                matched_entity = linked_entities_collection.find_one({"term": fuzzy_match_term.lower()})
                if matched_entity:
                    wikidata_id = matched_entity.get('kb_id')
                    logger.debug(f"Fuzzy matched term '{term_lower}' to '{fuzzy_match_term}' with Wikidata ID '{wikidata_id}'.")
            else:
                logger.debug(f"No match found for term '{term_lower}'.")

        if wikidata_id:
            logger.debug(f"Linked term '{term_lower}' to Wikidata ID '{wikidata_id}'.")

        update = {
            "$inc": {"frequency": frequency},
            "$addToSet": {"document_ids": {"$each": document_ids}},
            "$set": {"type": ent_type, "kb_id": wikidata_id}
        }

        operations.append(
            UpdateOne(
                {"term": term_lower},
                update,
                upsert=True
            )
        )

        linked_count += 1
        processed_entities += 1

        # Log progress every 1000 entities
        if processed_entities % 1000 == 0:
            logger.info(f"Processed {processed_entities}/{total_entities} entities.")

        # Execute operations in batches
        if len(operations) >= batch_size:
            logger.info(f"Writing batch of {len(operations)} entities to the database.")
            try:
                result = linked_entities_collection.bulk_write(operations, ordered=False)
                logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")
            except Exception as e:
                logger.error(f"Error bulk upserting linked entities: {e}")
                raise e
            operations = []

    # Execute remaining operations
    if operations:
        logger.info(f"Writing final batch of {len(operations)} entities to the database.")
        try:
            result = linked_entities_collection.bulk_write(operations, ordered=False)
            logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")
        except Exception as e:
            logger.error(f"Error bulk upserting linked entities: {e}")
            raise e

    logger.info(f"Successfully linked {linked_count} entities.")

# =======================
# Main Execution
# =======================
if __name__ == "__main__":
    try:
        client = get_client()
        db = get_db(client)
        logger.info("Connected to MongoDB.")

        documents_collection, linked_entities_collection = get_collections(db)

        # Determine if multiprocessing is enabled
        use_multiprocessing = os.getenv('ENABLE_MULTIPROCESSING', 'False').lower() in ('true', '1', 't')
        if use_multiprocessing:
            logger.info("Multiprocessing is ENABLED.")
        else:
            logger.info("Multiprocessing is DISABLED.")

        batch_size = int(os.getenv('BATCH_SIZE', 1000))  # Set default batch_size to 1000

        # Get list of fields to process from .env or default to ['ocr_text']
        fields_env = os.getenv('FIELDS_TO_PROCESS', '["ocr_text"]')
        try:
            fields_to_process = json.loads(fields_env)
            if not isinstance(fields_to_process, list):
                raise ValueError
        except ValueError:
            logger.error("Invalid FIELDS_TO_PROCESS format. It should be a JSON array of field names.")
            raise

        logger.info(f"Fields to process: {fields_to_process}")

        # Determine if Wikidata linking is enabled
        link_wikidata = os.getenv('LINK_WIKIDATA', 'False').lower() in ('true', '1', 't')
        if link_wikidata:
            logger.info("Wikidata linking is ENABLED.")
        else:
            logger.info("Wikidata linking is DISABLED. Focusing on fuzzy matching.")

        # Fetch configuration for fuzzy matching
        fuzzy_threshold = int(os.getenv('FUZZY_MATCH_THRESHOLD', 90))

        # Extract entities and perform linking
        extract_and_link_entities(
            documents_collection,
            linked_entities_collection,
            fields_to_process,
            link_wikidata,
            fuzzy_threshold=fuzzy_threshold,
            batch_size=batch_size,
            use_multiprocessing=use_multiprocessing
        )

    except Exception as e:
        logger.error(f"An error occurred during entity processing: {e}", exc_info=True)


********************************************************************************

File: app/generate_unique_terms.py
********************************************************************************

# generate_unique_terms.py

import os
import re
import logging
from collections import Counter, defaultdict
from tqdm import tqdm

from pymongo import UpdateOne, MongoClient
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('GenerateUniqueTermsLogger')
logger.setLevel(logging.INFO)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)

    file_handler = logging.FileHandler('generate_unique_terms.log', mode='a')
    file_handler.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Database Configuration
# =======================

def get_client():
    mongo_uri = os.getenv('MONGO_URI', 'mongodb://admin:secret@mongodbt:27017/admin')
    return MongoClient(mongo_uri)

def get_db(client):
    return client['railroad_documents']

def get_collections(db):
    documents = db['documents']
    unique_terms_collection = db['unique_terms']
    return documents, unique_terms_collection

# =======================
# Utility Functions
# =======================

def collect_unique_terms_from_text(text):
    """Collect unique words and phrases from a text string."""
    unique_terms = {'word': Counter(), 'phrase': Counter()}
    if isinstance(text, str):
        words = re.findall(r'\w+', text.lower())
        phrases = [' '.join(pair) for pair in zip(words, words[1:])]
        unique_terms['word'].update(words)
        unique_terms['phrase'].update(phrases)
    return unique_terms

def merge_counters(main_counter, new_counter):
    """Merge two unique terms dictionaries."""
    for term_type in ['word', 'phrase']:
        main_counter[term_type].update(new_counter.get(term_type, Counter()))

def is_text_field(value):
    """Check if the field value is a string or a list of strings."""
    if isinstance(value, str):
        return True
    elif isinstance(value, list):
        # Check if all elements in the list are strings
        return all(isinstance(item, str) for item in value)
    return False

def chunkify(iterable, chunk_size):
    """Split an iterable into chunks of a specified size."""
    chunk = []
    for item in iterable:
        chunk.append(item)
        if len(chunk) == chunk_size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk

# =======================
# Main Processing Functions
# =======================

def process_documents_batch(batch_docs):
    """Process a batch of documents to extract unique terms."""
    unique_terms = defaultdict(lambda: {'word': Counter(), 'phrase': Counter()})
    processed_doc_ids = []

    for doc in batch_docs:
        doc_id = doc['_id']
        doc_unique = process_document(doc)
        for field, terms in doc_unique.items():
            merge_counters(unique_terms[field], terms)
        processed_doc_ids.append(doc_id)

    # Establish a new MongoDB client for updating
    client = get_client()
    db = get_db(client)
    documents_collection, _ = get_collections(db)

    # Mark documents as processed
    if processed_doc_ids:
        documents_collection.update_many(
            {"_id": {"$in": processed_doc_ids}},
            {"$set": {"unique_terms_processed": True}}
        )

    client.close()
    return unique_terms

def process_document(document):
    """Extract and count unique terms from a single document by iterating over all text fields."""
    unique_terms = defaultdict(lambda: {'word': Counter(), 'phrase': Counter()})

    for field, value in document.items():
        if is_text_field(value):
            if isinstance(value, list):
                text = ' '.join(value)
            else:
                text = value
            terms = collect_unique_terms_from_text(text)
            if terms['word'] or terms['phrase']:
                merge_counters(unique_terms[field], terms)

    return unique_terms

def generate_unique_terms(db, batch_size=1000):
    """Generate unique terms from documents and populate the unique_terms collection."""
    documents_collection, unique_terms_collection = get_collections(db)

    # Filter: Only process documents that haven't been processed yet
    query = {"$or": [{"unique_terms_processed": {"$exists": False}}, {"unique_terms_processed": False}]}
    cursor = documents_collection.find(query)

    # Use count_documents() instead of cursor.count()
    total_documents = documents_collection.count_documents(query)
    logger.info(f"Total unprocessed documents: {total_documents}")

    if total_documents == 0:
        logger.info("No unprocessed documents found. Exiting.")
        return

    processed_count = 0
    batches = chunkify(cursor, batch_size)
    total_batches = (total_documents // batch_size) + (1 if total_documents % batch_size else 0)
    logger.info(f"Total batches to process: {total_batches}")

    # Load existing terms from unique_terms_collection
    existing_terms = {}
    for doc in unique_terms_collection.find({}, {"term": 1, "frequency": 1, "field": 1, "type": 1}):
        key = (doc['term'], doc['field'], doc['type'])
        existing_terms[key] = doc['frequency']

    for batch_docs in tqdm(batches, total=total_batches, desc="Processing batches"):
        batch_unique_terms = process_documents_batch(batch_docs)
        # Prepare bulk operations based on new terms and frequency differences
        operations = []
        
        for field, types in batch_unique_terms.items():
            for term_type, counter in types.items():
                for term, freq in counter.items():
                    key = (term, field, term_type)
                    existing_freq = existing_terms.get(key, 0)
                    freq_diff = freq - existing_freq
                    
                    if freq_diff > 0:  # Only upsert if there is a difference
                        operations.append(
                            UpdateOne(
                                {"term": term, "field": field, "type": term_type},
                                {"$inc": {"frequency": freq_diff}},
                                upsert=True
                            )
                        )
                        # Update existing_terms to avoid repeated increments
                        existing_terms[key] = freq

        processed_count += len(batch_docs)
        logger.info(f"Processed {processed_count} documents.")
        
        if operations:
            try:
                result = unique_terms_collection.bulk_write(operations, ordered=False)
                logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} unique terms.")
            except Exception as e:
                logger.error(f"Error bulk upserting unique terms: {e}")
                raise e
        else:
            logger.warning("No unique terms to upsert in this batch.")

    logger.info("Unique terms generation completed.")

# =======================
# Main Execution
# =======================
if __name__ == "__main__":
    try:
        client = get_client()
        db = get_db(client)
        logger.info("Connected to MongoDB.")

        # Set multiprocessing to False
        use_multiprocessing = False
        logger.info("Multiprocessing is DISABLED.")

        # Fetch batch size from .env or use default
        batch_size = int(os.getenv('BATCH_SIZE', 1000))

        generate_unique_terms(db, batch_size=batch_size)

    except Exception as e:
        logger.error(f"An error occurred: {e}", exc_info=True)


********************************************************************************

File: app/routes.py
********************************************************************************

# File: routes.py
# Path: routes.py

from flask import request, jsonify, render_template, redirect, url_for, flash, session, abort, Response, send_file, Flask
from functools import wraps
from app import app, cache
from database_setup import (
    get_client,
    get_db,
    get_collections,
    find_document_by_id,
    update_document,
    delete_document,
    get_field_structure
)
from bson import ObjectId
from werkzeug.security import generate_password_hash, check_password_hash
import math
import json
import re
import logging
import time
from datetime import datetime, timedelta
import random
import csv
from io import StringIO
from logging.handlers import RotatingFileHandler
import os
import uuid
import pymongo

# Create a logger instance
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# Define a log format
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Create a console handler (optional)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Create a file handler to log to routes.log
log_file_path = os.path.join(os.path.dirname(__file__), 'routes.log')
file_handler = logging.FileHandler(log_file_path)
file_handler.setLevel(logging.DEBUG)  # Set the level for the file handler
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Initialize database connection and collections
client = get_client()
db = get_db(client)
documents, unique_terms_collection, field_structure_collection = get_collections(db)

# Hashed password (generate this using generate_password_hash('your_actual_password'))
ADMIN_PASSWORD_HASH = 'pbkdf2:sha256:260000$uxZ1Fkjt9WQCHwuN$ca37dfb41ebc26b19daf24885ebcd09f607cab85f92dcab13625627fd9ee902a'

# Login attempt tracking
MAX_ATTEMPTS = 5
LOCKOUT_TIME = 15 * 60  # 15 minutes in seconds
login_attempts = {}

def is_locked_out(ip):
    if ip in login_attempts:
        attempts, last_attempt_time = login_attempts[ip]
        if attempts >= MAX_ATTEMPTS:
            if datetime.now() - last_attempt_time < timedelta(seconds=LOCKOUT_TIME):
                return True
            else:
                login_attempts[ip] = (0, datetime.now())
    return False

def update_login_attempts(ip, success):
    if ip in login_attempts:
        attempts, _ = login_attempts[ip]
        if success:
            login_attempts[ip] = (0, datetime.now())
        else:
            login_attempts[ip] = (attempts + 1, datetime.now())
    else:
        login_attempts[ip] = (0, datetime.now()) if success else (1, datetime.now())

# Login required decorator
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if 'logged_in' not in session:
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

@app.route('/')
# @login_required
def index():
    app.logger.info('Handling request to index')
    num_search_fields = 3  # Number of search fields to display
    field_structure = get_field_structure(db)  # Pass 'db' here
    return render_template('index.html', num_search_fields=num_search_fields, field_structure=field_structure)

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        ip = request.remote_addr

        if is_locked_out(ip):
            flash('Too many failed attempts. Please try again later.')
            return render_template('login.html')

        # Verify CAPTCHA
        user_captcha = request.form.get('captcha')
        correct_captcha = request.form.get('captcha_answer')
        if user_captcha != correct_captcha:
            flash('Incorrect CAPTCHA')
            return redirect(url_for('login'))

        if check_password_hash(ADMIN_PASSWORD_HASH, request.form['password']):
            session['logged_in'] = True
            update_login_attempts(ip, success=True)
            flash('You were successfully logged in')
            next_page = request.args.get('next')
            return redirect(next_page or url_for('index'))
        else:
            update_login_attempts(ip, success=False)
            time.sleep(2)  # Add a delay after failed attempt
            flash('Invalid password')

    # Generate CAPTCHA for GET requests
    captcha_num1 = random.randint(1, 10)
    captcha_num2 = random.randint(1, 10)
    captcha_answer = str(captcha_num1 + captcha_num2)

    return render_template('login.html', captcha_num1=captcha_num1, captcha_num2=captcha_num2, captcha_answer=captcha_answer)

@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    flash('You were logged out')
    return redirect(url_for('index'))

@app.route('/search', methods=['POST'])
# @login_required
def search():
    try:
        data = request.get_json()
        logger.debug(f"Received search request: {data}")

        page = int(data.get('page', 1))
        per_page = int(data.get('per_page', 50))

        query = build_query(data)
        logger.debug(f"Constructed MongoDB query: {query}")

        total_count = documents.count_documents(query)
        search_results = list(documents.find(query).skip((page - 1) * per_page).limit(per_page))

        for doc in search_results:
            doc['_id'] = str(doc['_id'])

        total_pages = math.ceil(total_count / per_page) if per_page else 1

        # Generate unique search ID
        search_id = str(uuid.uuid4())
        # Store the ordered list of document IDs
        ordered_ids = [doc['_id'] for doc in search_results]
        cache.set(f'search_{search_id}', ordered_ids, timeout=3600)  # Expires in 1 hour

        logger.debug(f"Search ID: {search_id}, Found {total_count} documents.")

        return jsonify({
            "search_id": search_id,
            "documents": search_results,
            "total_count": total_count,
            "current_page": page,
            "total_pages": total_pages,
            "per_page": per_page
        })

    except Exception as e:
        logger.error(f"An error occurred during search: {str(e)}", exc_info=True)
        return jsonify({"error": "An internal error occurred"}), 500

def build_query(data):
    query = {}
    criteria_list = []

    logger.debug(f"Building query from search data: {data}")

    for i in range(1, 4):
        field = data.get(f'field{i}')
        search_term = data.get(f'searchTerm{i}')
        operator = data.get(f'operator{i}')

        if field and search_term:
            condition = {}
            if operator == 'NOT':
                condition[field] = {'$not': {'$regex': re.escape(search_term), '$options': 'i'}}
            else:
                condition[field] = {'$regex': re.escape(search_term), '$options': 'i'}
            
            criteria_list.append((operator, condition))
            logger.debug(f"Processed field {field} with search term '{search_term}' and operator '{operator}'")

    if criteria_list:
        and_conditions = []
        or_conditions = []

        for operator, condition in criteria_list:
            if operator == 'AND' or operator == 'NOT':
                and_conditions.append(condition)
            elif operator == 'OR':
                or_conditions.append(condition)

        if and_conditions:
            query['$and'] = and_conditions

        if or_conditions:
            if '$or' not in query:
                query['$or'] = or_conditions
            else:
                query['$or'].extend(or_conditions)

    logger.debug(f"Final query: {query}")
    return query

@app.route('/document/<string:doc_id>')
# @login_required
def document_detail(doc_id):
    
    
    # Hard-coded SHOW_EMPTY variable
    SHOW_EMPTY = False  # Set to True to show empty fields, False to hide them

    # Function to clean the document data
    def clean_data(data):
        empty_values = [None, '', 'N/A', 'null', [], {}, 'None']
        if isinstance(data, dict):
            return {
                k: clean_data(v)
                for k, v in data.items()
                if v not in empty_values and clean_data(v) not in empty_values
            }
        elif isinstance(data, list):
            return [
                clean_data(item)
                for item in data
                if item not in empty_values and clean_data(item) not in empty_values
            ]
        else:
            return data
    
    
    
    
    search_id = request.args.get('search_id')
    if not search_id:
        flash('Missing search context.')
        return redirect(url_for('index'))

    try:
        # Fetch the document by ID
        document = find_document_by_id(db, doc_id)
        if not document:
            abort(404)

        document['_id'] = str(document['_id'])

        # Log the document information for debugging
        logger.debug(f"Retrieved document for ID {doc_id}: {document}")


        # Decide whether to clean the document based on SHOW_EMPTY
        if SHOW_EMPTY:
            document = document
        else:
            # Clean the document to remove empty fields
            document = clean_data(document)
        









        # Retrieve the ordered list from cache
        ordered_ids = cache.get(f'search_{search_id}')
        if not ordered_ids:
            flash('Search context expired. Please perform the search again.')
            return redirect(url_for('index'))

        try:
            current_index = ordered_ids.index(doc_id)
        except ValueError:
            flash('Document not found in the current search results.')
            return redirect(url_for('index'))

        # Determine previous and next IDs based on the search order
        prev_id = ordered_ids[current_index - 1] if current_index > 0 else None
        next_id = ordered_ids[current_index + 1] if current_index < len(ordered_ids) - 1 else None

        # Get the relative path from the document
        relative_path = document.get('relative_path')  # This should contain the relative path to the JSON file

        if relative_path:
            # Construct the image path by removing the '.json' extension
            image_path = relative_path.replace('.json', '')  # 'rolls/rolls/tray_1_roll_5_page3303_img1.png'
            logger.debug(f"Document ID: {doc_id}, Image path: {image_path}")

            # Check if the image file exists
            absolute_image_path = os.path.join('/app/archives', image_path)
            image_exists = os.path.exists(absolute_image_path)

            if not image_exists:
                logger.warning(f"Image not found at: {absolute_image_path}")
        else:
            # Log an error if relative_path is None or not found
            logger.error(f"Error: No relative_path found for document ID: {doc_id}. Document content: {document}")
            image_exists = False
            image_path = None

        # Render the template with all required variables
        return render_template(
            'document-detail.html',
            document=document,
            prev_id=prev_id,
            next_id=next_id,
            search_id=search_id,
            image_path=image_path,  # Pass the constructed image path
            image_exists=image_exists  # Pass the flag indicating if the image exists
        )
    except Exception as e:
        logger.error(f"Error in document_detail: {str(e)}", exc_info=True)
        abort(500)





@app.route('/images/<path:filename>')
def serve_image(filename):
    image_path = os.path.join('/app/archives', filename)
    logger.debug(f"Serving image from: {image_path}")
    if os.path.exists(image_path):
        return send_file(image_path)
    else:
        logger.warning(f"Image not found at: {image_path}")
        abort(404)

def get_top_unique_terms(db, field, term_type, query='', limit=1000, skip=0):
    """
    Retrieve top unique terms based on the field, term type, and optional search query.

    :param db: Database instance
    :param field: The field to filter terms by (e.g., 'title', 'description')
    :param term_type: The type of term ('word' or 'phrase')
    :param query: Optional search query string to filter terms
    :param limit: Number of top terms to retrieve
    :param skip: Number of records to skip for pagination
    :return: List of dictionaries with term and count
    """
    unique_terms_collection = db['unique_terms']
    
    try:
        # Base MongoDB query
        mongo_query = {"field": field, "type": term_type}
        
        # If a search query is provided, add a regex filter for the 'term' field
        if query:
            # Escape special regex characters to prevent injection attacks
            escaped_query = re.escape(query)
            # Case-insensitive search for terms containing the query substring
            mongo_query['term'] = {"$regex": f".*{escaped_query}.*", "$options": "i"}
        
        start_time = time.time()
        
        # Execute the query with sorting, skipping, and limiting for pagination
        cursor = unique_terms_collection.find(
            mongo_query,
            {"_id": 0, "term": 1, "frequency": 1}
        ).sort("frequency", pymongo.DESCENDING).skip(skip).limit(limit)
        
        terms_list = []
        for doc in cursor:
            key = 'word' if term_type == 'word' else 'phrase'
            terms_list.append({key: doc['term'], 'count': doc['frequency']})
        
        duration = time.time() - start_time
        logger.info(f"Retrieved top {len(terms_list)} {term_type}s in {duration:.4f} seconds for field '{field}' with query '{query}'.")
        
        return terms_list
    except Exception as e:
        logger.error(f"Error retrieving unique terms: {e}")
        return []

def get_unique_terms_count(db, field, term_type, query=''):
    """
    Get the count of unique terms based on the field, term type, and optional search query.

    :param db: Database instance
    :param field: The field to filter terms by
    :param term_type: The type of term ('word' or 'phrase')
    :param query: Optional search query string to filter terms
    :return: Integer count of unique terms
    """
    unique_terms_collection = db['unique_terms']
    
    try:
        # Base MongoDB query
        mongo_query = {"field": field, "type": term_type}
        
        # If a search query is provided, add a regex filter for the 'term' field
        if query:
            # Escape special regex characters to prevent injection attacks
            escaped_query = re.escape(query)
            # Case-insensitive search for terms containing the query substring
            mongo_query['term'] = {"$regex": f".*{escaped_query}.*", "$options": "i"}
        
        # Count the number of unique terms matching the query
        count = unique_terms_collection.count_documents(mongo_query)
        logger.info(f"Counted {count} unique {term_type}s for field '{field}' with query '{query}'.")
        return count
    except Exception as e:
        logger.error(f"Error counting unique terms: {e}")
        return 0



@app.route('/search-terms', methods=['GET'])
def search_terms():
    client = get_client()  # Initialize your MongoDB client
    db = get_db(client)    # Get the database instance

    if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
        # Handle AJAX request
        field = request.args.get('field')
        if not field:
            return jsonify({"error": "No field specified"}), 400

        # Extract the search query
        query = request.args.get('query', '').strip().lower()  # Normalize the query

        logger.debug(f"AJAX request for field: {field}, query: {query}")  # Using logger here

        # Define term types
        term_types = ['word', 'phrase']
        data = {}
        total_records = 0

        for term_type in term_types:
            page = int(request.args.get('page', 1))
            per_page = int(request.args.get('per_page', 100))
            skip = (page - 1) * per_page

            # Fetch filtered terms based on the query
            terms = get_top_unique_terms(db, field, term_type, query=query, limit=per_page, skip=skip)
            data[term_type + 's'] = terms

            # Fetch the count of unique terms based on the query
            count = get_unique_terms_count(db, field, term_type, query=query)
            data['unique_' + term_type + 's'] = count
            total_records += count

        data['total_records'] = total_records

        return jsonify(data)
    else:
        # Render the HTML template
        field_structure = get_field_structure(db)
        unique_fields = []  # Define if necessary
        return render_template('search-terms.html', field_structure=field_structure, unique_fields=unique_fields)

@app.route('/database-info')
# @login_required
def database_info():
    field_struct = get_field_structure(db)  # Pass 'db' here
    collection_info = []

    def count_documents_with_field(field_path):
        count = documents.count_documents({field_path: {'$exists': True}})
        return count

    def traverse_structure(structure, current_path=''):
        for field, value in structure.items():
            path = f"{current_path}.{field}" if current_path else field
            if isinstance(value, dict):
                traverse_structure(value, current_path=path)
            else:
                count = count_documents_with_field(path)
                collection_info.append({
                    'name': path,
                    'count': count
                })

    traverse_structure(field_struct)

    return render_template('database-info.html', collection_info=collection_info)

@app.route('/settings', methods=['GET', 'POST'])
# @login_required
def settings():
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')

    if request.method == 'POST':
        new_config = request.form.to_dict()

        for key in ['fonts', 'sizes', 'colors', 'spacing']:
            if key in new_config:
                try:
                    new_config[key] = json.loads(new_config[key])
                except json.JSONDecodeError:
                    flash(f"Invalid JSON format for {key}.", 'danger')
                    return redirect(url_for('settings'))

        try:
            with open(config_path, 'w') as config_file:
                json.dump(new_config, config_file, indent=4)
            app.config['UI_CONFIG'] = new_config
            flash('Settings updated successfully', 'success')
        except Exception as e:
            logger.error(f"Error updating settings: {str(e)}")
            flash('Failed to update settings.', 'danger')
        return redirect(url_for('settings'))

    try:
        if os.path.exists(config_path):
            with open(config_path) as config_file:
                config = json.load(config_file)
        else:
            config = {}
    except json.JSONDecodeError:
        config = {}
        flash('Configuration file is corrupted. Using default settings.', 'warning')

    return render_template('settings.html', config=config)

# Consider streaming if it ends up being thousands of documents
@app.route('/export_selected_csv', methods=['POST'])
# @login_required
def export_selected_csv():
    try:
        data = request.get_json()
        document_ids = data.get('document_ids', [])
        if not document_ids:
            return jsonify({"error": "No document IDs provided"}), 400

        # Convert string IDs to ObjectIds, handle invalid IDs
        valid_ids = []
        for doc_id in document_ids:
            try:
                valid_ids.append(ObjectId(doc_id))
            except Exception as e:
                logger.warning(f"Invalid document ID: {doc_id}")

        if not valid_ids:
            return jsonify({"error": "No valid document IDs provided"}), 400

        # Check if any documents exist with the provided IDs
        count = documents.count_documents({"_id": {"$in": valid_ids}})
        if count == 0:
            return jsonify({"error": "No documents found for the provided IDs."}), 404

        # Retrieve the documents
        documents_cursor = documents.find({"_id": {"$in": valid_ids}})

        # Create CSV
        output = StringIO()
        writer = csv.writer(output)
        writer.writerow(['filename', 'OCR', 'original_json'])  # Header row

        for doc in documents_cursor:
            filename = doc.get('filename', 'N/A')
            ocr = doc.get('summary', 'N/A')  # Adjust field as necessary
            original_json = json.dumps(doc, default=str)  # Convert ObjectId to string if necessary
            writer.writerow([filename, ocr, original_json])

        # Prepare CSV for download
        output.seek(0)
        return Response(
            output.getvalue(),
            mimetype='text/csv',
            headers={'Content-Disposition': 'attachment; filename=selected_documents.csv'}
        )

    except Exception as e:
        logger.error(f"Error exporting selected CSV: {str(e)}", exc_info=True)
        return jsonify({"error": "An internal error occurred"}), 500

@app.errorhandler(404)
def not_found_error(error):
    return render_template('error.html', message='Page not found'), 404

@app.errorhandler(500)
def internal_error(error):
    return render_template('error.html', message='An unexpected error has occurred'), 500


********************************************************************************

File: app/test_spacy.py
********************************************************************************

# Script: test_spacy_on_db_entries.py
# Purpose: Connects to MongoDB, retrieves sample documents, and tests spaCy entity recognition on 'ocr_text' field.
# Created: 2024-10-16

import os
from pymongo import MongoClient
from dotenv import load_dotenv
import spacy
import logging

# Load environment variables from .env file
load_dotenv()

# Logging configuration to log to both file and console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("test_spacy.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('TestSpacy')

def get_client():
    """Initialize and return a MongoDB client."""
    mongo_uri = os.getenv('MONGO_URI')
    if not mongo_uri:
        logger.error("MONGO_URI environment variable not set.")
        return None
    try:
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')  # Test connection
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        return None

def get_sample_documents(db, num_samples=5):
    """Retrieve a few sample documents with the 'ocr_text' field."""
    try:
        documents_collection = db['documents']
        sample_docs = documents_collection.find(
            {"ocr_text": {"$exists": True, "$ne": ""}},
            {"ocr_text": 1, "_id": 1}
        ).limit(num_samples)
        return list(sample_docs)
    except Exception as e:
        logger.error(f"Error retrieving sample documents: {e}")
        return []

def test_spacy_on_documents(nlp, documents):
    """Process the 'ocr_text' field of each document with spaCy and log recognized entities."""
    relevant_labels = {'PERSON', 'ORG', 'GPE', 'LOC'}
    texts = [doc.get("ocr_text", "") for doc in documents]
    doc_ids = [doc['_id'] for doc in documents]
    
    # Use nlp.pipe for efficient batch processing
    for doc_id, spacy_doc in zip(doc_ids, nlp.pipe(texts, batch_size=50)):
        logger.info(f"Document ID: {doc_id}")
        # Filter entities by relevant types
        filtered_entities = [ent for ent in spacy_doc.ents if ent.label_ in relevant_labels]
        if filtered_entities:
            for ent in filtered_entities:
                logger.info(f"Entity: {ent.text}, Label: {ent.label_}")
        else:
            logger.info("No relevant entities found.")

def main():
    # Initialize spaCy with only NER and disable other components for speed
    try:
        logger.info("Loading spaCy model...")
        nlp = spacy.load("en_core_web_lg", disable=["parser", "tagger", "lemmatizer"])
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}")
        return

    # Connect to MongoDB
    client = get_client()
    if not client:
        return

    db = client['railroad_documents']

    # Retrieve sample documents
    documents = get_sample_documents(db, num_samples=5)
    if not documents:
        logger.error("No sample documents retrieved.")
        return

    # Test spaCy on retrieved documents
    test_spacy_on_documents(nlp, documents)

if __name__ == "__main__":
    main()


********************************************************************************

File: app/entity_linking.py
********************************************************************************

#entity_linking.py

import os
import logging
from collections import defaultdict
from pymongo import UpdateOne, MongoClient
from dotenv import load_dotenv
import requests
import openai
from tqdm import tqdm
from rapidfuzz import process, fuzz
import uuid
import pymongo

# Load environment variables from .env file
load_dotenv()

# =======================
# Logging Configuration
# =======================
logger = logging.getLogger('EntityLinkingLogger')
logger.setLevel(logging.DEBUG)

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)

    file_handler = logging.FileHandler('entity_linking.log', mode='a')
    file_handler.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# =======================
# Database Functions
# =======================

def get_client():
    """Initialize and return a new MongoDB client."""
    try:
        mongo_uri = os.environ.get('MONGO_URI')
        if not mongo_uri:
            raise ValueError("MONGO_URI environment variable not set")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        logger.info("Successfully connected to MongoDB.")
        return client
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        raise e

def get_db(client):
    """Return the database instance."""
    return client['railroad_documents']

def get_collections(db):
    """Retrieve and return references to the required collections."""
    try:
        documents_collection = db['documents']
        linked_entities_collection = db['linked_entities']
        return documents_collection, linked_entities_collection
    except Exception as e:
        logger.error(f"Error getting collections: {e}")
        raise e

# =======================
# Utility Functions
# =======================

def generate_custom_kb_id():
    """Generate a unique custom kb_id."""
    return f"CUSTOM_{uuid.uuid4()}"

def entity_default():
    """Default factory function for defaultdict."""
    return {'frequency': 0, 'document_ids': set(), 'type': ''}

# In-memory cache for Wikidata entities
wikidata_cache = {}

def fetch_wikidata_entity(term):
    """
    Fetch Wikidata entity ID for a given term using the Wikidata API.
    Returns the entity ID if found, else None.
    """
    if term in wikidata_cache:
        return wikidata_cache[term]
    try:
        url = "https://www.wikidata.org/w/api.php"
        params = {
            'action': 'wbsearchentities',
            'search': term,
            'language': 'en',
            'format': 'json'
        }
        response = requests.get(url, params=params, timeout=5)
        data = response.json()
        if 'search' in data and len(data['search']) > 0:
            # Return the first matching entity ID
            wikidata_id = data['search'][0]['id']
            wikidata_cache[term] = wikidata_id
            logger.debug(f"Fetched Wikidata ID '{wikidata_id}' for term '{term}'.")
            return wikidata_id
        else:
            wikidata_cache[term] = None
            logger.debug(f"No Wikidata ID found for term '{term}'.")
            return None
    except Exception as e:
        logger.error(f"Error fetching Wikidata entity for term '{term}': {e}")
        return None

def link_entity_with_llm(term, context):
    """
    Use LLM (e.g., OpenAI's GPT-4) to disambiguate and link an entity.
    Returns the Wikidata ID if successful, else None.
    """
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        logger.error("OPENAI_API_KEY not set in environment variables.")
        return None

    openai.api_key = openai_api_key

    try:
        prompt = f"""
        Given the following context, disambiguate the entity "{term}" and provide its Wikidata ID.

        Context: {context}

        Only provide the Wikidata ID (e.g., Q42). If not found, respond with "Not Found".
        """
        response = openai.Completion.create(
            engine="text-davinci-003",  # Use the appropriate engine
            prompt=prompt,
            max_tokens=10,
            n=1,
            stop=None,
            temperature=0.3,
        )
        wikidata_id = response.choices[0].text.strip()
        if wikidata_id.lower() == "not found":
            logger.debug(f"LLM did not find a Wikidata ID for term '{term}'.")
            return None
        logger.debug(f"LLM linked term '{term}' to Wikidata ID '{wikidata_id}'.")
        return wikidata_id
    except Exception as e:
        logger.error(f"Error linking entity with LLM for term '{term}': {e}")
        return None

def fuzzy_match(term, reference_terms, threshold=90):
    """
    Perform fuzzy matching to find the best match for a term in reference_terms.
    Returns the matched term if similarity exceeds the threshold, else None.
    """
    try:
        result = process.extractOne(term, reference_terms, scorer=fuzz.token_sort_ratio)
        if result is None:
            logger.debug(f"No fuzzy match found for term '{term}'.")
            return None

        # Unpack only the first two elements: match and score
        match, score, *_ = result  # The *_ captures any additional elements

        logger.debug(f"Fuzzy match for term '{term}': '{match}' with score {score}.")

        if score >= threshold:
            return match
        return None
    except Exception as e:
        logger.error(f"Exception during fuzzy matching for term '{term}': {e}")
        return None

def chunkify(iterable, chunk_size):
    """
    Split an iterable into chunks of a specified size.
    """
    chunk = []
    for item in iterable:
        chunk.append(item)
        if len(chunk) == chunk_size:
            yield chunk
            chunk = []
    if chunk:
        yield chunk

# =======================
# Main Processing Functions
# =======================

def aggregate_entities(documents_collection, batch_size=1000):
    """
    Aggregate entities from the 'extracted_entities' field of documents.
    """
    # Query to fetch documents that have 'extracted_entities' and have not been processed
    query = {'extracted_entities': {'$exists': True, '$ne': []}, 'entity_linking_processed': {'$ne': True}}

    cursor = documents_collection.find(query, {'_id': 1, 'extracted_entities': 1})

    total_documents = documents_collection.count_documents(query)
    logger.info(f"Total documents with extracted entities to process: {total_documents}")

    aggregated_entities = defaultdict(entity_default)

    batches = chunkify(cursor, batch_size)
    for batch_docs in tqdm(batches, desc="Aggregating entities"):
        for doc in batch_docs:
            doc_id = doc['_id']
            entities = doc.get('extracted_entities', [])
            for entity in entities:
                term = entity['text']
                ent_type = entity['type']
                term_lower = term.lower()

                aggregated_entities[(term_lower, ent_type)]['frequency'] += 1
                aggregated_entities[(term_lower, ent_type)]['document_ids'].add(doc_id)
                aggregated_entities[(term_lower, ent_type)]['type'] = ent_type

    return aggregated_entities

def link_entities(aggregated_entities, linked_entities_collection, existing_linked_terms, link_wikidata, fuzzy_threshold, batch_size, documents_collection):
    """
    Link aggregated entities and update the linked_entities collection.
    """
    linked_count = 0
    processed_entities = 0
    total_entities = len(aggregated_entities)
    logger.info(f"Total unique entities to process: {total_entities}")

    operations = []

    for (term_lower, ent_type), data in aggregated_entities.items():
        frequency = data['frequency']
        document_ids = list(data['document_ids'])
        wikidata_id = None

        if link_wikidata:
            wikidata_id = fetch_wikidata_entity(term_lower)

        # If Wikidata linking is disabled or Wikidata ID not found, perform fuzzy matching
        if not wikidata_id:
            fuzzy_match_term = fuzzy_match(term_lower, existing_linked_terms, threshold=fuzzy_threshold)
            if fuzzy_match_term:
                # Fetch the corresponding Wikidata ID for the matched term
                matched_entity = linked_entities_collection.find_one({"term": fuzzy_match_term.lower()})
                if matched_entity:
                    wikidata_id = matched_entity.get('kb_id')
                    logger.debug(f"Fuzzy matched term '{term_lower}' to '{fuzzy_match_term}' with Wikidata ID '{wikidata_id}'.")
            else:
                logger.debug(f"No fuzzy match found for term '{term_lower}'.")

        # If still no wikidata_id, automate custom entry creation
        if not wikidata_id:
            # Generate a unique kb_id for the custom entry
            custom_kb_id = generate_custom_kb_id()
            try:
                # Insert the new linked entity
                linked_entities_collection.insert_one({
                    "term": term_lower,
                    "kb_id": custom_kb_id,
                    "frequency": frequency,
                    "document_ids": document_ids,
                    "type": ent_type
                })
                wikidata_id = custom_kb_id
                logger.debug(f"Inserted custom linked entity for term '{term_lower}' with kb_id '{custom_kb_id}'.")
            except pymongo.errors.DuplicateKeyError:
                # If the term was inserted by another process/thread, update it instead
                linked_entities_collection.update_one(
                    {"term": term_lower},
                    {
                        "$inc": {"frequency": frequency},
                        "$addToSet": {"document_ids": {"$each": document_ids}},
                        "$set": {"type": ent_type}
                    }
                )
                matched_entity = linked_entities_collection.find_one({"term": term_lower})
                wikidata_id = matched_entity.get('kb_id')
                logger.debug(f"Updated existing custom linked entity for term '{term_lower}' with kb_id '{wikidata_id}'.")

        update = {
            "$inc": {"frequency": frequency},
            "$addToSet": {"document_ids": {"$each": document_ids}},
            "$set": {"type": ent_type, "kb_id": wikidata_id}
        }

        operations.append(
            UpdateOne(
                {"term": term_lower},
                update,
                upsert=True
            )
        )

        linked_count += 1
        processed_entities += 1

        # Log progress every 1000 entities
        if processed_entities % 1000 == 0:
            logger.info(f"Processed {processed_entities}/{total_entities} entities.")

        # Execute operations in batches
        if len(operations) >= batch_size:
            logger.info(f"Writing batch of {len(operations)} entities to the database.")
            try:
                result = linked_entities_collection.bulk_write(operations, ordered=False)
                logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")
            except Exception as e:
                logger.error(f"Error bulk upserting linked entities: {e}")
                raise e
            operations = []

    # Execute remaining operations
    if operations:
        logger.info(f"Writing final batch of {len(operations)} entities to the database.")
        try:
            result = linked_entities_collection.bulk_write(operations, ordered=False)
            logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")
        except Exception as e:
            logger.error(f"Error bulk upserting linked entities: {e}")
            raise e

    # Mark documents as processed
    document_ids_to_mark = set()
    for (term_lower, ent_type), data in aggregated_entities.items():
        document_ids_to_mark.update(data['document_ids'])

    if document_ids_to_mark:
        try:
            documents_collection.update_many(
                {"_id": {"$in": list(document_ids_to_mark)}},
                {"$set": {"entity_linking_processed": True}}
            )
            logger.info(f"Marked {len(document_ids_to_mark)} documents as processed.")
        except Exception as e:
            logger.error(f"Error marking documents as processed: {e}")
            raise e

    logger.info(f"Successfully linked {linked_count} entities.")

# =======================
# Main Execution
# =======================
if __name__ == "__main__":
    try:
        client = get_client()
        db = get_db(client)
        logger.info("Connected to MongoDB.")

        documents_collection, linked_entities_collection = get_collections(db)

        # Determine if LLM integration is enabled
        enable_llm = os.getenv('ENABLE_LLM', 'False').lower() in ('true', '1', 't')
        if enable_llm:
            logger.info("LLM integration is ENABLED.")
        else:
            logger.info("LLM integration is DISABLED.")

        fuzzy_threshold = int(os.getenv('FUZZY_MATCH_THRESHOLD', 90))
        batch_size = int(os.getenv('BATCH_SIZE', 1000))  # Set default batch_size to 1000

        # Aggregate entities from unprocessed documents
        aggregated_entities = aggregate_entities(documents_collection, batch_size=batch_size)

        if not aggregated_entities:
            logger.info("No new entities to process.")
            exit(0)

        # Fetch existing linked terms for fuzzy matching
        existing_linked_terms = [doc['term'] for doc in linked_entities_collection.find({}, {'term': 1})]

        # Perform entity linking
        link_entities(
            aggregated_entities,
            linked_entities_collection,
            existing_linked_terms,
            link_wikidata=enable_llm,
            fuzzy_threshold=fuzzy_threshold,
            batch_size=batch_size,
            documents_collection=documents_collection
        )

    except Exception as e:
        logger.error(f"An error occurred during entity linking: {e}", exc_info=True)


********************************************************************************

File: app/templates/index.html
********************************************************************************

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Historical Document Reader</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Open+Sans&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="container">
        <h1 class="mt-4">Historical Document Reader</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
                {% if 'logged_in' in session %}
                    <li><a href="{{ url_for('logout') }}">Logout</a></li>
                {% else %}
                    <li><a href="{{ url_for('login') }}">Login</a></li>
                {% endif %}
            </ul>
        </nav>

        <form id="searchForm">
            <div id="searchFields">
                {% for i in range(1, num_search_fields + 1) %}
                <div class="search-field">
                    <label for="field{{ i }}">Field:</label>
                    <select name="field{{ i }}" id="field{{ i }}">
                        <option value="" disabled selected>Select a field</option>
                        {% for field in field_structure|dictsort %}
                            <option value="{{ field[0] }}">{{ field[0] }}</option>
                        {% endfor %}
                    </select>
                    <label for="operator{{ i }}">Operator:</label>
                    <select name="operator{{ i }}" id="operator{{ i }}">
                        <option value="AND">AND</option>
                        <option value="OR">OR</option>
                        <option value="NOT">NOT</option>
                    </select>
                    <label for="searchTerm{{ i }}">Search Term:</label>
                    <input type="text" name="searchTerm{{ i }}" id="searchTerm{{ i }}" placeholder="Enter search term">
                </div>
                {% endfor %}
            </div>
            <button type="submit" id="searchButton">Search</button>
            <!-- Export Selected to CSV Button -->
            <button id="exportSelectedCsv" style="display: none; margin-top: 10px;">Export Selected to CSV</button>
        </form>

        <!-- Loading Indicator -->
        <div id="loadingIndicator" style="display: none;">
            <div class="spinner"></div>
            <p>Loading... Please wait.</p>
        </div>

        <!-- Cancel Search Button -->
        <button id="cancelSearch" style="display: none;">Cancel Search</button>

        <!-- Total Results Display -->
        <div id="totalResults" class="mt-4"></div>

        <!-- Results Container -->
        <div id="results" class="mt-4">
            <!-- The table will be dynamically inserted here -->
        </div>

        
    </div>

    <!-- Include jQuery -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <!-- Include your custom script -->
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>


********************************************************************************

File: app/templates/search-terms.html
********************************************************************************

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Search Terms</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <!-- Include DataTables CSS -->
    <link rel="stylesheet" href="https://cdn.datatables.net/1.10.21/css/jquery.dataTables.css">
    <style>
        /* Spinner Styles */
        #loadingIndicator {
            display: none;
            text-align: center;
            margin-top: 20px;
        }
        .spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
            margin: 0 auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        /* Container and Navigation Styles */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        nav ul {
            list-style-type: none;
            padding: 0;
            display: flex;
            gap: 15px;
        }
        nav ul li {
            display: inline;
        }
        nav ul li a {
            text-decoration: none;
            color: #3498db;
            font-weight: bold;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .section {
            margin-top: 30px;
        }
        #noDataMessage {
            display: none;
            color: red;
            margin-top: 20px;
            font-weight: bold;
            text-align: center;
        }

        /* Pagination Controls Styles */
        .pagination-controls {
            margin-top: 20px;
            text-align: center;
        }
        .pagination-controls button {
            padding: 10px 20px;
            margin: 0 5px;
            font-size: 16px;
            cursor: pointer;
            border: none;
            background-color: #3498db;
            color: white;
            border-radius: 5px;
        }
        .pagination-controls button:disabled {
            background-color: #95a5a6;
            cursor: not-allowed;
        }

        /* Search Box Styles */
        #searchContainer {
            margin-top: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        #searchInput {
            flex: 1;
            padding: 8px;
            font-size: 16px;
        }
        #searchButton {
            padding: 8px 16px;
            font-size: 16px;
            cursor: pointer;
            border: none;
            background-color: #3498db;
            color: white;
            border-radius: 5px;
        }
        #searchButton:hover {
            background-color: #2980b9;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Search Terms</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
            </ul>
        </nav>

        <!-- Search Box Container -->
        <div id="searchContainer">
            <label for="searchInput">Search Terms:</label>
            <input type="text" id="searchInput" placeholder="Enter search term..." aria-label="Search Terms">
            <button id="searchButton" aria-label="Search">Search</button>
        </div>

        <div style="margin-top: 20px;">
            <label for="fieldSelect">Select Field:</label>
            <select id="fieldSelect">
                <option value="" disabled selected>Select a field</option>
                {% for field in field_structure|dictsort %}
                    {% if field[0] not in unique_fields %}
                        <option value="{{ field[0] }}">{{ field[0] }}</option>
                    {% endif %}
                {% endfor %}
            </select>
        </div>
        <div id="tableInfo" style="margin-top: 10px;">
            <p>Number of unique words: <span id="uniqueWords">0</span></p>
            <p>Number of unique phrases: <span id="uniquePhrases">0</span></p>
            <p>Total number of records: <span id="totalRecords">0</span></p>
        </div>
        <div id="loadingIndicator">
            <div class="spinner"></div>
            <p>Loading data...</p>
        </div>
        <!-- No Data Message -->
        <div id="noDataMessage"></div>
        <div class="section">
            <h2>Words</h2>
            <table id="wordsTable" class="display" style="width:100%; margin-top: 10px;">
                <thead>
                    <tr>
                        <th>Word</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Initially empty; populated via AJAX -->
                </tbody>
            </table>
        </div>
        <div class="section">
            <h2>Phrases</h2>
            <table id="phrasesTable" class="display" style="width:100%; margin-top: 10px;">
                <thead>
                    <tr>
                        <th>Phrase</th>
                        <th>Count</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Initially empty; populated via AJAX -->
                </tbody>
            </table>
        </div>
        <!-- Pagination Controls -->
        <div class="pagination-controls">
            <button id="prevPage" disabled>Previous</button>
            <span>Page <span id="currentPage">1</span></span>
            <button id="nextPage">Next</button>
        </div>
    </div>
    <!-- Include jQuery and DataTables JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.datatables.net/1.10.21/js/jquery.dataTables.js"></script>
    <script>
        $(document).ready(function() {
            // Initialize DataTables once
            var wordsTable = $('#wordsTable').DataTable({
                "pageLength": 25,
                "order": [[ 0, "asc" ]],
                "columns": [
                    { "data": "word" },
                    { "data": "count" }
                ],
                "paging": false,
                "info": false,
                "searching": false // Disable default search
            });

            var phrasesTable = $('#phrasesTable').DataTable({
                "pageLength": 25,
                "order": [[ 0, "asc" ]],
                "columns": [
                    { "data": "phrase" },
                    { "data": "count" }
                ],
                "paging": false,
                "info": false,
                "searching": false // Disable default search
            });

            // Function to update table information
            function updateTableInfo(data) {
                $('#uniqueWords').text(data.unique_words);
                $('#uniquePhrases').text(data.unique_phrases);
                $('#totalRecords').text(data.total_records);
            }

            // Pagination variables
            var currentPage = 1;
            var perPage = 100;
            var selectedField = '';
            var searchQuery = '';

            // Function to fetch data from the backend
            function fetchData(page) {
                $('#loadingIndicator').show();
                $('#noDataMessage').hide();

                $.ajax({
                    url: '/search-terms',
                    method: 'GET',
                    data: { 
                        field: selectedField, 
                        page: page, 
                        per_page: perPage,
                        query: searchQuery // Include search query
                    },
                    headers: {
                        'X-Requested-With': 'XMLHttpRequest'
                    },
                    success: function(data) {
                        if (data.words.length === 0 && data.phrases.length === 0) {
                            $('#noDataMessage').text('No data available for the selected field or search query.').show();
                        } else {
                            $('#noDataMessage').hide();
                        }

                        // Update the words table
                        wordsTable.clear();
                        data.words.forEach(function(item) {
                            wordsTable.row.add(item);
                        });
                        wordsTable.draw();

                        // Update the phrases table
                        phrasesTable.clear();
                        data.phrases.forEach(function(item) {
                            phrasesTable.row.add(item);
                        });
                        phrasesTable.draw();

                        // Update table info
                        updateTableInfo({
                            unique_words: data.unique_words,
                            unique_phrases: data.unique_phrases,
                            total_records: data.total_records
                        });

                        // Update pagination controls
                        $('#currentPage').text(page);
                        $('#prevPage').prop('disabled', page === 1);
                        $('#nextPage').prop('disabled', data.words.length < perPage && data.phrases.length < perPage);

                        $('#loadingIndicator').hide();
                    },
                    error: function(xhr, status, error) {
                        alert('Error fetching data: ' + error);
                        $('#loadingIndicator').hide();
                    }
                });
            }

            // Event listener for field selection
            $('#fieldSelect').on('change', function() {
                selectedField = $(this).val();
                currentPage = 1;
                fetchData(currentPage);
            });

            // Event listener for search button
            $('#searchButton').on('click', function() {
                searchQuery = $('#searchInput').val().trim();
                currentPage = 1;
                fetchData(currentPage);
            });

            // Allow pressing Enter to trigger search
            $('#searchInput').on('keyup', function(e) {
                if (e.key === 'Enter' || e.keyCode === 13) {
                    $('#searchButton').click();
                }
            });

            // Event listeners for pagination buttons
            $('#prevPage').on('click', function() {
                if (currentPage > 1) {
                    currentPage -= 1;
                    fetchData(currentPage);
                }
            });

            $('#nextPage').on('click', function() {
                currentPage += 1;
                fetchData(currentPage);
            });

            // Optional: Initial data fetch
            // fetchData(currentPage);
        });
    </script>
</body>
</html>


********************************************************************************

File: app/templates/settings.html
********************************************************************************

<!-- File: templates/settings.html -->

{% extends "base.html" %}

{% block content %}
<div class="container">
    <h1>Settings</h1>
    <form method="POST">
        <h2>Fonts</h2>
        <label for="fonts_main">Main Font:</label>
        <input type="text" id="fonts_main" name="fonts[main]" value="{{ config['fonts']['main'] }}"><br>
        <label for="fonts_headers">Headers Font:</label>
        <input type="text" id="fonts_headers" name="fonts[headers]" value="{{ config['fonts']['headers'] }}"><br>

        <h2>Sizes</h2>
        <label for="sizes_base">Base Size:</label>
        <input type="text" id="sizes_base" name="sizes[base]" value="{{ config['sizes']['base'] }}"><br>
        <label for="sizes_h1">H1 Size:</label>
        <input type="text" id="sizes_h1" name="sizes[h1]" value="{{ config['sizes']['h1'] }}"><br>
        <label for="sizes_h2">H2 Size:</label>
        <input type="text" id="sizes_h2" name="sizes[h2]" value="{{ config['sizes']['h2'] }}"><br>
        <label for="sizes_h3">H3 Size:</label>
        <input type="text" id="sizes_h3" name="sizes[h3]" value="{{ config['sizes']['h3'] }}"><br>

        <h2>Colors</h2>
        <label for="colors_primary">Primary Color:</label>
        <input type="color" id="colors_primary" name="colors[primary]" value="{{ config['colors']['primary'] }}"><br>
        <label for="colors_secondary">Secondary Color:</label>
        <input type="color" id="colors_secondary" name="colors[secondary]" value="{{ config['colors']['secondary'] }}"><br>
        <label for="colors_background">Background Color:</label>
        <input type="color" id="colors_background" name="colors[background]" value="{{ config['colors']['background'] }}"><br>
        <label for="colors_text">Text Color:</label>
        <input type="color" id="colors_text" name="colors[text]" value="{{ config['colors']['text'] }}"><br>

        <h2>Spacing</h2>
        <label for="spacing_small">Small Spacing:</label>
        <input type="text" id="spacing_small" name="spacing[small]" value="{{ config['spacing']['small'] }}"><br>
        <label for="spacing_medium">Medium Spacing:</label>
        <input type="text" id="spacing_medium" name="spacing[medium]" value="{{ config['spacing']['medium'] }}"><br>
        <label for="spacing_large">Large Spacing:</label>
        <input type="text" id="spacing_large" name="spacing[large]" value="{{ config['spacing']['large'] }}"><br>

        <button type="submit">Save Settings</button>
    </form>
</div>
<script>
    // Convert nested objects to JSON strings before form submission
    document.querySelector('form').addEventListener('submit', function(e) {
        ['fonts', 'sizes', 'colors', 'spacing'].forEach(function(key) {
            var inputs = document.querySelectorAll(`[name^="${key}\\["]`);
            var obj = {};
            inputs.forEach(function(input) {
                var prop = input.name.match(/\[(.*?)\]/)[1];
                obj[prop] = input.value;
            });
            var hiddenInput = document.createElement('input');
            hiddenInput.type = 'hidden';
            hiddenInput.name = key;
            hiddenInput.value = JSON.stringify(obj);
            this.appendChild(hiddenInput);
        }, this);
    });
</script>
{% endblock %}


********************************************************************************

File: app/templates/error.html
********************************************************************************

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Error</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <h1>Error</h1>
    <p>{{ message }}</p>
    <a href="{{ url_for('index') }}">Return to Home</a>
</body>
</html>


********************************************************************************

File: app/templates/login.html
********************************************************************************

<!-- File: templates/login.html -->
<!-- Created: 2024-08-12 11:00 -->

{% extends "base.html" %}

{% block content %}
    <h1>Login</h1>
    
    {% with messages = get_flashed_messages() %}
        {% if messages %}
            <ul class="flashes">
                {% for message in messages %}
                    <li>{{ message }}</li>
                {% endfor %}
            </ul>
        {% endif %}
    {% endwith %}
    
    <form method="post">
        <label for="password">Password:</label>
        <input type="password" id="password" name="password" required>
        <br><br>
        
        <label for="captcha">CAPTCHA: What is {{ captcha_num1 }} + {{ captcha_num2 }}?</label>
        <input type="number" id="captcha" name="captcha" required>
        <input type="hidden" name="captcha_answer" value="{{ captcha_answer }}">
        <br><br>
        
        <input type="submit" value="Login">
    </form>
{% endblock %}

********************************************************************************

File: app/templates/base.html
********************************************************************************

<!-- templates/base.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>{% block title %}Historical Document Reader{% endblock %}</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <!-- Add this in the <head> section of base.html -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-yHnj4MvlYYKxE/6q9ZsH5mJ5RkJyKpjtXHYQVdG1Vl6HjnAsfsj0G9aJv1bQZoKEtZnSmhMWj58ZwFZJe61D1A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    {% block styles %}{% endblock %}
</head>
<body>
    <nav>
        <ul>
            <li><a href="{{ url_for('index') }}">Home</a></li>
            {% if session.get('logged_in') %}
                <li><a href="{{ url_for('logout') }}">Logout</a></li>
            {% else %}
                <li><a href="{{ url_for('login') }}">Login</a></li>
            {% endif %}
            <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
            <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
            <li><a href="{{ url_for('settings') }}">Settings</a></li>
        </ul>
    </nav>
    
    {% with messages = get_flashed_messages() %}
        {% if messages %}
            <ul class="flashes">
                {% for message in messages %}
                    <li>{{ message }}</li>
                {% endfor %}
            </ul>
        {% endif %}
    {% endwith %}
    
    {% block content %}{% endblock %}
    {% block scripts %}{% endblock %}
</body>
</html>


********************************************************************************

File: app/templates/database-info.html
********************************************************************************

<!-- templates/database-info.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Database Information</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="container">
        <h1>Database Information</h1>
        <nav>
            <ul>
                <li><a href="{{ url_for('index') }}">Home</a></li>
                <li><a href="{{ url_for('search_terms') }}">Search Terms</a></li>
                <li><a href="{{ url_for('database_info') }}">Database Info</a></li>
                <li><a href="{{ url_for('settings') }}">Settings</a></li>
            </ul>
        </nav>
        <table>
            <thead>
                <tr>
                    <th>Field Name</th>
                    <th>Number of Records</th>
                </tr>
            </thead>
            <tbody>
                {% for field in collection_info %}
                <tr>
                    <td>{{ field.name }}</td>
                    <td>{{ field.count }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
</body>
</html>


********************************************************************************

File: app/templates/document-detail.html
********************************************************************************

{% extends "base.html" %}

{% macro render_section(section) %}
  <div class="info-section">
    <h2>{{ section.section_name | replace('_', ' ') | title }}</h2>
    <table class="info-table">
      {% for field in section.fields %}
        <tr>
          <th>{{ field.field_name | replace('_', ' ') | title }}</th>
          <td>
            {{ field.value if field.value is not none else 'N/A' }}
            {% if field.linked_information %}
              {{ render_linked_information(field.linked_information) }}
            {% endif %}
          </td>
        </tr>
      {% endfor %}
    </table>
  </div>
{% endmacro %}

{% macro render_linked_information(linked_info) %}
  {% if linked_info %}
    <div class="linked-information">
      <h4>Linked Information:</h4>
      {% for key, value in linked_info.items() %}
        {% if value %}
          <div class="linked-section">
            <h5>{{ key.replace('_', ' ') | title }}</h5>
            {% if value is mapping %}
              <table class="info-table nested">
                {% for subkey, subvalue in value.items() %}
                  <tr>
                    <th>{{ subkey.replace('_', ' ') | title }}</th>
                    <td>
                      {% if subvalue is mapping or (subvalue is iterable and not subvalue is string) %}
                        {{ render_nested_value(subvalue) }}
                      {% else %}
                        {{ subvalue }}
                      {% endif %}
                    </td>
                  </tr>
                {% endfor %}
              </table>
            {% elif value is iterable and not value is string %}
              <div class="nested-block">
                {% for item in value %}
                  <div class="nested-item">
                    {% if item is mapping %}
                      {{ render_nested_value(item) }}
                    {% else %}
                      {{ item }}
                    {% endif %}
                  </div>
                {% endfor %}
              </div>
            {% else %}
              {{ value }}
            {% endif %}
          </div>
        {% endif %}
      {% endfor %}
    </div>
  {% endif %}
{% endmacro %}

{% macro render_nested_value(value) %}
  {% if value is mapping %}
    <table class="info-table nested">
      {% for subkey, subvalue in value.items() %}
        <tr>
          <th>{{ subkey.replace('_', ' ') | title }}</th>
          <td>
            {% if subvalue is mapping or (subvalue is iterable and not subvalue is string) %}
              {{ render_nested_value(subvalue) }}
            {% else %}
              {{ subvalue }}
            {% endif %}
          </td>
        </tr>
      {% endfor %}
    </table>
  {% elif value is iterable and not value is string %}
    <div class="nested-block">
      {% for item in value %}
        <div class="nested-item">
          {% if item is mapping %}
            {{ render_nested_value(item) }}
          {% else %}
            {{ item }}
          {% endif %}
        </div>
      {% endfor %}
    </div>
  {% else %}
    {{ value }}
  {% endif %}
{% endmacro %}

{% block styles %}
<style>
  /* Nested Blocks */
  .nested-block {
    background-color: #f9f9f9; /* Light gray background */
    padding: 10px;
    border-radius: 5px;
    margin-top: 10px;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  }

  .nested-item {
    background-color: #e0f7fa; /* Light cyan background */
    padding: 8px;
    border-radius: 4px;
    margin-bottom: 5px;
  }
  .nested-item:nth-child(even) {
    background-color: #e0f7fa; /* Light cyan */
  }

  .nested-item:nth-child(odd) {
    background-color: #b2ebf2; /* Slightly darker cyan */
  }
  
  /* Layout Containers */
  .document-detail .detail-container {
    display: flex;
    gap: 20px;
  }

  .document-detail .info-panel {
    flex: 1;
    overflow-y: auto;
    max-height: calc(100vh - 40px);
  }

  .document-detail .image-panel {
    flex: 1;
    position: sticky;
    top: 20px;
    align-self: flex-start;
  }

  /* Image Container */
  #imageContainer {
    width: 100%;
    height: auto;
    max-height: calc(100vh - 100px);
    overflow: auto;
    cursor: grab;
    position: relative;
  }

  #imageContainer.active {
    cursor: grabbing;
  }

  #documentImage {
    display: block;
    transform-origin: center;
  }

  /* Controls */
  .zoom-controls,
  .adjustment-controls {
    margin-top: 10px;
  }

  .zoom-controls button,
  .adjustment-controls button {
    padding: 8px 12px;
    font-size: 14px;
  }

  .adjustment-controls label {
    margin-right: 10px;
  }

  .adjustment-controls {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
    gap: 10px;
    align-items: center;
  }

  .adjustment-controls .control-group {
    display: flex;
    align-items: center;
  }

  .adjustment-controls .control-group label {
    flex: 1;
  }

  .adjustment-controls .control-group input[type="range"] {
    flex: 2;
  }

  .adjustment-controls .buttons-group {
    grid-column: span 2;
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
  }

  /* Optional: Style for better tooltip visibility */
  input[type="range"] {
    position: relative;
  }

  /* Information Sections */
  .document-detail .info-section {
    margin-bottom: 20px;
  }

  /* Tables */
  .info-table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 10px;
  }

  .info-table th {
    text-align: left;
    padding: 8px;
    background-color: #f2f2f2;
    width: 30%;
    vertical-align: top;
  }

  .info-table td {
    padding: 8px;
    border-bottom: 1px solid #ddd;
  }

  .info-table.nested th {
    padding-left: 20px;
    background-color: #e6e6e6;
  }

  /* Linked Information */
  .linked-information {
    margin-top: 10px;
    padding-left: 10px;
    border-left: 2px solid #ccc;
  }

  .linked-section {
    margin-bottom: 10px;
  }

  .linked-section h5 {
    margin-bottom: 5px;
  }

  /* Lists within Tables */
  .info-table ul {
    list-style-type: disc;
    margin-left: 40px;
  }



    /* Extracted Entities Section */
  .entity-group {
    margin-bottom: 15px;
  }

  .entity-group h3 {
    text-transform: uppercase;
    color: #333;
    margin-bottom: 5px;
  }

  .entity-group ul {
    list-style-type: disc;
    margin-left: 20px;
    padding-left: 0;
  }

  .entity-group li {
    padding: 3px 0;
    color: #555;
  }

</style>
{% endblock %}

{% block content %}
<div class="document-detail">
  <div class="detail-container">
    <div class="info-panel">
      <!-- Navigation -->
      <div class="navigation">
        <a href="{{ url_for('index') }}?return_to_search=true" class="nav-button" title="Return to Search Results">
          <span>Search</span>
        </a>
        <a href="{{ prev_id and url_for('document_detail', doc_id=prev_id, search_id=search_id) or '#' }}" class="nav-button {{ 'disabled' if not prev_id else '' }}" title="Previous Result">
          <span>Previous</span>
        </a>
        <a href="{{ url_for('index') }}" class="nav-button" title="Home">
          <span>Home</span>
        </a>
        <a href="{{ next_id and url_for('document_detail', doc_id=next_id, search_id=search_id) or '#' }}" class="nav-button {{ 'disabled' if not next_id else '' }}" title="Next Result">
          <span>Next</span>
        </a>
      </div>

      <!-- Document Title -->
      <h1>{{ document.get('filename', 'Untitled Document') }}</h1>

      <div class="info-container">
        <!-- Render Summary -->
        {% if document.summary %}
          <div class="info-section">
            <h2>Summary</h2>
            <p>{{ document.summary }}</p>
          </div>
        {% endif %}

        <!-- Render OCR Text -->
        {% if document.ocr_text %}
          <div class="info-section">
            <h2>OCR Text</h2>
            <p>{{ document.ocr_text }}</p>
          </div>
        {% endif %}

        <!-- Render Sections -->
        {% for section in document.sections %}
          {{ render_section(section) }}
        {% endfor %}

        <!-- Render All Extracted Entities -->
        {% if document.extracted_entities %}
        <div class="info-section">
          <h2>Extracted Entities</h2>
          {% for type, entities in document.extracted_entities | groupby('type') %}
            <div class="entity-group">
              <h3>{{ type }}</h3>
              <ul>
                {% for entity in entities %}
                  <li>{{ entity.text }}</li>
                {% endfor %}
              </ul>
            </div>
          {% endfor %}
  </div>
{% endif %}
      </div>
    </div>
    
    <!-- Image Panel -->
    <div class="image-panel">
      {% if image_exists %}
        <div id="imageContainer">
          <img id="documentImage" src="{{ url_for('serve_image', filename=image_path) }}" alt="Document Image">
        </div>
        <div class="zoom-controls">
          <button id="rotateLeft">Rotate Left</button>
          <button id="rotateRight">Rotate Right</button>
          <button id="zoomIn">Zoom In</button>
          <button id="zoomOut">Zoom Out</button>
          <button id="resetZoom">Reset</button>
        </div>
        <div class="adjustment-controls">
          <div class="control-group">
            <label for="brightnessRange">Brightness:</label>
            <input type="range" id="brightnessRange" min="0.5" max="1.5" step="0.1" value="1" title="1">
          </div>
          
          <div class="control-group">
            <label for="contrastRange">Contrast:</label>
            <input type="range" id="contrastRange" min="0.5" max="2" step="0.1" value="1" title="1">
          </div>
          
          <div class="control-group">
            <label for="saturationRange">Saturation:</label>
            <input type="range" id="saturationRange" min="0" max="3" step="0.1" value="1" title="1">
          </div>
          
          <div class="control-group">
            <label for="hueRange">Hue Rotation:</label>
            <input type="range" id="hueRange" min="0" max="360" step="10" value="0" title="0">
          </div>
          
          <div class="control-group">
            <label for="sharpenRange">Sharpen:</label>
            <input type="range" id="sharpenRange" min="0" max="5" step="0.1" value="0" title="0">
          </div>
          
          <div class="control-group">
            <label for="thresholdRange">Threshold:</label>
            <input type="range" id="thresholdRange" min="0" max="255" step="1" value="128" title="128">
          </div>
          
          <div class="buttons-group">
            <button id="grayscaleBtn">Grayscale</button>
            <button id="sepiaBtn">Sepia</button>
            <button id="invertBtn">Invert Colors</button>
            <button id="flipHorizontalBtn">Flip Horizontal</button>
            <button id="flipVerticalBtn">Flip Vertical</button>
            <button id="resetAllBtn">Reset All</button>
          </div>
        </div>
        <!-- SVG Filters -->
        <svg width="0" height="0">
          <!-- Sharpen Filter -->
          <filter id="sharpen">
            <feConvolveMatrix in="SourceGraphic" order="3" kernelMatrix="0 -1 0 -1 5 -1 0 -1 0" />
          </filter>
          
          <!-- Threshold Filter -->
          <filter id="threshold">
            <feComponentTransfer>
              <feFuncR type="linear" slope="1" intercept="0"/>
              <feFuncG type="linear" slope="1" intercept="0"/>
              <feFuncB type="linear" slope="1" intercept="0"/>
            </feComponentTransfer>
            <feColorMatrix type="matrix" values="
              1 0 0 0 0
              0 1 0 0 0
              0 0 1 0 0
              0 0 0 1 0" />
            <feComponentTransfer>
              <feFuncR type="discrete" tableValues="0 1"/>
              <feFuncG type="discrete" tableValues="0 1"/>
              <feFuncB type="discrete" tableValues="0 1"/>
            </feComponentTransfer>
          </filter>
        </svg>
      {% else %}
        <div class="placeholder-image">
          <p>Image not found: {{ image_path }}</p>
        </div>
      {% endif %}
    </div>
    
  </div>
</div>
{% endblock %}

{% block scripts %}
<script>
  document.addEventListener('DOMContentLoaded', () => {
    let scale = 1;
    let rotation = 0;
    const ZOOM_STEP = 0.1;
    const MAX_SCALE = 3;
    const MIN_SCALE = 0.5;

    // Filter Controls
    const brightnessRange = document.getElementById('brightnessRange');
    const contrastRange = document.getElementById('contrastRange');
    const saturationRange = document.getElementById('saturationRange');
    const hueRange = document.getElementById('hueRange');
    const sharpenRange = document.getElementById('sharpenRange');
    const thresholdRange = document.getElementById('thresholdRange');

    // Buttons
    const grayscaleBtn = document.getElementById('grayscaleBtn');
    const sepiaBtn = document.getElementById('sepiaBtn');
    const invertBtn = document.getElementById('invertBtn');
    const flipHorizontalBtn = document.getElementById('flipHorizontalBtn');
    const flipVerticalBtn = document.getElementById('flipVerticalBtn');
    const resetAllBtn = document.getElementById('resetAllBtn');

    // Image Elements
    const imageContainer = document.getElementById('imageContainer');
    const documentImage = document.getElementById('documentImage');

    // SVG Filters
    const sharpenFilter = document.getElementById('sharpen');
    const thresholdFilter = document.getElementById('threshold');

    // State Variables
    let isGrayscale = false;
    let isSepia = false;
    let isInverted = false;
    let isFlippedHorizontal = false;
    let isFlippedVertical = false;

    // Transform Functions
    function setImageTransform() {
      let transforms = [];

      // Scaling
      transforms.push(`scale(${scale})`);

      // Rotation
      transforms.push(`rotate(${rotation}deg)`);

      // Flipping
      const flipX = isFlippedHorizontal ? -1 : 1;
      const flipY = isFlippedVertical ? -1 : 1;
      transforms.push(`scale(${flipX}, ${flipY})`);

      documentImage.style.transform = transforms.join(' ');
    }

    // Filter Update Function
    function updateFilters() {
      let filters = [];

      // Basic Filters
      const brightnessValue = brightnessRange.value;
      const contrastValue = contrastRange.value;
      const saturationValue = saturationRange.value;
      const hueValue = hueRange.value;
      const sharpenValue = parseFloat(sharpenRange.value);
      const thresholdValue = parseInt(thresholdRange.value, 10);

      // Update title attributes
      brightnessRange.title = brightnessValue;
      contrastRange.title = contrastValue;
      saturationRange.title = saturationValue;
      hueRange.title = hueValue;
      sharpenRange.title = sharpenValue;
      thresholdRange.title = thresholdValue;

      filters.push(`brightness(${brightnessValue})`);
      filters.push(`contrast(${contrastValue})`);
      filters.push(`saturate(${saturationValue})`);
      filters.push(`hue-rotate(${hueValue}deg)`);

      // Sharpen
      if (sharpenValue > 0) {
        const newCenter = 5 + (sharpenValue * 2);
        sharpenFilter.setAttribute('kernelMatrix', `0 -1 0 -1 ${newCenter} -1 0 -1 0`);
        filters.push('url(#sharpen)');
      } else {
        sharpenFilter.setAttribute('kernelMatrix', '0 0 0 0 1 0 0 0 0');
      }

      // Threshold
      if (thresholdValue !== 128) { // Default value
        const normalized = thresholdValue / 255;
        thresholdFilter.querySelectorAll('feFuncR, feFuncG, feFuncB').forEach(func => {
          func.setAttribute('slope', '1');
          func.setAttribute('intercept', normalized < 0.5 ? '-1' : '0');
        });
        filters.push('url(#threshold)');
      }

      // Grayscale
      if (isGrayscale) {
        filters.push('grayscale(1)');
      }

      // Sepia
      if (isSepia) {
        filters.push('sepia(1)');
      }

      // Invert
      if (isInverted) {
        filters.push('invert(1)');
      }

      documentImage.style.filter = filters.join(' ');
    }

    // Event Listeners for Sliders
    [brightnessRange, contrastRange, saturationRange, hueRange, sharpenRange, thresholdRange].forEach(slider => {
      slider.addEventListener('input', updateFilters);
    });

    // Button Event Listeners
    grayscaleBtn.addEventListener('click', () => {
      isGrayscale = !isGrayscale;
      if (isGrayscale) isSepia = false;
      sepiaBtn.textContent = isGrayscale ? 'Sepia' : 'Sepia';
      updateFilters();
    });

    sepiaBtn.addEventListener('click', () => {
      isSepia = !isSepia;
      if (isSepia) isGrayscale = false;
      grayscaleBtn.textContent = isSepia ? 'Grayscale' : 'Grayscale';
      updateFilters();
    });

    invertBtn.addEventListener('click', () => {
      isInverted = !isInverted;
      updateFilters();
    });

    flipHorizontalBtn.addEventListener('click', () => {
      isFlippedHorizontal = !isFlippedHorizontal;
      setImageTransform();
    });

    flipVerticalBtn.addEventListener('click', () => {
      isFlippedVertical = !isFlippedVertical;
      setImageTransform();
    });

    resetAllBtn.addEventListener('click', () => {
      // Reset transforms
      scale = 1;
      rotation = 0;
      isFlippedHorizontal = false;
      isFlippedVertical = false;
      setImageTransform();

      // Reset filters
      brightnessRange.value = 1;
      contrastRange.value = 1;
      saturationRange.value = 1;
      hueRange.value = 0;
      sharpenRange.value = 0;
      thresholdRange.value = 128;
      isGrayscale = false;
      isSepia = false;
      isInverted = false;
      updateFilters();
    });

    // Zoom and Rotate Functions
    const zoomInBtn = document.getElementById('zoomIn');
    const zoomOutBtn = document.getElementById('zoomOut');
    const resetZoomBtn = document.getElementById('resetZoom');
    const rotateLeftBtn = document.getElementById('rotateLeft');
    const rotateRightBtn = document.getElementById('rotateRight');

    function zoomIn() {
      if (scale < MAX_SCALE) {
        scale += ZOOM_STEP;
        setImageTransform();
      }
    }

    function zoomOut() {
      if (scale > MIN_SCALE) {
        scale -= ZOOM_STEP;
        setImageTransform();
      }
    }

    function resetZoom() {
      scale = 1;
      rotation = 0;
      setImageTransform();
    }

    function rotateLeft() {
      rotation -= 90;
      setImageTransform();
    }

    function rotateRight() {
      rotation += 90;
      setImageTransform();
    }

    zoomInBtn.addEventListener('click', zoomIn);
    zoomOutBtn.addEventListener('click', zoomOut);
    resetZoomBtn.addEventListener('click', resetZoom);
    rotateLeftBtn.addEventListener('click', rotateLeft);
    rotateRightBtn.addEventListener('click', rotateRight);

    // Dragging Functionality
    let isDragging = false;
    let startX, startY, scrollLeft, scrollTop;

    imageContainer.addEventListener('mousedown', (e) => {
      isDragging = true;
      imageContainer.classList.add('active');
      startX = e.pageX - imageContainer.offsetLeft;
      startY = e.pageY - imageContainer.offsetTop;
      scrollLeft = imageContainer.scrollLeft;
      scrollTop = imageContainer.scrollTop;
    });

    imageContainer.addEventListener('mouseleave', () => {
      isDragging = false;
      imageContainer.classList.remove('active');
    });

    imageContainer.addEventListener('mouseup', () => {
      isDragging = false;
      imageContainer.classList.remove('active');
    });

    imageContainer.addEventListener('mousemove', (e) => {
      if (!isDragging) return;
      e.preventDefault();
      const x = e.pageX - imageContainer.offsetLeft;
      const y = e.pageY - imageContainer.offsetTop;
      const walkX = x - startX;
      const walkY = y - startY;
      imageContainer.scrollLeft = scrollLeft - walkX;
      imageContainer.scrollTop = scrollTop - walkY;
    });

    // Initial Filter Setup
    updateFilters();
  });
</script>
{% endblock %}


********************************************************************************

File: app/templates/document-list.html
********************************************************************************

<!-- templates/document-list.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document List</title>
</head>
<body>
    <h1>Document List</h1>

    <ul id="documentList">
    {% for document in documents %}
        <li>
            <a href="{{ url_for('document_detail', doc_id=document['_id']) }}" target="_blank" rel="noopener noreferrer" class="document-link">{{ document.get('filename', 'Untitled Document') }}</a>
        </li>
    {% endfor %}
    </ul>
</body>
</html>


********************************************************************************

File: app/static/style.css
********************************************************************************

/* static/style.css */

/* Base Styles */
body {
    font-family: 'Open Sans', sans-serif;
    font-size: 16px;
    line-height: 1.6;
    background-color: #ffffff;
    color: #333;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

h1, h2, h3 {
    font-family: 'Montserrat', sans-serif;
    color: #333;
    margin-bottom: 20px;
}

/* Navigation */
nav ul {
    list-style-type: none;
    padding: 0;
    display: flex;
    gap: 15px;
    margin-bottom: 20px;
}

nav ul li {
    display: inline;
}

nav ul li a {
    text-decoration: none;
    color: #007bff;
    font-weight: bold;
}

nav ul li a:hover {
    text-decoration: underline;
}

/* Flash Messages */
.flashes {
    list-style-type: none;
    padding: 0;
    color: red;
}

/* Search Form */
.search-field {
    display: flex;
    gap: 10px;
    margin-bottom: 15px;
    align-items: center;
}

input[type="text"], select {
    padding: 10px;
    border: 1px solid #ccc;
    border-radius: 4px;
    font-size: 16px;
}

button {
    padding: 10px 20px;
    background-color: #007bff;
    color: #fff;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s ease;
    font-size: 16px;
}

button:hover {
    background-color: #0056b3;
}

/* Navigation Buttons */
.nav-button {
    display: inline-block;
    padding: 8px 12px;
    margin-right: 5px;
    background-color: #007bff;
    color: #fff;
    text-decoration: none;
    border-radius: 4px;
    transition: background-color 0.3s ease;
    font-size: 14px;
    text-align: center;
}

.nav-button:hover {
    background-color: #0056b3;
}

.nav-button.disabled {
    background-color: #6c757d;
    pointer-events: none;
    cursor: default;
}

/* Results Table */
#results table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 20px;
}

#results th, #results td {
    padding: 12px;
    text-align: left;
    border-bottom: 1px solid #ddd;
}

#results th {
    background-color: #f0f0f0;
    font-weight: bold;
}

#results tbody tr:nth-child(odd) {
    background-color: #f9f9f9;
}

#results tbody tr:nth-child(even) {
    background-color: #ffffff;
}

#results tbody tr:hover {
    background-color: #eaeaea;
}

#results a {
    color: #007bff;
    text-decoration: none;
}

#results a:hover {
    text-decoration: underline;
}

/* Loading Indicator */
#loadingIndicator {
    text-align: center;
    margin-top: 20px;
}

.spinner {
    border: 4px solid #f3f3f3;
    border-top: 4px solid #3498db;
    border-radius: 50%;
    width: 40px;
    height: 40px;
    animation: spin 1s linear infinite;
    margin: 0 auto;
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

/* Additional Styles */
.mt-4 {
    margin-top: 1rem;
}

.ml-auto {
    margin-left: auto;
}

/* Error Messages */
.error {
    color: #dc3545;
    font-weight: bold;
}

/* Document Detail Styles */
.document-detail .detail-container {
    display: flex;
    gap: 20px;
}

.document-detail .info-panel {
    flex: 1;
}

.document-detail .image-panel {
    flex: 1;
    position: relative;
}

#imageContainer {
    overflow: auto;
    max-width: 100%;
    max-height: 600px;
}

#documentImage {
    max-width: 100%;
    max-height: 100%;
    transform-origin: top left;
}

.zoom-controls {
    margin-top: 10px;
}

.zoom-controls button {
    margin-right: 5px;
}

.info-section {
    margin-bottom: 20px;
    border: 1px solid #e0e0e0;
    border-radius: 5px;
    padding: 15px;
}

.info-table, .nested-table {
    width: 100%;
    border-collapse: collapse;
}

.info-table th, .info-table td,
.nested-table th, .nested-table td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
}

.info-table th, .nested-table th {
    background-color: #f2f2f2;
}

.nested-list, .info-list {
    list-style-type: none;
    padding-left: 0;
}

.nested-list li, .info-list li {
    margin-bottom: 10px;
}


********************************************************************************

File: app/static/script.js
********************************************************************************

document.addEventListener('DOMContentLoaded', function() {
    // ===============================
    // Initialization
    // ===============================
    const searchForm = document.getElementById('searchForm');
    const resultsDiv = document.getElementById('results');
    const loadingIndicator = document.getElementById('loadingIndicator');
    const cancelButton = document.getElementById('cancelSearch');
    const totalResultsDiv = document.getElementById('totalResults');
    const exportSelectedCsvButton = document.getElementById('exportSelectedCsv');

    // Add console warnings for missing elements
    if (!searchForm) console.warn('Search form not found');
    if (!resultsDiv) console.warn('Results div not found');
    if (!loadingIndicator) console.warn('Loading indicator not found');
    if (!cancelButton) console.warn('Cancel button not found');
    if (!totalResultsDiv) console.warn('Total results div not found');
    if (!exportSelectedCsvButton) console.warn('Export Selected CSV button not found');

    // Pagination and Search Variables
    let controller;
    let page = 1;
    let totalPages = 1;
    const perPage = 50;  // Fixed number of results per request
    let totalResults = 0;
    let isLoading = false;
    let hasMore = true;
    let currentQuery = {};
    let prefetchedData = null;

    // Selection Management
    let selectedDocuments = new Set();

    // Variable to store current search_id
    let searchId = null;

    // ===============================
    // Utility Functions
    // ===============================

    /**
     * Debounce function to limit the rate at which a function can fire.
     * @param {Function} func - The function to debounce.
     * @param {number} delay - The delay in milliseconds.
     * @returns {Function} - The debounced function.
     */
    function debounce(func, delay) {
        let timeoutId;
        return function(...args) {
            clearTimeout(timeoutId);
            timeoutId = setTimeout(() => func.apply(this, args), delay);
        };
    }

    /**
     * Toggles the selection of a document.
     * @param {string} docId - The ID of the document.
     * @param {boolean} isSelected - Whether the document is selected.
     */
    function toggleDocumentSelection(docId, isSelected) {
        if (isSelected) {
            selectedDocuments.add(docId);
        } else {
            selectedDocuments.delete(docId);
        }
        updateExportButtonVisibility();
        saveSelectedDocuments();
    }

    /**
     * Updates the visibility of the Export Selected button based on selections.
     */
    function updateExportButtonVisibility() {
        if (!exportSelectedCsvButton) return;
        if (selectedDocuments.size > 0) {
            exportSelectedCsvButton.style.display = 'inline-block';
        } else {
            exportSelectedCsvButton.style.display = 'none';
        }
    }

    /**
     * Saves the selected documents to localStorage.
     */
    function saveSelectedDocuments() {
        localStorage.setItem('selectedDocuments', JSON.stringify(Array.from(selectedDocuments)));
    }

    /**
     * Loads the selected documents from localStorage.
     */
    function loadSelectedDocuments() {
        const savedSelectedDocuments = JSON.parse(localStorage.getItem('selectedDocuments') || '[]');
        savedSelectedDocuments.forEach(id => selectedDocuments.add(id));
        updateExportButtonVisibility();
    }

    /**
     * Shows the loading indicator.
     */
    function showLoadingIndicator() {
        if (loadingIndicator && loadingIndicator.style) {
            loadingIndicator.style.display = 'block';
        }
    }

    /**
     * Hides the loading indicator.
     */
    function hideLoadingIndicator() {
        if (loadingIndicator && loadingIndicator.style) {
            loadingIndicator.style.display = 'none';
        }
    }

    /**
     * Resets the search parameters and UI elements.
     */
    function resetSearch() {
        page = 1;
        hasMore = true;
        if (resultsDiv) resultsDiv.innerHTML = '';
        if (totalResultsDiv) totalResultsDiv.textContent = '';
        searchId = null;  // Reset search_id for new search
    }

    /**
     * Gathers search parameters from the form.
     */
    function gatherSearchParameters() {
        const formData = new FormData(searchForm);
        currentQuery = {};
        for (let i = 1; i <= 3; i++) {
            currentQuery[`field${i}`] = formData.get(`field${i}`);
            currentQuery[`operator${i}`] = formData.get(`operator${i}`);
            currentQuery[`searchTerm${i}`] = formData.get(`searchTerm${i}`);
        }
    }

    /**
     * Validates the search parameters.
     * @returns {boolean} True if valid, else false.
     */
    function validateSearchParameters() {
        if (!currentQuery['field1'] || !currentQuery['searchTerm1']) {
            console.error('Please enter a valid search term in the first field.');
            alert('Please enter a valid search term in the first field.');
            return false;
        }
        return true;
    }

    /**
     * Handles the scroll event for infinite scrolling.
     */
    function handleScroll() {
        const scrollPosition = window.innerHeight + window.scrollY;
        const threshold = document.body.offsetHeight - 100;

        if (scrollPosition >= threshold) {
            if (prefetchedData) {
                // Use prefetched data
                appendResults(prefetchedData.documents);
                page += 1;
                totalPages = prefetchedData.total_pages;
                totalResults = prefetchedData.total_count;
                updateTotalResults();

                // Clear prefetched data and prefetch the next page
                prefetchedData = null;
                if (page <= totalPages) {
                    prefetchNextPage();
                } else {
                    hasMore = false;
                }
            } else {
                performSearch();
            }
        } else if (prefetchedData === null && (scrollPosition >= threshold / 2)) {
            // Start prefetching when user scrolls halfway
            prefetchNextPage();
        }
    }

    // ===============================
    // Event Listeners
    // ===============================

    // Handle form submission for search
    if (searchForm) {
        searchForm.addEventListener('submit', function(e) {
            e.preventDefault();
            resetSearch();
            gatherSearchParameters();
            if (validateSearchParameters()) {
                performSearch(true);
            }
        });
    }

    // Handle checkbox changes using event delegation
    if (resultsDiv) {
        resultsDiv.addEventListener('change', function(e) {
            if (e.target && e.target.matches('.select-document')) {
                const docId = e.target.getAttribute('data-doc-id');
                toggleDocumentSelection(docId, e.target.checked);
            }
        });
    }

    // Handle "Select All" functionality
    if (resultsDiv) {
        resultsDiv.addEventListener('change', function(e) {
            if (e.target && e.target.matches('#selectAll')) {
                const checkboxes = resultsDiv.querySelectorAll('.select-document');
                const isChecked = e.target.checked;
                checkboxes.forEach(cb => {
                    cb.checked = isChecked;
                    const docId = cb.getAttribute('data-doc-id');
                    if (isChecked) {
                        selectedDocuments.add(docId);
                    } else {
                        selectedDocuments.delete(docId);
                    }
                });
                updateExportButtonVisibility();
                saveSelectedDocuments();
            }
        });
    }

    // Handle Export Selected to CSV Button Click
    if (exportSelectedCsvButton) {
        exportSelectedCsvButton.addEventListener('click', function() {
            exportSelectedDocuments();
        });
    }

    // Handle Cancel Search Button Click
    if (cancelButton) {
        cancelButton.addEventListener('click', function() {
            cancelSearch();
        });
    }

    // Handle Infinite Scroll with Debounce
    window.addEventListener('scroll', debounce(handleScroll, 200));

    // ===============================
    // Search Functionality
    // ===============================

    /**
     * Performs the search by sending a POST request to the server.
     * @param {boolean} isNewSearch - Indicates if it's a new search.
     */
    function performSearch(isNewSearch = false) {
        if (isLoading || !hasMore) return;
        isLoading = true;

        if (isNewSearch) {
            prefetchedData = null;
            searchId = null;  // Reset search_id for new search
        }

        showLoadingIndicator();
        if (cancelButton) cancelButton.style.display = 'inline-block';

        // Add page and perPage to currentQuery
        currentQuery.page = page;
        currentQuery.per_page = perPage;

        controller = new AbortController();
        const signal = controller.signal;

        fetch('/search', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(currentQuery),
            signal: signal
        })
        .then(response => response.json())
        .then(data => {
            hideLoadingIndicator();
            if (cancelButton) cancelButton.style.display = 'none';
            if (data.documents && data.documents.length > 0) {
                // Store search_id if it's a new search
                if (isNewSearch) {
                    searchId = data.search_id;
                }

                appendResults(data.documents);
                totalPages = data.total_pages;
                totalResults = data.total_count;

                // Prefetch the next page if there are more pages
                if (page < totalPages) {
                    page += 1;
                    prefetchNextPage();
                } else {
                    hasMore = false;
                }
            } else {
                hasMore = false;
                if (isNewSearch && resultsDiv && resultsDiv.innerHTML === '') {
                    resultsDiv.innerHTML = '<p>No results found.</p>';
                }
            }
            updateTotalResults();
            isLoading = false;
        })
        .catch(error => {
            hideLoadingIndicator();
            if (cancelButton) cancelButton.style.display = 'none';
            isLoading = false;
            if (error.name === 'AbortError') {
                console.log('Search was cancelled');
            } else {
                console.error('Error:', error);
                alert('An error occurred during the search. Please try again.');
            }
        });
    }

    /**
     * Appends search results to the resultsDiv.
     * @param {Array} documents - List of document objects.
     */
    function appendResults(documents) {
        if (!resultsDiv) return;

        let table = document.getElementById('resultsTable');
        let tbody;

        // If the table doesn't exist yet, create it
        if (!table) {
            table = document.createElement('table');
            table.id = 'resultsTable';

            // Create table headers with a Select All checkbox
            const thead = document.createElement('thead');
            thead.innerHTML = `
                <tr>
                    <th><input type="checkbox" id="selectAll" /></th>
                    <th>File</th>
                    <th>Summary</th>
                </tr>
            `;
            table.appendChild(thead);

            // Create table body
            tbody = document.createElement('tbody');
            table.appendChild(tbody);

            // Append the table to the results div
            resultsDiv.appendChild(table);
        } else {
            // If the table exists, get its tbody
            tbody = table.querySelector('tbody');
        }

        // Append new rows to the table body
        documents.forEach(doc => {
            const row = document.createElement('tr');
            row.innerHTML = `
                <td><input type="checkbox" class="select-document" data-doc-id="${doc._id}" /></td>
                <td><a href="/document/${doc._id}?search_id=${searchId}">${doc.filename || 'No file name'}</a></td>
                <td>${doc.summary || 'No summary available.'}</td>
            `;
            // If the document is already selected, check the box
            if (selectedDocuments.has(doc._id)) {
                row.querySelector('.select-document').checked = true;
            }
            tbody.appendChild(row);
        });
    }

    /**
     * Updates the total results display.
     */
    function updateTotalResults() {
        if (totalResultsDiv) {
            totalResultsDiv.textContent = `Total results: ${totalResults}`;
        }
    }

    /**
     * Prefetches the next page of results.
     */
    function prefetchNextPage() {
        if (prefetchedData || !hasMore) return;

        const prefetchQuery = { ...currentQuery, page: page };

        fetch('/search', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(prefetchQuery),
        })
        .then(response => response.json())
        .then(data => {
            if (data.documents && data.documents.length > 0) {
                prefetchedData = data;
            } else {
                hasMore = false;
            }
        })
        .catch(error => {
            console.error('Error during prefetching:', error);
        });
    }

    // ===============================
    // Selection Management
    // ===============================

    /**
     * Exports the selected documents to CSV.
     */
    function exportSelectedDocuments() {
        if (selectedDocuments.size === 0) {
            alert('No documents selected.');
            return;
        }

        // Prepare the list of selected document IDs
        const selectedIds = Array.from(selectedDocuments);

        // Show a loading modal or notification
        showExportModal('Exporting selected documents...');

        // Send the list to the server via POST
        fetch('/export_selected_csv', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ document_ids: selectedIds }),
        })
        .then(response => {
            if (response.ok) {
                return response.blob();
            } else {
                return response.json().then(err => { throw err; });
            }
        })
        .then(blob => {
            // Create a link to download the blob
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'selected_documents.csv';
            document.body.appendChild(a);
            a.click();
            a.remove();
            window.URL.revokeObjectURL(url);

            // Hide the export modal and show success message
            hideExportModal();
            showExportModal('Export successful!', 'success');

            // Reset the export button
            exportSelectedCsvButton.disabled = false;
            exportSelectedCsvButton.textContent = 'Export Selected to CSV';

            // Optionally, clear the selectedDocuments set
            selectedDocuments.clear();
            updateExportButtonVisibility();

            // Uncheck all checkboxes
            const checkboxes = resultsDiv.querySelectorAll('.select-document');
            checkboxes.forEach(cb => cb.checked = false);

            // Uncheck "Select All" checkbox
            const selectAllCheckbox = document.getElementById('selectAll');
            if (selectAllCheckbox) {
                selectAllCheckbox.checked = false;
            }

            // Remove success message after a short delay
            setTimeout(() => {
                hideExportModal();
            }, 3000);
        })
        .catch(error => {
            console.error('Error exporting selected documents:', error);
            hideExportModal();
            alert('Error exporting selected documents.');

            // Reset the export button
            exportSelectedCsvButton.disabled = false;
            exportSelectedCsvButton.textContent = 'Export Selected to CSV';
        });
    }

    /**
     * Cancels the ongoing search.
     */
    function cancelSearch() {
        if (controller) {
            controller.abort();
            hideLoadingIndicator();
            isLoading = false;
            hasMore = false;
            if (cancelButton) cancelButton.style.display = 'none';
        }
    }

    // ===============================
    // Export Feedback Mechanism
    // ===============================

    /**
     * Creates and displays an export modal for feedback.
     * @param {string} message - The message to display.
     * @param {string} type - The type of message ('info', 'success', 'error').
     */
    function showExportModal(message, type = 'info') {
        // Check if modal already exists
        let modal = document.getElementById('exportModal');
        if (!modal) {
            modal = document.createElement('div');
            modal.id = 'exportModal';
            modal.innerHTML = `
                <div class="modal-content">
                    <span class="close-button">&times;</span>
                    <p id="exportMessage">${message}</p>
                </div>
            `;
            document.body.appendChild(modal);

            // Style the modal
            const style = document.createElement('style');
            style.textContent = `
                #exportModal {
                    display: block; 
                    position: fixed; 
                    z-index: 1000; 
                    left: 0;
                    top: 0;
                    width: 100%; 
                    height: 100%; 
                    overflow: auto; 
                    background-color: rgba(0,0,0,0.4); 
                }
                .modal-content {
                    background-color: #fefefe;
                    margin: 15% auto; 
                    padding: 20px;
                    border: 1px solid #888;
                    width: 300px; 
                    text-align: center;
                    border-radius: 5px;
                }
                .close-button {
                    color: #aaa;
                    float: right;
                    font-size: 28px;
                    font-weight: bold;
                    cursor: pointer;
                }
                .close-button:hover,
                .close-button:focus {
                    color: black;
                    text-decoration: none;
                }
                .modal-content.success {
                    border-color: #28a745;
                }
                .modal-content.error {
                    border-color: #dc3545;
                }
            `;
            document.head.appendChild(style);

            // Handle close button click
            modal.querySelector('.close-button').addEventListener('click', hideExportModal);
        }

        // Update the message and style based on type
        const exportMessage = modal.querySelector('#exportMessage');
        exportMessage.textContent = message;
        modal.querySelector('.modal-content').className = 'modal-content'; // Reset classes

        if (type === 'success') {
            modal.querySelector('.modal-content').classList.add('success');
        } else if (type === 'error') {
            modal.querySelector('.modal-content').classList.add('error');
        }

        // Display the modal
        modal.style.display = 'block';
    }

    /**
     * Hides the export modal.
     */
    function hideExportModal() {
        const modal = document.getElementById('exportModal');
        if (modal) {
            modal.style.display = 'none';
        }
    }

    // ===============================
    // Initial Load
    // ===============================

    // Load selected documents from localStorage
    loadSelectedDocuments();
});


********************************************************************************

File: docs/design_document.md
********************************************************************************

---
author: Louis Hyman
operator: Louis Hyman
---

# Design Document: Automated Offloading of NER and Fuzzy Matching Processing to HPC Cluster

## Table of Contents

## 1. Introduction

This design document outlines the implementation of an automated system
to offload intensive Named Entity Recognition (NER) and fuzzy matching
processing tasks from a local MongoDB/Flask application to a
High-Performance Computing (HPC) cluster. Leveraging the HPC's
computational power will significantly enhance processing efficiency,
especially as the dataset scales beyond 200k documents. The goal is to
maintain data integrity, ensure security, and achieve seamless
integration between the local server and the HPC cluster.

## 2. Objectives

-   **Automation:** Fully automate the workflow for identifying,
    processing, and integrating data between the local server and HPC
    cluster.

-   **Security:** Ensure secure data transmission and restrict access to
    authorized entities.

-   **Data Consistency:** Maintain synchronization and consistency
    between local MongoDB documents and processed results.

-   **Efficiency:** Optimize data transfer by handling only necessary
    updates using delta updates and efficient data formats.

-   **Reliability:** Implement robust error handling and idempotent
    operations to prevent data corruption and ensure reliable
    processing.

-   **Scalability:** Design the system to accommodate future offloaded
    calculations beyond NER and fuzzy matching.

## 3. System Architecture

### 3.1 Overview

The system comprises two main environments:

1.  **Local Environment:**

    -   **Flask Application Container:** Hosts the web interface and
        manages the local MongoDB as the authoritative data source.

    -   **MongoDB Container:** Stores the definitive dataset with a
        persistent data volume.

    -   **ngrok Tunnel:** Securely exposes the Flask API to the HPC
        cluster.

<!-- -->

1.  **HPC Cluster:**

    -   **Processing Scripts:** Execute NER and fuzzy matching tasks by
        interacting with the local server's API.

    -   **Secure Connection:** Initiates connections to the local server
        via the ngrok tunnel.

    -   **Extensible Processing Modules:** Facilitates the addition of
        future processing tasks.

### 3.2 Architectural Diagram

    sql

    Copy code

    +---------------------+                       +---------------------+

    |    Local Machine    |                       |      HPC Cluster    |

    |                     |                       |                     |

    | +-----------------+ |                       | +-----------------+ |

    | | Flask Container | | <--- ngrok Tunnel --->| | Processing Scripts|

    | +-----------------+ |                       | +-----------------+ |

    |         |           |                       |                     |

    |         | MongoDB    |                       |                     |

    |         | Container  |                       |                     |

    +---------------------+                       +---------------------+

## 4. Components Description

### 4.1 Local Environment

-   **Flask Application Container:**

    -   Hosts the Flask web application.

    -   Exposes RESTful API endpoints for data retrieval and update.

    -   Interfaces with the local MongoDB container.

    -   Implements health checks to ensure readiness.

<!-- -->

-   **MongoDB Container:**

    -   Stores all documents as the authoritative data source.

    -   Contains processed fields such as NER tags and fuzzy match
        scores.

    -   Utilizes a persistent data volume to ensure data durability.

    -   Executes initialization scripts for setting up collections and
        indexes.

<!-- -->

-   **ngrok Tunnel:**

    -   Exposes the Flask API securely over the internet.

    -   Provides HTTPS encryption for data in transit.

    -   Implements authentication mechanisms to restrict access.

    -   Configured with persistent URLs for consistent access.

### 4.2 HPC Cluster

-   **Processing Scripts:**

    -   Scripts responsible for performing NER and fuzzy matching tasks.

    -   Utilize existing `data_processing.py`, `entity_linking.py`, and
        `ner_processing.py` for processing logic.

    -   Designed to be modular to allow future offloaded calculations.

    -   Containerized (e.g., Docker, Singularity) to ensure environment
        consistency.

<!-- -->

-   **Secure Connection:**

    -   Initiates outbound connections to the local server via the ngrok
        tunnel.

    -   Retrieves data needing processing and sends back processed
        results.

<!-- -->

-   **Extensible Processing Modules:**

    -   Structured to add new processing tasks with minimal changes to
        existing code.

    -   Encapsulates processing logic to maintain separation of
        concerns.

## 5. Data Flow

1.  **Initialization:**

    -   The local server starts and establishes an ngrok tunnel to
        expose its API securely.

<!-- -->

1.  **Task Identification:**

    -   The HPC cluster connects to the local server's API via the ngrok
        tunnel.

    -   It queries the API to retrieve a list of documents that require
        NER and fuzzy matching processing.

<!-- -->

1.  **Data Retrieval:**

    -   The HPC pulls the necessary document data (e.g., document IDs
        and text content) using the API.

<!-- -->

1.  **Processing:**

    -   The HPC performs NER and fuzzy matching on the retrieved
        documents using the existing processing scripts
        (`data_processing.py`, `entity_linking.py`,
        `ner_processing.py`).

<!-- -->

1.  **Result Transmission:**

    -   The HPC sends the processed results back to the local server via
        a secure API endpoint.

    -   Data is compressed and formatted efficiently (e.g., JSON or
        BSON) before transmission.

<!-- -->

1.  **Data Integration:**

    -   The local server receives the processed data and updates the
        corresponding documents in MongoDB.

    -   Ensures data consistency and handles idempotent operations.

<!-- -->

1.  **Confirmation:**

    -   The local server acknowledges successful integration, and the
        processed documents are updated accordingly.

## 6. Security Considerations

### 6.1 Secure Tunneling

-   **HTTPS Encryption:**

    -   Use ngrok's HTTPS feature to encrypt all data transmitted
        between the HPC and the local server.

<!-- -->

-   **Authentication:**

    -   Leverage ngrok's built-in authentication (e.g., basic auth) to
        ensure only authorized entities can access the API endpoints.

    -   Implement API keys or OAuth tokens for additional security on
        the Flask API.

### 6.2 Firewall and Network Policies

-   **Port Restrictions:**

    -   Only expose necessary ports (e.g., the port running the Flask
        API) via ngrok.

    -   Ensure that the local firewall blocks all other inbound
        connections.

<!-- -->

-   **IP Whitelisting:**

    -   If possible, restrict API access to the HPC's IP addresses using
        ngrok's IP filtering features.

### 6.3 Data Protection

-   **Data Encryption:**

    -   Ensure data at rest in MongoDB is encrypted, especially
        sensitive information.

<!-- -->

-   **Access Controls:**

    -   Implement role-based access controls (RBAC) in MongoDB to
        restrict who can read/write data.

### 6.4 Monitoring and Logging

-   **Access Logs:**

    -   Monitor ngrok and Flask application logs to detect any
        unauthorized access attempts.

<!-- -->

-   **Audit Trails:**

    -   Maintain logs of all data transfers and processing activities
        for accountability and compliance.

<!-- -->

-   **Log Rotation:**

    -   Configure Docker logging drivers with rotation policies to
        manage log file sizes.

## 7. API Design

### 7.1 Overview

The Flask application will expose the following API endpoints to
facilitate communication with the HPC cluster:

1.  **GET /api/processing-tasks**

    -   **Purpose:** Retrieve a list of documents that require
        processing.

    -   **Response:** JSON array containing document IDs and necessary
        fields for processing.

<!-- -->

1.  **POST /api/processed-data**

    -   **Purpose:** Receive processed NER and fuzzy matching results
        from the HPC.

    -   **Request Body:** JSON array containing document IDs and their
        corresponding processed data.

    -   **Response:** Confirmation of successful data integration.

### 7.2 Endpoint Specifications

#### 7.2.1 GET /api/processing-tasks

-   **Authentication:**

    -   Requires a valid API key or OAuth token.

<!-- -->

-   **Parameters:**

    -   `batch_size` (optional): Number of documents to retrieve per
        request.

<!-- -->

-   **Response Structure:**

<!-- -->

    json

    Copy code

    {

      "tasks": [

        {

          "document_id": "unique_id_1",

          "content": "Document text for processing."

        },

        {

          "document_id": "unique_id_2",

          "content": "Another document text."

        }

        // ... more documents

      ]

    }

#### 7.2.2 POST /api/processed-data

-   **Authentication:**

    -   Requires a valid API key or OAuth token.

<!-- -->

-   **Request Body Structure:**

<!-- -->

    json

    Copy code

    {

      "processed_data": [

        {

          "document_id": "unique_id_1",

          "ner_tags": ["Entity1", "Entity2"],

          "fuzzy_match_scores": {"EntityA": 0.95, "EntityB": 0.89}

        },

        {

          "document_id": "unique_id_2",

          "ner_tags": ["Entity3"],

          "fuzzy_match_scores": {"EntityC": 0.92}

        }

        // ... more processed results

      ]

    }

-   **Response Structure:**

<!-- -->

    json

    Copy code

    {

      "status": "success",

      "updated_documents": ["unique_id_1", "unique_id_2"]

    }

### 7.3 Data Formats and Compression

-   **Data Formats:**

    -   Use JSON for structured and lightweight data transfer.

    -   Alternatively, use BSON for binary-encoded serialization if
        performance gains are necessary.

<!-- -->

-   **Compression:**

    -   Compress the request payloads using gzip before transmission.

    -   Implement compression/decompression on both the client (HPC) and
        server (Flask) sides.

## 8. Error Handling and Data Consistency

### 8.1 Idempotent Operations

-   **Design:**

    -   Ensure that processing tasks can be safely retried without
        causing duplicate updates or data corruption.

    -   Use document IDs to check if processed data already exists
        before applying updates.

<!-- -->

-   **Implementation:**

    -   On receiving processed data, verify if the `ner_tags` and
        `fuzzy_match_scores` fields are already populated.

    -   If they are, decide whether to overwrite based on timestamps or
        other criteria.

### 8.2 Robust Error Handling

-   **API Responses:**

    -   Use appropriate HTTP status codes to indicate success or
        specific failure reasons.

    -   Provide meaningful error messages in the response body for
        debugging.

<!-- -->

-   **Retry Mechanisms:**

    -   Implement retries on the HPC side for transient failures such as
        network issues.

    -   Use exponential backoff strategies to avoid overwhelming the
        server.

### 8.3 Data Validation

-   **Incoming Data:**

    -   Validate the structure and content of incoming processed data
        before integration.

    -   Ensure that all required fields are present and correctly
        formatted.

<!-- -->

-   **Database Transactions:**

    -   Utilize MongoDB's transaction capabilities to perform atomic
        updates.

    -   Roll back transactions in case of failures to maintain data
        integrity.

### 8.4 Conflict Resolution

-   **Versioning:**

    -   Implement versioning for documents to handle concurrent updates
        gracefully.

    -   Use MongoDB's `findAndModify` operations with version checks.

<!-- -->

-   **Audit Logs:**

    -   Maintain logs of all updates to track changes and resolve
        conflicts if they arise.

## 9. Integration with Existing Scripts

### 9.1 Overview

The existing scripts (`data_processing.py`, `database_setup.py`,
`entity_linking.py`, `ner_processing.py`) handle data ingestion,
validation, and entity linking within the local environment. To
integrate these scripts into the new architecture, modifications and
extensions are necessary to facilitate remote invocation and data
exchange with the HPC cluster.

### 9.2 Modifications and Enhancements

#### 9.2.1 data_processing.py

-   **Purpose:** Processes JSON and TXT files, inserts them into
    MongoDB, and updates field structures.

-   **Enhancements:**

    -   **API Integration:** Modify the script to accept data retrieved
        via the API instead of directly accessing the file system.

    -   **Incremental Updates:** Ensure that only new or changed
        documents are processed and inserted.

    -   **Remote Invocation:** Allow the script to be invoked remotely
        by the HPC cluster, possibly via command-line arguments or
        environment variables.

#### 9.2.2 database_setup.py

-   **Purpose:** Sets up MongoDB collections, indexes, and provides
    utility functions for database interactions.

-   **Enhancements:**

    -   **API Endpoint Expansion:** Ensure that the Flask API can
        utilize existing database functions for updating documents based
        on processed data.

    -   **Security:** Implement additional checks to ensure that only
        authenticated requests can perform database modifications.

#### 9.2.3 entity_linking.py

-   **Purpose:** Performs entity linking using NER and fuzzy matching,
    integrating results back into MongoDB.

-   **Enhancements:**

    -   **Remote Execution:** Allow the script to accept data from the
        local server via API endpoints.

    -   **Result Packaging:** Ensure that processed data is packaged in
        the required format for seamless transmission back to the local
        server.

#### 9.2.4 ner_processing.py

-   **Purpose:** Handles NER processing on documents, extracting
    entities and preparing them for linking.

-   **Enhancements:**

    -   **API Integration:** Modify the script to accept data via API
        requests and return results accordingly.

    -   **Modular Processing:** Ensure that NER processing can be
        modularly invoked by the HPC scripts.

### 9.3 Workflow Integration

1.  **HPC Processing Scripts:**

    -   Utilize the existing processing logic from `data_processing.py`,
        `entity_linking.py`, and `ner_processing.py`.

    -   Implement remote invocation capabilities to interact with the
        local Flask API for data retrieval and result submission.

<!-- -->

1.  **Local Flask API:**

    -   Extend API endpoints to support receiving data from HPC scripts.

    -   Integrate existing database utility functions to handle data
        updates based on processed results.

<!-- -->

1.  **Data Exchange:**

    -   Ensure that the data formats between the local server and HPC
        cluster are compatible.

    -   Utilize efficient serialization (e.g., JSON, BSON) and
        compression (e.g., gzip) for data transfer.

## 10. Deployment and Infrastructure

### 10.1 Local Environment Setup

-   **Containers:**

    -   **MongoDB Container:**

        -   Persistent data volume to ensure data durability.

        -   Expose necessary ports internally within the Docker network.

        -   Includes health checks to verify readiness.

        -   Mounts initialization scripts for setting up collections and
            indexes.

    <!-- -->

    -   **Flask Application Container:**

        -   Linked to the MongoDB container via a user-defined bridge
            network (`app_network`).

        -   Configured to communicate with MongoDB using environment
            variables.

        -   Exposes API endpoints required for processing tasks.

        -   Utilizes a multi-stage Docker build for optimized image size
            and pre-installed dependencies.

        -   Implements log rotation to manage log file sizes.

<!-- -->

-   **ngrok Configuration:**

    -   Run ngrok on the host machine to expose the Flask API.

    -   Use ngrok's configuration file to set up authentication and
        specify the local port.

    -   Configured with persistent URLs for consistent access.

    -   Secured with API keys or OAuth tokens to restrict access.

### 10.2 HPC Cluster Setup

-   **Environment:**

    -   Ensure that the HPC cluster has outbound internet access to
        connect to the ngrok tunnel.

    -   Install necessary dependencies for running processing scripts
        (e.g., Python, NER libraries, RapidFuzz).

    -   Utilize containerization (e.g., Docker, Singularity) for
        consistency with the local environment.

<!-- -->

-   **Processing Scripts:**

    -   Develop or modify scripts to:

        1.  Connect to the Flask API via ngrok.

        2.  Retrieve documents needing processing.

        3.  Perform NER and fuzzy matching using `data_processing.py`,
            `entity_linking.py`, and `ner_processing.py`.

        4.  Compress and send processed data back to the local server.

    <!-- -->

    -   Design scripts to be modular, allowing easy integration of
        additional processing tasks in the future.

### 10.3 Networking

-   **ngrok Tunnel:**

    -   Configured with persistent URLs or reserved domains to maintain
        consistent endpoints.

    -   Ensures that the tunnel remains active when processing tasks are
        expected.

<!-- -->

-   **DNS and Domain Management:**

    -   If using custom domains with ngrok, manage DNS settings
        accordingly to ensure seamless access from the HPC cluster.

## 11. Automation

### 11.1 Scheduling Processing Tasks

-   **Trigger Mechanism:**

    -   Set up a scheduled job on the HPC cluster (e.g., cron job) to
        initiate processing at defined intervals or based on specific
        triggers.

<!-- -->

-   **On-Demand Processing:**

    -   Allow manual initiation of processing tasks via scripts if
        necessary.

### 11.2 Data Transfer Automation

-   **Scripts:**

    -   Develop automated scripts on the HPC to handle:

        -   Authentication with the Flask API.

        -   Retrieval of processing tasks.

        -   Data compression.

        -   Transmission of processed data back to the local server.

<!-- -->

-   **Workflow Orchestration:**

    -   Use workflow managers (e.g., Apache Airflow) if the processing
        involves multiple dependent steps.

### 11.3 Integration with Containers

-   **Container Management:**

    -   Use Docker Compose or Kubernetes for managing containers in the
        local environment.

    -   Ensure that containers restart automatically in case of
        failures.

<!-- -->

-   **Script Integration:**

    -   Containerize processing scripts to ensure environment
        consistency between local and HPC deployments.

<!-- -->

-   **Extensibility:**

    -   Design processing containers to accept different processing
        modules, facilitating the addition of new tasks without major
        changes.

## 12. Monitoring and Logging

### 12.1 Monitoring

-   **Local Server:**

    -   Monitor API performance and uptime using tools like Prometheus
        and Grafana.

    -   Track MongoDB performance metrics to ensure database health.

<!-- -->

-   **HPC Cluster:**

    -   Monitor processing script performance, resource utilization, and
        task completion rates.

### 12.2 Logging

-   **API Logs:**

    -   Implement comprehensive logging in the Flask application to
        record incoming requests, processing outcomes, and errors.

<!-- -->

-   **Processing Logs:**

    -   Maintain logs on the HPC cluster for task retrieval, processing
        steps, and data transmission.

<!-- -->

-   **Centralized Logging:**

    -   Use centralized logging solutions (e.g., ELK Stack) to aggregate
        and analyze logs from both environments.

<!-- -->

-   **Log Rotation:**

    -   Configure Docker logging drivers with rotation policies to
        manage log file sizes.

### 12.3 Alerting

-   **Set Up Alerts:**

    -   Configure alerts for critical failures, such as:

        -   Failed data transmissions.

        -   API authentication errors.

        -   Unusually high processing times.

<!-- -->

-   **Notification Channels:**

    -   Use email, Slack, or other communication tools to receive alerts
        in real-time.

## 13. Maintenance and Scalability

### 13.1 Maintenance

-   **Regular Updates:**

    -   Keep all software components, including containers and
        dependencies, up to date with security patches.

<!-- -->

-   **Backup Strategies:**

    -   Implement regular backups of the MongoDB data volume to prevent
        data loss.

    -   Store backups securely, possibly in an offsite location or cloud
        storage.

<!-- -->

-   **Routine Checks:**

    -   Periodically verify the integrity of the ngrok tunnel and ensure
        it is functioning as expected.

### 13.2 Scalability

-   **Data Volume Growth:**

    -   Design the system to handle increasing numbers of documents by:

        -   Optimizing MongoDB indexes.

        -   Scaling the HPC processing capacity as needed.

<!-- -->

-   **Parallel Processing:**

    -   Enable the HPC cluster to process multiple tasks in parallel to
        reduce processing time.

<!-- -->

-   **Load Balancing:**

    -   If necessary, implement load balancing strategies to distribute
        processing workloads efficiently.

### 13.3 Future Enhancements

-   **Extended Processing Capabilities:**

    -   Incorporate additional processing tasks beyond NER and fuzzy
        matching as needed.

<!-- -->

-   **Advanced Security Measures:**

    -   Implement multi-factor authentication (MFA) for API access.

    -   Utilize more sophisticated encryption standards for data at rest
        and in transit.

<!-- -->

-   **User Management:**

    -   Develop role-based access controls within the Flask application
        to manage user permissions.

<!-- -->

-   **Modular Processing Framework:**

    -   Develop a plugin-based processing framework to easily add new
        processing modules without altering the core system.

## 14. Conclusion

This design document presents a comprehensive plan to automate the
offloading of NER and fuzzy matching processing tasks from a local
MongoDB/Flask application to an HPC cluster. By leveraging secure
tunneling with ngrok, implementing robust API authentication, and
ensuring data consistency through idempotent operations and
transactional updates, the system aims to achieve efficient, secure, and
reliable processing. Additionally, by optimizing data transfer with
delta updates and efficient data formats, the system minimizes bandwidth
usage and enhances performance. Proper error handling, monitoring, and
logging further ensure the system\'s resilience and maintainability,
while the modular and encapsulated design facilitates future scalability
and the addition of new processing tasks.

## Appendices

### A. Technology Stack

-   **Local Environment:**

    -   **Flask:** Web framework for the API and web interface.

    -   **MongoDB:** NoSQL database for storing documents.

    -   **Docker:** Containerization platform for Flask and MongoDB.

    -   **ngrok:** Secure tunneling service to expose the Flask API.

<!-- -->

-   **HPC Cluster:**

    -   **Processing Scripts:** Python scripts utilizing NER libraries
        (e.g., spaCy), fuzzy matching tools (e.g., RapidFuzz), and
        entity linking (`entity_linking.py`).

    -   **Containerization:** Docker or Singularity containers to ensure
        consistent processing environments.

### B. Security Best Practices

-   **Secret Management:**

    -   Store API keys and tokens securely using environment variables
        or secret management tools (e.g., HashiCorp Vault).

<!-- -->

-   **Least Privilege:**

    -   Grant the minimal necessary permissions to API endpoints and
        database operations.

<!-- -->

-   **Regular Audits:**

    -   Conduct periodic security audits to identify and mitigate
        vulnerabilities.

### C. References

-   ngrok Documentation

-   Docker Best Practices

-   spaCy NER Documentation

-   RapidFuzz Documentation

# Implementation Scripts

Below are the necessary scripts to implement the automated offloading of
NER and fuzzy matching processing from the local server to the HPC
cluster. The system is designed to be modular and extensible, allowing
for the addition of new processing tasks in the future.

## 1. Local Server Scripts

### 1.1 Docker Compose Configuration (`docker-compose.yml`)

Sets up the MongoDB and Flask application containers with health checks,
logging configurations, and network settings.

    yaml

    Copy code

    version: '3.8'

    services:

      mongodb:

        image: mongo:6.0

        container_name: mongodb

        environment:

          MONGO_INITDB_ROOT_USERNAME: admin

          MONGO_INITDB_ROOT_PASSWORD: secret

        volumes:

          - mongodb_data:/data/db

          - ./mongo-init/:/docker-entrypoint-initdb.d/

        ports:

          - "27017:27017"

        healthcheck:

          test: [

            "CMD",

            "mongosh",

            "--username",

            "admin",

            "--password",

            "secret",

            "--authenticationDatabase",

            "admin",

            "--eval",

            "db.adminCommand('ping')"

          ]

          interval: 10s

          timeout: 5s

          retries: 5

        networks:

          - app_network

        logging:

          driver: "json-file"

          options:

            max-size: "10m"

            max-file: "3"

      app:

        build:

          context: ./app

          dockerfile: Dockerfile

        container_name: flask_app

        ports:

          - "5000:5000"

        environment:

          MONGO_URI: "mongodb://${MONGO_INITDB_ROOT_USERNAME}:${MONGO_INITDB_ROOT_PASSWORD}@mongodb:27017/admin"

          FLASK_APP: app.py

          FLASK_ENV: ${FLASK_ENV}

          FLASK_DEBUG: ${FLASK_DEBUG}

          SECRET_KEY: ${SECRET_KEY}

        volumes:

          - ./app:/app

          - ./archives:/app/archives

        depends_on:

          mongodb:

            condition: service_healthy

        networks:

          - app_network

        logging:

          driver: "json-file"

          options:

            max-size: "10m"

            max-file: "3"

    volumes:

      mongodb_data:

    networks:

      app_network:

        driver: bridge

    # Notes:

    # - Ensure you have a .env file in your project root with the necessary variables.

    # - Add .env to your .gitignore to prevent committing sensitive information.

### 1.2 Multi-Stage Dockerfile for Flask App (`app/Dockerfile`)

Optimizes the Docker image by separating the build and runtime
environments and pre-downloading spaCy models.

    dockerfile

    Copy code

    # Stage 1: Build

    FROM python:3.10-slim AS builder

    # Set environment variables to prevent .pyc files and ensure output is flushed

    ENV PYTHONDONTWRITEBYTECODE=1 

    ENV PYTHONUNBUFFERED=1          

    # Set the working directory inside the container

    WORKDIR /app

    # Install system dependencies required for building Python packages

    RUN apt-get update && apt-get install -y --no-install-recommends \

        build-essential \

        curl \

        && rm -rf /var/lib/apt/lists/*

    # Copy the requirements file and install Python dependencies

    COPY requirements.txt /app/

    RUN pip install --upgrade pip && pip install --user -r requirements.txt

    # Create the directory for spaCy models to ensure it exists

    RUN mkdir -p /root/.local/share/spacy/models

    # Download spaCy models only if they are not already downloaded

    RUN if [ ! -d "/root/.local/share/spacy/models/en_core_web_lg" ]; then \

            python -m spacy download en_core_web_lg; \

        fi && \

        if [ ! -d "/root/.local/share/spacy/models/en_core_web_trf" ]; then \

            python -m spacy download en_core_web_trf; \

        fi

    # Copy project files into the container

    COPY . .

    # Stage 2: Runtime

    FROM python:3.10-slim

    # Set environment variables for the runtime stage

    ENV PYTHONDONTWRITEBYTECODE=1  

    ENV PYTHONUNBUFFERED=1          

    # Set the working directory inside the runtime container

    WORKDIR /app

    # Install only necessary system dependencies for the runtime

    RUN apt-get update && apt-get install -y --no-install-recommends \

        && rm -rf /var/lib/apt/lists/*  

    # Copy installed Python packages and spaCy models from the builder stage

    COPY --from=builder /root/.local /root/.local

    COPY --from=builder /root/.local/share/spacy/models /root/.local/share/spacy/models

    # Set PATH to include user-installed binaries

    ENV PATH=/root/.local/bin:$PATH

    # Copy project files from the builder stage

    COPY --from=builder /app /app

    # Copy and set permissions for the entrypoint script

    COPY entrypoint.sh /app/entrypoint.sh

    RUN chmod +x /app/entrypoint.sh

    # Set the entrypoint to the entrypoint script for container initialization

    ENTRYPOINT ["/app/entrypoint.sh"]

    # Command to run the Flask app

    CMD ["python", "app.py"]

### 1.3 Entrypoint Script (`entrypoint.sh`)

Handles service initialization, including waiting for MongoDB to be
ready before starting the Flask application.

    bash

    Copy code

    #!/bin/bash 

    # Exit immediately if a command exits with a non-zero status

    set -e

    # Configuration for backoff

    MAX_RETRIES=10

    SLEEP_TIME=10

    MONGO_HOST="mongodb"

    MONGO_PORT=27017

    MONGO_URI="mongodb://admin:secret@${MONGO_HOST}:${MONGO_PORT}/admin"

    echo "Waiting for MongoDB to be ready..."

    # Backoff loop to wait for MongoDB

    for ((i=1;i<=MAX_RETRIES;i++)); do

        echo "Attempt $i/$MAX_RETRIES: Checking MongoDB connection..."

        python -c "import pymongo; client = pymongo.MongoClient('${MONGO_URI}', serverSelectionTimeoutMS=5000); client.admin.command('ping')" && break

        echo "MongoDB is not ready yet. Waiting ${SLEEP_TIME} seconds..."

        sleep ${SLEEP_TIME}

    done

    # Verify if MongoDB is up after retries

    python -c "import pymongo; client = pymongo.MongoClient('${MONGO_URI}', serverSelectionTimeoutMS=5000); client.admin.command('ping')" || { echo "MongoDB did not become ready in time after ${MAX_RETRIES} attempts. Exiting."; exit 1; }

    echo "MongoDB is up and running."

    # Optional: Run database setup and processing scripts

    # Uncomment the following lines if needed

    # echo "Running database_setup.py..."

    # python database_setup.py

    # echo "database_setup.py completed."

    # echo "Running data_processing.py..."

    # python data_processing.py

    # echo "data_processing.py completed."

    # echo "Running generate_unique_terms.py..."

    # python generate_unique_terms.py

    # echo "generate_unique_terms.py completed."

    # echo "Starting Flask app..."

    exec "$@"

    # Testing Version (Optional)

    # Uncomment for profiling purposes

    # echo "***Running testing version***"

    # python -m cProfile -o show_env.prof show_env.py

    # python -m cProfile -o database_setup.prof database_setup.py

    # python -m cProfile -o data_processing.prof data_processing.py

    # python -m cProfile -o generate_unique_terms.prof generate_unique_terms.py

    # echo "Setup scripts completed. Starting Flask app..."

### 1.4 Flask Routes (`app/routes.py`)

Implements the Flask routes with enhanced security, logging, and error
handling.

    python

    Copy code

    # routes.py

    from flask import request, jsonify, render_template, redirect, url_for, flash, session, abort, Response, send_file

    from functools import wraps

    from app import app, cache

    from database_setup import (

        get_client,

        get_db,

        get_collections,

        find_document_by_id,

        update_document,

        delete_document,

        get_field_structure

    )

    from bson import ObjectId

    from werkzeug.security import generate_password_hash, check_password_hash

    import math

    import json

    import re

    import logging

    import time

    from datetime import datetime, timedelta

    import random

    import csv

    from io import StringIO

    from logging.handlers import RotatingFileHandler

    import os

    import uuid

    import pymongo

    # Create a logger instance

    logger = logging.getLogger(__name__)

    logger.setLevel(logging.DEBUG)

    # Define a log format

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Create a console handler (optional)

    console_handler = logging.StreamHandler()

    console_handler.setLevel(logging.DEBUG)

    console_handler.setFormatter(formatter)

    logger.addHandler(console_handler)

    # Create a file handler to log to routes.log

    log_file_path = os.path.join(os.path.dirname(__file__), 'routes.log')

    file_handler = logging.FileHandler(log_file_path)

    file_handler.setLevel(logging.DEBUG)  # Set the level for the file handler

    file_handler.setFormatter(formatter)

    logger.addHandler(file_handler)

    # Initialize database connection and collections

    client = get_client()

    db = get_db(client)

    documents, unique_terms_collection, field_structure_collection = get_collections(db)

    # Hashed password (generate this using generate_password_hash('your_actual_password'))

    ADMIN_PASSWORD_HASH = 'pbkdf2:sha256:260000$uxZ1Fkjt9WQCHwuN$ca37dfb41ebc26b19daf24885ebcd09f607cab85f92dcab13625627fd9ee902a'

    # Login attempt tracking

    MAX_ATTEMPTS = 5

    LOCKOUT_TIME = 15 * 60  # 15 minutes in seconds

    login_attempts = {}

    def is_locked_out(ip):

        if ip in login_attempts:

            attempts, last_attempt_time = login_attempts[ip]

            if attempts >= MAX_ATTEMPTS:

                if datetime.now() - last_attempt_time < timedelta(seconds=LOCKOUT_TIME):

                    return True

                else:

                    login_attempts[ip] = (0, datetime.now())

        return False

    def update_login_attempts(ip, success):

        if ip in login_attempts:

            attempts, _ = login_attempts[ip]

            if success:

                login_attempts[ip] = (0, datetime.now())

            else:

                login_attempts[ip] = (attempts + 1, datetime.now())

        else:

            login_attempts[ip] = (0, datetime.now()) if success else (1, datetime.now())

    # Login required decorator

    def login_required(f):

        @wraps(f)

        def decorated_function(*args, **kwargs):

            if 'logged_in' not in session:

                return redirect(url_for('login', next=request.url))

            return f(*args, **kwargs)

        return decorated_function

    @app.route('/')

    # @login_required

    def index():

        app.logger.info('Handling request to index')

        num_search_fields = 3  # Number of search fields to display

        field_structure = get_field_structure(db)  # Pass 'db' here

        return render_template('index.html', num_search_fields=num_search_fields, field_structure=field_structure)

    @app.route('/login', methods=['GET', 'POST'])

    def login():

        if request.method == 'POST':

            ip = request.remote_addr

            if is_locked_out(ip):

                flash('Too many failed attempts. Please try again later.')

                return render_template('login.html')

            # Verify CAPTCHA

            user_captcha = request.form.get('captcha')

            correct_captcha = request.form.get('captcha_answer')

            if user_captcha != correct_captcha:

                flash('Incorrect CAPTCHA')

                return redirect(url_for('login'))

            if check_password_hash(ADMIN_PASSWORD_HASH, request.form['password']):

                session['logged_in'] = True

                update_login_attempts(ip, success=True)

                flash('You were successfully logged in')

                next_page = request.args.get('next')

                return redirect(next_page or url_for('index'))

            else:

                update_login_attempts(ip, success=False)

                time.sleep(2)  # Add a delay after failed attempt

                flash('Invalid password')

        # Generate CAPTCHA for GET requests

        captcha_num1 = random.randint(1, 10)

        captcha_num2 = random.randint(1, 10)

        captcha_answer = str(captcha_num1 + captcha_num2)

        return render_template('login.html', captcha_num1=captcha_num1, captcha_num2=captcha_num2, captcha_answer=captcha_answer)

    @app.route('/logout')

    def logout():

        session.pop('logged_in', None)

        flash('You were logged out')

        return redirect(url_for('index'))

    @app.route('/search', methods=['POST'])

    # @login_required

    def search():

        try:

            data = request.get_json()

            logger.debug(f"Received search request: {data}")

            page = int(data.get('page', 1))

            per_page = int(data.get('per_page', 50))

            query = build_query(data)

            logger.debug(f"Constructed MongoDB query: {query}")

            total_count = documents.count_documents(query)

            search_results = list(documents.find(query).skip((page - 1) * per_page).limit(per_page))

            for doc in search_results:

                doc['_id'] = str(doc['_id'])

            total_pages = math.ceil(total_count / per_page) if per_page else 1

            # Generate unique search ID

            search_id = str(uuid.uuid4())

            # Store the ordered list of document IDs

            ordered_ids = [doc['_id'] for doc in search_results]

            cache.set(f'search_{search_id}', ordered_ids, timeout=3600)  # Expires in 1 hour

            logger.debug(f"Search ID: {search_id}, Found {total_count} documents.")

            return jsonify({

                "search_id": search_id,

                "documents": search_results,

                "total_count": total_count,

                "current_page": page,

                "total_pages": total_pages,

                "per_page": per_page

            })

        except Exception as e:

            logger.error(f"An error occurred during search: {str(e)}", exc_info=True)

            return jsonify({"error": "An internal error occurred"}), 500

    def build_query(data):

        query = {}

        criteria_list = []

        logger.debug(f"Building query from search data: {data}")

        for i in range(1, 4):

            field = data.get(f'field{i}')

            search_term = data.get(f'searchTerm{i}')

            operator = data.get(f'operator{i}')

            if field and search_term:

                condition = {}

                if operator == 'NOT':

                    condition[field] = {'$not': {'$regex': re.escape(search_term), '$options': 'i'}}

                else:

                    condition[field] = {'$regex': re.escape(search_term), '$options': 'i'}

                

                criteria_list.append((operator, condition))

                logger.debug(f"Processed field {field} with search term '{search_term}' and operator '{operator}'")

        if criteria_list:

            and_conditions = []

            or_conditions = []

            for operator, condition in criteria_list:

                if operator == 'AND' or operator == 'NOT':

                    and_conditions.append(condition)

                elif operator == 'OR':

                    or_conditions.append(condition)

            if and_conditions:

                query['$and'] = and_conditions

            if or_conditions:

                if '$or' not in query:

                    query['$or'] = or_conditions

                else:

                    query['$or'].extend(or_conditions)

        logger.debug(f"Final query: {query}")

        return query

    @app.route('/document/<string:doc_id>')

    # @login_required

    def document_detail(doc_id):

        # Hard-coded SHOW_EMPTY variable

        SHOW_EMPTY = False  # Set to True to show empty fields, False to hide them

        # Function to clean the document data

        def clean_data(data):

            empty_values = [None, '', 'N/A', 'null', [], {}, 'None']

            if isinstance(data, dict):

                return {

                    k: clean_data(v)

                    for k, v in data.items()

                    if v not in empty_values and clean_data(v) not in empty_values

                }

            elif isinstance(data, list):

                return [

                    clean_data(item)

                    for item in data

                    if item not in empty_values and clean_data(item) not in empty_values

                ]

            else:

                return data

        search_id = request.args.get('search_id')

        if not search_id:

            flash('Missing search context.')

            return redirect(url_for('index'))

        try:

            # Fetch the document by ID

            document = find_document_by_id(db, doc_id)

            if not document:

                abort(404)

            document['_id'] = str(document['_id'])

            # Log the document information for debugging

            logger.debug(f"Retrieved document for ID {doc_id}: {document}")

            # Decide whether to clean the document based on SHOW_EMPTY

            if SHOW_EMPTY:

                document = document

            else:

                # Clean the document to remove empty fields

                document = clean_data(document)

            # Retrieve the ordered list from cache

            ordered_ids = cache.get(f'search_{search_id}')

            if not ordered_ids:

                flash('Search context expired. Please perform the search again.')

                return redirect(url_for('index'))

            try:

                current_index = ordered_ids.index(doc_id)

            except ValueError:

                flash('Document not found in the current search results.')

                return redirect(url_for('index'))

            # Determine previous and next IDs based on the search order

            prev_id = ordered_ids[current_index - 1] if current_index > 0 else None

            next_id = ordered_ids[current_index + 1] if current_index < len(ordered_ids) - 1 else None

            # Get the relative path from the document

            relative_path = document.get('relative_path')  # This should contain the relative path to the JSON file

            if relative_path:

                # Construct the image path by removing the '.json' extension

                image_path = relative_path.replace('.json', '')  # e.g., 'rolls/rolls/tray_1_roll_5_page3303_img1.png'

                logger.debug(f"Document ID: {doc_id}, Image path: {image_path}")

                # Check if the image file exists

                absolute_image_path = os.path.join('/app/archives', image_path)

                image_exists = os.path.exists(absolute_image_path)

                if not image_exists:

                    logger.warning(f"Image not found at: {absolute_image_path}")

            else:

                # Log an error if relative_path is None or not found

                logger.error(f"Error: No relative_path found for document ID: {doc_id}. Document content: {document}")

                image_exists = False

                image_path = None

            # Render the template with all required variables

            return render_template(

                'document-detail.html',

                document=document,

                prev_id=prev_id,

                next_id=next_id,

                search_id=search_id,

                image_path=image_path,  # Pass the constructed image path

                image_exists=image_exists  # Pass the flag indicating if the image exists

            )

        except Exception as e:

            logger.error(f"Error in document_detail: {str(e)}", exc_info=True)

            abort(500)

    @app.route('/images/<path:filename>')

    def serve_image(filename):

        image_path = os.path.join('/app/archives', filename)

        logger.debug(f"Serving image from: {image_path}")

        if os.path.exists(image_path):

            return send_file(image_path)

        else:

            logger.warning(f"Image not found at: {image_path}")

            abort(404)

    def get_top_unique_terms(db, field, term_type, query='', limit=1000, skip=0):

        """

        Retrieve top unique terms based on the field, term type, and optional search query.

        :param db: Database instance

        :param field: The field to filter terms by (e.g., 'title', 'description')

        :param term_type: The type of term ('word' or 'phrase')

        :param query: Optional search query string to filter terms

        :param limit: Number of top terms to retrieve

        :param skip: Number of records to skip for pagination

        :return: List of dictionaries with term and count

        """

        unique_terms_collection = db['unique_terms']

        

        try:

            # Base MongoDB query

            mongo_query = {"field": field, "type": term_type}

            

            # If a search query is provided, add a regex filter for the 'term' field

            if query:

                # Escape special regex characters to prevent injection attacks

                escaped_query = re.escape(query)

                # Case-insensitive search for terms containing the query substring

                mongo_query['term'] = {"$regex": f".*{escaped_query}.*", "$options": "i"}

            

            start_time = time.time()

            

            # Execute the query with sorting, skipping, and limiting for pagination

            cursor = unique_terms_collection.find(

                mongo_query,

                {"_id": 0, "term": 1, "frequency": 1}

            ).sort("frequency", pymongo.DESCENDING).skip(skip).limit(limit)

            

            terms_list = []

            for doc in cursor:

                key = 'word' if term_type == 'word' else 'phrase'

                terms_list.append({key: doc['term'], 'count': doc['frequency']})

            

            duration = time.time() - start_time

            logger.info(f"Retrieved top {len(terms_list)} {term_type}s in {duration:.4f} seconds for field '{field}' with query '{query}'.")

            return terms_list

        except Exception as e:

            logger.error(f"Error retrieving unique terms: {e}")

            return []

    def get_unique_terms_count(db, field, term_type, query=''):

        """

        Get the count of unique terms based on the field, term type, and optional search query.

        :param db: Database instance

        :param field: The field to filter terms by

        :param term_type: The type of term ('word' or 'phrase')

        :param query: Optional search query string to filter terms

        :return: Integer count of unique terms

        """

        unique_terms_collection = db['unique_terms']

        

        try:

            # Base MongoDB query

            mongo_query = {"field": field, "type": term_type}

            

            # If a search query is provided, add a regex filter for the 'term' field

            if query:

                # Escape special regex characters to prevent injection attacks

                escaped_query = re.escape(query)

                # Case-insensitive search for terms containing the query substring

                mongo_query['term'] = {"$regex": f".*{escaped_query}.*", "$options": "i"}

            

            # Count the number of unique terms matching the query

            count = unique_terms_collection.count_documents(mongo_query)

            logger.info(f"Counted {count} unique {term_type}s for field '{field}' with query '{query}'.")

            return count

        except Exception as e:

            logger.error(f"Error counting unique terms: {e}")

            return 0

    @app.route('/search-terms', methods=['GET'])

    def search_terms():

        client = get_client()  # Initialize your MongoDB client

        db = get_db(client)    # Get the database instance

        if request.headers.get('X-Requested-With') == 'XMLHttpRequest':

            # Handle AJAX request

            field = request.args.get('field')

            if not field:

                return jsonify({"error": "No field specified"}), 400

            # Extract the search query

            query = request.args.get('query', '').strip().lower()  # Normalize the query

            logger.debug(f"AJAX request for field: {field}, query: {query}")  # Using logger here

            # Define term types

            term_types = ['word', 'phrase']

            data = {}

            total_records = 0

            for term_type in term_types:

                page = int(request.args.get('page', 1))

                per_page = int(request.args.get('per_page', 100))

                skip = (page - 1) * per_page

                # Fetch filtered terms based on the query

                terms = get_top_unique_terms(db, field, term_type, query=query, limit=per_page, skip=skip)

                data[term_type + 's'] = terms

                # Fetch the count of unique terms based on the query

                count = get_unique_terms_count(db, field, term_type, query=query)

                data['unique_' + term_type + 's'] = count

                total_records += count

            data['total_records'] = total_records

            return jsonify(data)

        else:

            # Render the HTML template

            field_structure = get_field_structure(db)

            unique_fields = []  # Define if necessary

            return render_template('search-terms.html', field_structure=field_structure, unique_fields=unique_fields)

    @app.route('/database-info')

    # @login_required

    def database_info():

        field_struct = get_field_structure(db)  # Pass 'db' here

        collection_info = []

        def count_documents_with_field(field_path):

            count = documents.count_documents({field_path: {'$exists': True}})

            return count

        def traverse_structure(structure, current_path=''):

            for field, value in structure.items():

                path = f"{current_path}.{field}" if current_path else field

                if isinstance(value, dict):

                    traverse_structure(value, current_path=path)

                else:

                    count = count_documents_with_field(path)

                    collection_info.append({

                        'name': path,

                        'count': count

                    })

        traverse_structure(field_struct)

        return render_template('database-info.html', collection_info=collection_info)

    @app.route('/settings', methods=['GET', 'POST'])

    # @login_required

    def settings():

        config_path = os.path.join(os.path.dirname(__file__), 'config.json')

        if request.method == 'POST':

            new_config = request.form.to_dict()

            for key in ['fonts', 'sizes', 'colors', 'spacing']:

                if key in new_config:

                    try:

                        new_config[key] = json.loads(new_config[key])

                    except json.JSONDecodeError:

                        flash(f"Invalid JSON format for {key}.", 'danger')

                        return redirect(url_for('settings'))

            try:

                with open(config_path, 'w') as config_file:

                    json.dump(new_config, config_file, indent=4)

                app.config['UI_CONFIG'] = new_config

                flash('Settings updated successfully', 'success')

            except Exception as e:

                logger.error(f"Error updating settings: {str(e)}")

                flash('Failed to update settings.', 'danger')

            return redirect(url_for('settings'))

        try:

            if os.path.exists(config_path):

                with open(config_path) as config_file:

                    config = json.load(config_file)

            else:

                config = {}

        except json.JSONDecodeError:

            config = {}

            flash('Configuration file is corrupted. Using default settings.', 'warning')

        return render_template('settings.html', config=config)

    # Consider streaming if it ends up being thousands of documents

    @app.route('/export_selected_csv', methods=['POST'])

    # @login_required

    def export_selected_csv():

        try:

            data = request.get_json()

            document_ids = data.get('document_ids', [])

            if not document_ids:

                return jsonify({"error": "No document IDs provided"}), 400

            # Convert string IDs to ObjectIds, handle invalid IDs

            valid_ids = []

            for doc_id in document_ids:

                try:

                    valid_ids.append(ObjectId(doc_id))

                except Exception as e:

                    logger.warning(f"Invalid document ID: {doc_id}")

            if not valid_ids:

                return jsonify({"error": "No valid document IDs provided"}), 400

            # Check if any documents exist with the provided IDs

            count = documents.count_documents({"_id": {"$in": valid_ids}})

            if count == 0:

                return jsonify({"error": "No documents found for the provided IDs."}), 404

            # Retrieve the documents

            documents_cursor = documents.find({"_id": {"$in": valid_ids}})

            # Create CSV

            output = StringIO()

            writer = csv.writer(output)

            writer.writerow(['filename', 'OCR', 'original_json'])  # Header row

            for doc in documents_cursor:

                filename = doc.get('filename', 'N/A')

                ocr = doc.get('summary', 'N/A')  # Adjust field as necessary

                original_json = json.dumps(doc, default=str)  # Convert ObjectId to string if necessary

                writer.writerow([filename, ocr, original_json])

            # Prepare CSV for download

            output.seek(0)

            return Response(

                output.getvalue(),

                mimetype='text/csv',

                headers={'Content-Disposition': 'attachment; filename=selected_documents.csv'}

            )

        except Exception as e:

            logger.error(f"Error exporting selected CSV: {str(e)}", exc_info=True)

            return jsonify({"error": "An internal error occurred"}), 500

    @app.errorhandler(404)

    def not_found_error(error):

        return render_template('error.html', message='Page not found'), 404

    @app.errorhandler(500)

    def internal_error(error):

        return render_template('error.html', message='An unexpected error has occurred'), 500

### 1.5 Flask Application Initialization (`app/app.py`)

Initializes the Flask application, cache, and database connections.

    python

    Copy code

    # app/app.py

    from flask import Flask

    from flask_caching import Cache

    from database_setup import get_client, get_db, get_collections

    app = Flask(__name__)

    app.config.from_envvar('APP_CONFIG_FILE')  # Ensure the environment variable points to your config file

    # Initialize Cache (e.g., SimpleCache or RedisCache for production)

    cache = Cache(app, config={'CACHE_TYPE': 'simple'})

    # Initialize database connection

    client = get_client()

    db = get_db(client)

    documents, unique_terms_collection, field_structure_collection = get_collections(db)

    # Import routes after initializing app and db

    from . import routes

### 1.6 Database Setup Script (`app/database_setup.py`)

Sets up MongoDB collections, indexes, and provides utility functions for
database interactions.

    python

    Copy code

    # app/database_setup.py

    import os

    import logging

    from pymongo import MongoClient, ASCENDING, DESCENDING

    from pymongo.errors import ConnectionFailure

    # Configure logging

    logger = logging.getLogger('DatabaseSetup')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

    def get_client():

        """Initialize and return a new MongoDB client."""

        try:

            mongo_uri = os.environ.get('MONGO_URI')

            if not mongo_uri:

                raise ValueError("MONGO_URI environment variable not set")

            client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)

            # Test connection

            client.admin.command('ping')

            logger.info("Successfully connected to MongoDB.")

            return client

        except Exception as e:

            logger.error(f"Failed to connect to MongoDB: {e}")

            raise e

    def get_db(client):

        """Return the database instance."""

        return client['railroad_documents']

    def get_collections(db):

        """Retrieve and return references to the required collections."""

        try:

            documents_collection = db['documents']

            unique_terms_collection = db['unique_terms']

            field_structure_collection = db['field_structure']

            return documents_collection, unique_terms_collection, field_structure_collection

        except Exception as e:

            logger.error(f"Error getting collections: {e}")

            raise e

    def initialize_database(client):

        """Initialize database collections, indexes, and any necessary setup."""

        db = get_db(client)

        documents, unique_terms_collection, field_structure_collection = get_collections(db)

        # Create indexes for the documents collection

        documents.create_index([('filename', ASCENDING)], unique=True)

        documents.create_index([('ocr_text', 'text'), ('summary', 'text')])

        # Create indexes for unique_terms collection

        unique_terms_collection.create_index([('term', ASCENDING)], unique=True)

        unique_terms_collection.create_index([('field', ASCENDING), ('type', ASCENDING)])

        # Create indexes for field_structure collection

        field_structure_collection.create_index([('field', ASCENDING)], unique=True)

        logger.info("Database initialized with necessary collections and indexes.")

    def find_document_by_id(db, doc_id):

        """Find a document by its ID."""

        try:

            documents, _, _ = get_collections(db)

            document = documents.find_one({"_id": ObjectId(doc_id)})

            return document

        except Exception as e:

            logger.error(f"Error finding document by ID {doc_id}: {e}")

            return None

    def update_document(db, doc_id, update_fields):

        """Update a document with given fields."""

        try:

            documents, _, _ = get_collections(db)

            result = documents.update_one({"_id": ObjectId(doc_id)}, {"$set": update_fields})

            return result.modified_count

        except Exception as e:

            logger.error(f"Error updating document {doc_id}: {e}")

            return 0

    def delete_document(db, doc_id):

        """Delete a document by its ID."""

        try:

            documents, _, _ = get_collections(db)

            result = documents.delete_one({"_id": ObjectId(doc_id)})

            return result.deleted_count

        except Exception as e:

            logger.error(f"Error deleting document {doc_id}: {e}")

            return 0

    def get_field_structure(db):

        """Retrieve the field structure of the documents."""

        try:

            _, _, field_structure_collection = get_collections(db)

            structure = field_structure_collection.find_one({})

            return structure.get('structure', {}) if structure else {}

        except Exception as e:

            logger.error(f"Error retrieving field structure: {e}")

            return {}

### 1.7 Data Processing Script (`app/data_processing.py`)

Processes incoming JSON and TXT files, inserting them into MongoDB and
updating field structures.

    python

    Copy code

    # app/data_processing.py

    import os

    import json

    import logging

    from database_setup import get_client, get_db, get_collections

    from bson.objectid import ObjectId

    from json_validator import validate_json  # Assume you have a JSON validator script

    # Configure logging

    logger = logging.getLogger('DataProcessing')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

    def process_files(directory):

        """Process JSON and TXT files in the given directory."""

        client = get_client()

        db = get_db(client)

        documents, unique_terms_collection, field_structure_collection = get_collections(db)

        for filename in os.listdir(directory):

            filepath = os.path.join(directory, filename)

            if not os.path.isfile(filepath):

                continue

            if filename.endswith('.json'):

                with open(filepath, 'r', encoding='utf-8') as file:

                    try:

                        data = json.load(file)

                        if not validate_json(data):

                            logger.warning(f"Validation failed for {filename}. Skipping.")

                            continue

                        # Insert document into MongoDB

                        result = documents.insert_one(data)

                        logger.info(f"Inserted document {result.inserted_id} from {filename}.")

                    except json.JSONDecodeError:

                        logger.error(f"Invalid JSON format in {filename}. Skipping.")

                    except Exception as e:

                        logger.error(f"Error processing {filename}: {e}")

            elif filename.endswith('.txt'):

                # Handle TXT files if necessary

                logger.info(f"Processing TXT file: {filename}")

                # Implement TXT processing logic here

    def main():

        """Main function to process files."""

        archives_dir = os.path.join(os.getcwd(), 'archives')

        if not os.path.exists(archives_dir):

            logger.error(f"Archives directory does not exist: {archives_dir}")

            return

        process_files(archives_dir)

        logger.info("Data processing completed.")

    if __name__ == "__main__":

        main()

### 1.8 NER Processing Script (`app/ner_processing.py`)

Handles NER processing on documents, extracting entities and preparing
them for linking.

    python

    Copy code

    # app/ner_processing.py

    import os

    import logging

    from collections import defaultdict

    from database_setup import get_client, get_db, get_collections

    from ner_worker import process_documents_batch, initialize_spacy

    from entity_linking import link_entities

    # Configure logging

    logger = logging.getLogger('NERProcessing')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

    def main():

        """Main function to perform NER and fuzzy matching."""

        client = get_client()

        db = get_db(client)

        documents, linked_entities_collection, _ = get_collections(db)

        # Define processing parameters

        FIELDS_TO_PROCESS = ["ocr_text", "summary"]

        LINK_WIKIDATA = False

        FUZZY_MATCH_THRESHOLD = 90

        BATCH_SIZE = 1000

        ENABLE_MULTIPROCESSING = True

        # Perform entity extraction and linking

        extract_and_link_entities(

            documents_collection=documents,

            linked_entities_collection=linked_entities_collection,

            fields_to_process=FIELDS_TO_PROCESS,

            link_wikidata=LINK_WIKIDATA,

            fuzzy_threshold=FUZZY_MATCH_THRESHOLD,

            batch_size=BATCH_SIZE,

            use_multiprocessing=ENABLE_MULTIPROCESSING

        )

    if __name__ == "__main__":

        main()

## 1.9 Entity Linking Script (`entity_linking.py`)

This script handles the linking of extracted entities to external
knowledge bases (e.g., Wikidata) and updates the MongoDB collection with
the linked entities.

    python

    Copy code

    # entity_linking.py

    import os

    import logging

    import requests

    from pymongo import UpdateOne, MongoClient

    from dotenv import load_dotenv

    from collections import defaultdict

    from rapidfuzz import process, fuzz

    # Load environment variables from .env file

    load_dotenv()

    # =======================

    # Logging Configuration

    # =======================

    logger = logging.getLogger('EntityLinkingLogger')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.DEBUG)

        file_handler = logging.FileHandler('entity_linking.log', mode='a')

        file_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

        logger.addHandler(file_handler)

    # =======================

    # Database Functions

    # =======================

    def get_client():

        """Initialize and return a new MongoDB client."""

        try:

            mongo_uri = os.environ.get('MONGO_URI')

            if not mongo_uri:

                raise ValueError("MONGO_URI environment variable not set")

            client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)

            # Test connection

            client.admin.command('ping')

            logger.info("Successfully connected to MongoDB.")

            return client

        except Exception as e:

            logger.error(f"Failed to connect to MongoDB: {e}")

            raise e

    def get_db(client):

        """Return the database instance."""

        return client['railroad_documents']

    def get_collections(db):

        """Retrieve and return references to the required collections."""

        try:

            documents_collection = db['documents']

            linked_entities_collection = db['linked_entities']

            return documents_collection, linked_entities_collection

        except Exception as e:

            logger.error(f"Error getting collections: {e}")

            raise e

    # =======================

    # Utility Functions

    # =======================

    def fuzzy_match(term, reference_terms, threshold=90):

        """

        Perform fuzzy matching to find the best match for a term in reference_terms.

        Returns the matched term if similarity exceeds the threshold, else None.

        """

        try:

            result = process.extractOne(term, reference_terms, scorer=fuzz.token_sort_ratio)

            if result is None:

                logger.debug(f"No fuzzy match found for term '{term}'.")

                return None

            match, score = result

            logger.debug(f"Fuzzy match for term '{term}': '{match}' with score {score}.")

            if score >= threshold:

                return match

            return None

        except Exception as e:

            logger.error(f"Exception during fuzzy matching for term '{term}': {e}")

            return None

    def fetch_wikidata_entity(term):

        """

        Fetch Wikidata entity ID for a given term using the Wikidata API.

        Returns the entity ID if found, else None.

        """

        wikidata_cache = {}

        if term in wikidata_cache:

            return wikidata_cache[term]

        try:

            url = "https://www.wikidata.org/w/api.php"

            params = {

                'action': 'wbsearchentities',

                'search': term,

                'language': 'en',

                'format': 'json'

            }

            response = requests.get(url, params=params, timeout=5)

            data = response.json()

            if 'search' in data and len(data['search']) > 0:

                # Return the first matching entity ID

                wikidata_id = data['search'][0]['id']

                wikidata_cache[term] = wikidata_id

                logger.debug(f"Fetched Wikidata ID '{wikidata_id}' for term '{term}'.")

                return wikidata_id

            else:

                wikidata_cache[term] = None

                logger.debug(f"No Wikidata ID found for term '{term}'.")

                return None

        except Exception as e:

            logger.error(f"Error fetching Wikidata entity for term '{term}': {e}")

            return None

    # =======================

    # Entity Linking Function

    # =======================

    def link_entities(aggregated_entities, linked_entities_collection, existing_linked_terms, link_wikidata, fuzzy_threshold, batch_size=1000):

        """

        Link aggregated entities and update the linked_entities collection.

        """

        linked_count = 0

        processed_entities = 0

        total_entities = len(aggregated_entities)

        logger.info(f"Total unique entities to process: {total_entities}")

        operations = []

        for (term_lower, ent_type), data in aggregated_entities.items():

            frequency = data['frequency']

            document_ids = list(data['document_ids'])

            wikidata_id = None

            if link_wikidata:

                wikidata_id = fetch_wikidata_entity(term_lower)

            # If Wikidata linking is disabled or Wikidata ID not found, perform fuzzy matching

            if not wikidata_id:

                fuzzy_match_term = fuzzy_match(term_lower, existing_linked_terms, threshold=fuzzy_threshold)

                if fuzzy_match_term:

                    # Fetch the corresponding Wikidata ID for the matched term

                    matched_entity = linked_entities_collection.find_one({"term": fuzzy_match_term.lower()})

                    if matched_entity:

                        wikidata_id = matched_entity.get('kb_id')

                        logger.debug(f"Fuzzy matched term '{term_lower}' to '{fuzzy_match_term}' with Wikidata ID '{wikidata_id}'.")

                else:

                    logger.debug(f"No match found for term '{term_lower}'.")

            if wikidata_id:

                logger.debug(f"Linked term '{term_lower}' to Wikidata ID '{wikidata_id}'.")

            update = {

                "$inc": {"frequency": frequency},

                "$addToSet": {"document_ids": {"$each": document_ids}},

                "$set": {"type": ent_type, "kb_id": wikidata_id}

            }

            operations.append(

                UpdateOne(

                    {"term": term_lower},

                    update,

                    upsert=True

                )

            )

            linked_count += 1

            processed_entities += 1

            # Log progress every 1000 entities

            if processed_entities % 1000 == 0:

                logger.info(f"Processed {processed_entities}/{total_entities} entities.")

            # Execute operations in batches

            if len(operations) >= batch_size:

                logger.info(f"Writing batch of {len(operations)} entities to the database.")

                try:

                    result = linked_entities_collection.bulk_write(operations, ordered=False)

                    logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")

                except Exception as e:

                    logger.error(f"Error bulk upserting linked entities: {e}")

                    raise e

                operations = []

        # Execute remaining operations

        if operations:

            logger.info(f"Writing final batch of {len(operations)} entities to the database.")

            try:

                result = linked_entities_collection.bulk_write(operations, ordered=False)

                logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} linked entities.")

            except Exception as e:

                logger.error(f"Error bulk upserting linked entities: {e}")

                raise e

        logger.info(f"Successfully linked {linked_count} entities.")

## 1.10 API Integration Script (`api_integration.py`)

This script facilitates communication between the local server and the
HPC cluster by defining additional API endpoints if necessary. It
ensures that the system remains abstracted, allowing for future
offloading of various computational tasks.

    python

    Copy code

    # api_integration.py

    from flask import Blueprint, request, jsonify

    from functools import wraps

    import os

    import logging

    import requests

    import json

    import gzip

    # Create a Blueprint for API routes

    api_bp = Blueprint('api', __name__)

    # =======================

    # Logging Configuration

    # =======================

    logger = logging.getLogger('APIIntegrationLogger')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.DEBUG)

        file_handler = logging.FileHandler('api_integration.log', mode='a')

        file_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

        logger.addHandler(file_handler)

    # =======================

    # Authentication Decorator

    # =======================

    API_KEY = os.getenv('API_KEY', 'default_api_key')

    def require_api_key(f):

        @wraps(f)

        def decorated(*args, **kwargs):

            key = request.headers.get('x-api-key')

            if key and key == API_KEY:

                return f(*args, **kwargs)

            else:

                logger.warning(f"Unauthorized access attempt from {request.remote_addr}")

                return jsonify({"message": "Unauthorized"}), 401

        return decorated

    # =======================

    # API Endpoints

    # =======================

    @api_bp.route('/api/execute-task', methods=['POST'])

    @require_api_key

    def execute_task():

        """

        Endpoint to receive tasks from the HPC cluster for execution.

        Allows offloading of additional computational tasks.

        """

        try:

            if not request.is_json:

                logger.error("Received non-JSON data.")

                return jsonify({"message": "Invalid data format. JSON expected."}), 400

            data = request.get_json()

            task_type = data.get('task_type')

            task_payload = data.get('payload')

            if not task_type or not task_payload:

                logger.error("Missing 'task_type' or 'payload' in the request.")

                return jsonify({"message": "Missing 'task_type' or 'payload'."}), 400

            # Handle different task types

            if task_type == 'ner_fuzzy':

                # Process NER and fuzzy matching

                from ner_processing import extract_and_link_entities  # Import here to avoid circular imports

                from database_setup import get_client, get_db, get_collections

                client = get_client()

                db = get_db(client)

                documents_collection, linked_entities_collection = get_collections(db)

                # Extract entities and perform linking

                extract_and_link_entities(

                    documents_collection,

                    linked_entities_collection,

                    fields_to_process=task_payload.get('fields_to_process', ["ocr_text", "summary"]),

                    link_wikidata=task_payload.get('link_wikidata', False),

                    fuzzy_threshold=task_payload.get('fuzzy_threshold', 90),

                    batch_size=task_payload.get('batch_size', 1000),

                    use_multiprocessing=task_payload.get('use_multiprocessing', False)

                )

                logger.info(f"Completed task_type: {task_type}")

                return jsonify({"status": "success", "message": f"Task {task_type} completed."}), 200

            else:

                logger.error(f"Unsupported task_type: {task_type}")

                return jsonify({"message": f"Unsupported task_type: {task_type}."}), 400

        except Exception as e:

            logger.error(f"Error executing task: {e}", exc_info=True)

            return jsonify({"message": "An error occurred while executing the task."}), 500

**Integration with `app.py`:**

To incorporate the `api_integration.py` blueprint into your Flask
application, update `app.py` as follows:

    python

    Copy code

    # app.py

    from flask import Flask

    from api_integration import api_bp  # Import the blueprint

    app = Flask(__name__)

    # ... existing configurations and routes ...

    # Register the API blueprint

    app.register_blueprint(api_bp)

    # ... rest of the app ...

## 1.11 Entry Point Script (`entrypoint.sh`)

This script initializes the application by ensuring that MongoDB is
ready before starting the Flask app. It also includes provisions for
running setup scripts and can be extended for additional initialization
tasks.

    bash

    Copy code

    #!/bin/bash

    # Exit immediately if a command exits with a non-zero status

    set -e

    # Configuration for backoff

    MAX_RETRIES=10

    SLEEP_TIME=10

    MONGO_HOST="mongodb"

    MONGO_PORT=27017

    MONGO_URI="mongodb://admin:secret@${MONGO_HOST}:${MONGO_PORT}/admin"

    echo "Waiting for MongoDB to be ready..."

    # Backoff loop to wait for MongoDB

    for ((i=1;i<=MAX_RETRIES;i++)); do

        echo "Attempt $i/$MAX_RETRIES: Checking MongoDB connection..."

        python -c "import pymongo; client = pymongo.MongoClient('${MONGO_URI}', serverSelectionTimeoutMS=5000); client.admin.command('ping')" && break

        echo "MongoDB is not ready yet. Waiting ${SLEEP_TIME} seconds..."

        sleep ${SLEEP_TIME}

    done

    # Verify if MongoDB is up after retries

    python -c "import pymongo; client = pymongo.MongoClient('${MONGO_URI}', serverSelectionTimeoutMS=5000); client.admin.command('ping')" || { echo "MongoDB did not become ready in time after ${MAX_RETRIES} attempts. Exiting."; exit 1; }

    echo "MongoDB is up and running."

    # Uncomment the following lines to run setup scripts

    # echo "Running database_setup.py..."

    # python database_setup.py

    # echo "database_setup.py completed."

    # echo "Running data_processing.py..."

    # python data_processing.py

    # echo "data_processing.py completed."

    # echo "Running generate_unique_terms.py..."

    # python generate_unique_terms.py

    # echo "generate_unique_terms.py completed."

    # Uncomment if you want to run NER processing at startup

    # echo "Running ner_processing.py..."

    # python ner_processing.py

    # echo "ner_processing.py completed."

    echo "Setup scripts completed. Starting Flask app..."

    # Start the Flask app

    exec "$@"

**Explanation:**

-   **MongoDB Health Check:** The script attempts to connect to MongoDB
    using the provided URI. It retries the connection up to
    `MAX_RETRIES` times, waiting `SLEEP_TIME` seconds between attempts.

-   **Setup Scripts:** Provision is made to run setup scripts
    (`database_setup.py`, `data_processing.py`,
    `generate_unique_terms.py`, `ner_processing.py`) before starting the
    Flask app. These can be uncommented as needed.

-   **Starting the Flask App:** After ensuring MongoDB is ready and
    running setup scripts, the script starts the Flask application.

## 1.12 Environment Variables (`.env`)

Ensure that all necessary environment variables are defined in the
`.env` file. This file should be located in the root directory and
included in `.gitignore` to prevent sensitive information from being
committed to version control.

    env

    Copy code

    # MongoDB Credentials

    MONGO_INITDB_ROOT_USERNAME=admin

    MONGO_INITDB_ROOT_PASSWORD=secret

    # Flask Configuration

    FLASK_ENV=development

    FLASK_DEBUG=1

    SECRET_KEY=your_secure_secret_key

    # MongoDB URI

    MONGO_URI=mongodb://admin:secret@mongodb:27017/admin

    # API Configuration

    API_KEY=your_secure_api_key  # Replace with a strong, secure key

    # LLM Integration (Future Use)

    OPENAI_API_KEY=your_API_KEY  # Placeholder for future LLM integration

    ENABLE_LLM=False

    # Fuzzy Matching Configuration

    FUZZY_MATCH_THRESHOLD=90       # Similarity threshold (0-100)

    BATCH_SIZE=1000                # Number of entities to process per batch

    FIELDS_TO_PROCESS=["ocr_text", "summary"]  # Fields to perform NER and fuzzy matching on

    ENABLE_MULTIPROCESSING=True    # Enable or disable multiprocessing in processing scripts

    LINK_WIKIDATA=False            # Enable or disable linking to Wikidata

**Security Note:** Ensure that `.env` is added to `.gitignore` to
prevent accidental exposure of sensitive information.

## 2. Final Design Document Updates

### 2.1 Deployment and Infrastructure Enhancements

-   **Health Checks for MongoDB:**

    -   Implemented health checks in the Docker Compose configuration to
        ensure MongoDB is ready before the Flask application starts.
        This enhances reliability by preventing the Flask app from
        attempting to connect to an unavailable database.

<!-- -->

-   **Multi-Stage Dockerfile:**

    -   Utilized a multi-stage Dockerfile to optimize the image size and
        separate build dependencies from the runtime environment.
        Pre-downloaded spaCy models in the build stage to expedite NER
        processing during runtime.

<!-- -->

-   **Entrypoint Script (`entrypoint.sh`):**

    -   Introduced an entrypoint script to handle service
        initialization, including waiting for MongoDB to be ready. This
        script also allows for running setup scripts before launching
        the Flask application.

<!-- -->

-   **Volume Mounts for Initialization Scripts:**

    -   Configured volume mounts for MongoDB initialization scripts
        (`./mongo-init/`) to automate the setup of databases,
        collections, and indexes upon container startup.

### 2.2 Security Considerations Enhancements

-   **API Key Authentication:**

    -   Added API key-based authentication for all API endpoints to
        secure communication between the local server and the HPC
        cluster.

<!-- -->

-   **CAPTCHA and Login Attempt Tracking:**

    -   Enhanced the authentication mechanism in `routes.py` by
        implementing CAPTCHA verification and tracking login attempts to
        prevent brute-force attacks.

<!-- -->

-   **Environment Variable Management:**

    -   Emphasized the use of environment variables for sensitive
        configurations, ensuring they are securely managed via the
        `.env` file and excluded from version control.

### 2.3 Monitoring and Logging Enhancements

-   **Centralized Logging:**

    -   Configured Docker logging drivers to manage log rotation,
        preventing log files from consuming excessive disk space. Logs
        for different services are segregated for better traceability.

<!-- -->

-   **Comprehensive Logging in Scripts:**

    -   Enhanced logging in all scripts (`entity_linking.py`,
        `api_integration.py`, etc.) to facilitate easier debugging and
        monitoring of the processing workflows.

### 2.4 API Design Enhancements

-   **Modular API Endpoints:**

    -   Created additional API endpoints in `api_integration.py` to
        handle a variety of computational tasks, keeping the system
        abstracted and ready for future offloading requirements.

<!-- -->

-   **Compression and Data Transfer Optimization:**

    -   Implemented data compression (gzip) for sending large payloads
        between the HPC cluster and the Flask API, optimizing bandwidth
        usage and improving transfer speeds.

### 2.5 Extensibility for Future Offloading Tasks

-   **Abstracted Task Handling:**

    -   Designed the API integration to handle different `task_type`
        values, allowing the system to support additional computational
        tasks beyond NER and fuzzy matching in the future.

<!-- -->

-   **Encapsulated Processing Logic:**

    -   Structured processing scripts to be modular, making it easier to
        integrate new processing functionalities without disrupting
        existing workflows.

## 3. Additional Scripts and Components

### 3.1 Unique Terms Generation Script (`generate_unique_terms.py`)

This script generates a collection of unique terms from the documents
for efficient fuzzy matching and analysis.

    python

    Copy code

    # generate_unique_terms.py

    import os

    import logging

    import json

    from collections import defaultdict

    from pymongo import UpdateOne, MongoClient

    from dotenv import load_dotenv

    # Load environment variables from .env file

    load_dotenv()

    # =======================

    # Logging Configuration

    # =======================

    logger = logging.getLogger('UniqueTermsGenerator')

    logger.setLevel(logging.DEBUG)

    if not logger.handlers:

        console_handler = logging.StreamHandler()

        console_handler.setLevel(logging.INFO)

        file_handler = logging.FileHandler('unique_terms.log', mode='a')

        file_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler.setFormatter(formatter)

        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)

        logger.addHandler(file_handler)

    # =======================

    # Database Functions

    # =======================

    def get_client():

        """Initialize and return a new MongoDB client."""

        try:

            mongo_uri = os.environ.get('MONGO_URI')

            if not mongo_uri:

                raise ValueError("MONGO_URI environment variable not set")

            client = MongoClient(mongo_uri, serverSelectionTimeoutMS=5000)

            # Test connection

            client.admin.command('ping')

            logger.info("Successfully connected to MongoDB.")

            return client

        except Exception as e:

            logger.error(f"Failed to connect to MongoDB: {e}")

            raise e

    def get_db(client):

        """Return the database instance."""

        return client['railroad_documents']

    def get_collections(db):

        """Retrieve and return references to the required collections."""

        try:

            documents_collection = db['documents']

            unique_terms_collection = db['unique_terms']

            return documents_collection, unique_terms_collection

        except Exception as e:

            logger.error(f"Error getting collections: {e}")

            raise e

    # =======================

    # Unique Terms Generation

    # =======================

    def generate_unique_terms(documents_collection, unique_terms_collection, fields_to_process, term_type='word', batch_size=1000):

        """

        Generate unique terms from specified fields in documents and update the unique_terms collection.

        

        :param documents_collection: MongoDB collection containing documents

        :param unique_terms_collection: MongoDB collection to store unique terms

        :param fields_to_process: List of fields to extract terms from

        :param term_type: Type of term ('word' or 'phrase')

        :param batch_size: Number of documents to process per batch

        """

        logger.info(f"Starting unique terms generation for term_type: {term_type}")

        cursor = documents_collection.find({}, {field: 1 for field in fields_to_process})

        

        term_counts = defaultdict(int)

        

        for doc in cursor.batch_size(batch_size):

            for field in fields_to_process:

                text = doc.get(field, '')

                if text:

                    if term_type == 'word':

                        words = text.split()

                        for word in words:

                            clean_word = word.strip().lower()

                            if clean_word:

                                term_counts[clean_word] += 1

                    elif term_type == 'phrase':

                        # Example: Extract bigrams as phrases

                        words = text.split()

                        for i in range(len(words)-1):

                            phrase = f"{words[i].strip().lower()} {words[i+1].strip().lower()}"

                            if phrase:

                                term_counts[phrase] += 1

        

        # Prepare bulk operations

        operations = []

        for term, count in term_counts.items():

            operations.append(

                UpdateOne(

                    {"term": term, "type": term_type, "field": "ocr_text"},  # Assuming 'ocr_text' is the field

                    {"$set": {"term": term, "type": term_type, "field": "ocr_text"}, "$inc": {"frequency": count}},

                    upsert=True

                )

            )

        

        if operations:

            try:

                result = unique_terms_collection.bulk_write(operations, ordered=False)

                logger.info(f"Bulk upserted {result.upserted_count + result.modified_count} unique terms.")

            except Exception as e:

                logger.error(f"Error bulk upserting unique terms: {e}")

                raise e

        else:

            logger.info("No unique terms to upsert.")

        logger.info("Unique terms generation completed.")

    # =======================

    # Main Execution

    # =======================

    if __name__ == "__main__":

        try:

            client = get_client()

            db = get_db(client)

            documents_collection, unique_terms_collection = get_collections(db)

            

            # Define fields to process and term types

            fields_to_process = ["ocr_text", "summary"]

            term_types = ['word', 'phrase']

            

            for term_type in term_types:

                generate_unique_terms(

                    documents_collection,

                    unique_terms_collection,

                    fields_to_process=fields_to_process,

                    term_type=term_type,

                    batch_size=1000

                )

        

        except Exception as e:

            logger.error(f"An error occurred during unique terms generation: {e}", exc_info=True)

## 1.12 Docker Compose Configuration (`docker-compose.yml`)

Updated Docker Compose configuration incorporating health checks,
multi-stage builds, and logging settings.

    yaml

    Copy code

    version: '3.8'

    services:

      mongodb:

        image: mongo:6.0

        container_name: mongodb

        environment:

          MONGO_INITDB_ROOT_USERNAME: admin

          MONGO_INITDB_ROOT_PASSWORD: secret

        volumes:

          - mongodb_data:/data/db

          - ./mongo-init/:/docker-entrypoint-initdb.d/

        ports:

          - "27017:27017"

        healthcheck:

          test: [

            "CMD",

            "mongosh",

            "--username",

            "admin",

            "--password",

            "secret",

            "--authenticationDatabase",

            "admin",

            "--eval",

            "db.adminCommand('ping')"

          ]

          interval: 10s

          timeout: 5s

          retries: 5

        networks:

          - app_network

        logging:

          driver: "json-file"

          options:

            max-size: "10m"

            max-file: "3"

      flask_app:

        build:

          context: ./app

          dockerfile: Dockerfile

        container_name: flask_app

        ports:

          - "5000:5000"

        environment:

          MONGO_URI: "mongodb://${MONGO_INITDB_ROOT_USERNAME}:${MONGO_INITDB_ROOT_PASSWORD}@mongodb:27017/admin"

          FLASK_APP: app.py

          FLASK_ENV: ${FLASK_ENV}

          FLASK_DEBUG: ${FLASK_DEBUG}

          SECRET_KEY: ${SECRET_KEY}

          API_KEY: ${API_KEY}

        volumes:

          - ./app:/app

          - ./archives:/app/archives

        depends_on:

          mongodb:

            condition: service_healthy

        networks:

          - app_network

        logging:

          driver: "json-file"

          options:

            max-size: "10m"

            max-file: "3"

    volumes:

      mongodb_data:

    networks:

      app_network:

        driver: bridge

    # Notes:

    # - Ensure you have a .env file in your project root with the necessary variables.

    # - Add .env to your .gitignore to prevent committing sensitive information.

**Key Enhancements:**

-   **Health Checks:** Added health checks for MongoDB to ensure it\'s
    ready before the Flask app starts.

-   **Logging:** Configured log rotation to prevent log files from
    growing indefinitely.

-   **Environment Variables:** Included `API_KEY` in the Flask app\'s
    environment variables for secure API access.

## 3. Final Considerations for Extensibility

### 3.1 Abstracted Task Handling

The system is designed to handle various computational tasks beyond NER
and fuzzy matching. By utilizing the `execute_task` endpoint in
`api_integration.py`, additional processing scripts can be integrated
seamlessly. Future tasks can be added by defining new `task_type`
handlers within this script.

### 3.2 Encapsulation of Processing Logic

Processing scripts like `entity_linking.py`, `ner_processing.py`, and
`generate_unique_terms.py` are modular and can be invoked independently
or in conjunction with other scripts. This encapsulation ensures that
adding new functionalities does not disrupt existing workflows.

### 3.3 Scalable HPC Integration

The HPC processing script (`hpc_processor.py`) is designed to fetch and
execute tasks in batches, allowing for scalable processing as
computational demands grow. Multiprocessing is enabled to utilize
available CPU cores efficiently.

### 3.4 Secure and Efficient Communication

All communications between the HPC cluster and the local server are
secured via API key authentication and can be further protected using
HTTPS tunnels like ngrok. Data compression ensures efficient bandwidth
usage, especially when dealing with large datasets.

## 4. Final Design Document Overview

The **Revised Design Document** incorporates the following key updates
based on the newly provided Docker configurations and scripts:

-   **Deployment and Infrastructure:**

    -   Health checks for MongoDB.

    -   Multi-stage Docker builds for optimized images.

    -   Entrypoint scripts for service initialization.

    -   Volume mounts for MongoDB initialization scripts.

<!-- -->

-   **Security Considerations:**

    -   API key-based authentication for secure API access.

    -   CAPTCHA and login attempt tracking to enhance security.

    -   Secure management of environment variables.

<!-- -->

-   **Monitoring and Logging:**

    -   Centralized and rotated logging configurations.

    -   Comprehensive logging in all scripts for easier debugging.

<!-- -->

-   **API Design:**

    -   Modular API endpoints to handle diverse computational tasks.

    -   Data compression and efficient transfer mechanisms.

<!-- -->

-   **Extensibility:**

    -   Abstracted and encapsulated processing logic to allow for future
        task offloading.

    -   Scalable HPC integration with support for multiprocessing.

<!-- -->

-   **File Structure and Docker Setup:**

    -   Clear and organized directory structure.

    -   Docker Compose setup for easy deployment and management of
        services.

By implementing the above scripts and configurations, your system is now
equipped to securely and efficiently offload NER and fuzzy matching
tasks to an HPC cluster, with the flexibility to incorporate additional
processing tasks in the future. Ensure that all environment variables
are correctly set in the `.env` file and that sensitive information
remains secured and excluded from version control.

If you have any further files or components to share, feel free to
provide them, and I can assist in integrating them into the design and
implementation.


********************************************************************************

