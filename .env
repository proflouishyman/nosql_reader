
# --- 1. MONGODB CONFIGURATION (DOCKER & APP) ---
MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=secret
MONGO_URI=mongodb://admin:secret@mongodb:27017/admin
APP_MONGO_URI=mongodb://admin:secret@mongodb:27017/admin
MONGO_ROOT_USERNAME=admin
MONGO_ROOT_PASSWORD=secret
DB_NAME=railroad_documents
CHUNKS_COLLECTION=document_chunks
DOCS_COLLECTION=documents

# =============================================================================
# MONGODB CONFIGURATION
# =============================================================================

# MongoDB connection URI (required)
# Format: mongodb://username:password@host:port


# Database name
MONGO_DB_NAME=railroad_documents

# Collection names
DOCUMENTS_COLLECTION=documents


# Connection timeouts (milliseconds)
MONGO_CONNECT_TIMEOUT_MS=5000
MONGO_SERVER_TIMEOUT_MS=5000

# =============================================================================
# CHROMADB (VECTOR STORE) CONFIGURATION
# =============================================================================

# Directory for ChromaDB persistence
CHROMA_PERSIST_DIRECTORY=/app/data/chroma

# Collection name for vector embeddings
CHROMA_COLLECTION_NAME=historian_documents

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================

# Embedding provider: "ollama", "openai", or "local"
EMBEDDING_PROVIDER=ollama

# Embedding model (for Ollama)
HISTORIAN_AGENT_EMBEDDING_MODEL=qwen3-embedding:0.6b

# Embedding dimension (1024 for qwen3-embedding:0.6b)
EMBEDDING_DIMENSION=1024

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=32

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Number of final documents to return (after reranking)
HISTORIAN_AGENT_TOP_K=5

# Size of initial retrieval pool (before reranking)
RETRIEVAL_POOL_SIZE=40

# Maximum parent documents to retrieve in Tier 2
PARENT_RETRIEVAL_CAP=8

# Hybrid retrieval weights (must sum to 1.0)
VECTOR_WEIGHT=0.7
KEYWORD_WEIGHT=0.3

# Reciprocal Rank Fusion constant
RRF_K=60

# =============================================================================
# OLLAMA CONFIGURATION (LOCAL LLM)
# =============================================================================

# Ollama API base URL
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Ollama timeout (seconds)
OLLAMA_TIMEOUT=120

# Ollama-specific options
OLLAMA_NUM_CTX=131072
OLLAMA_REPEAT_PENALTY=1.15

# =============================================================================
# LLM MODELS (OLLAMA)
# =============================================================================

# Main generation model (quality profile)
LLM_MODEL=qwen2.5:32b

# Fast model for quick operations (fast profile)
LLM_FAST_MODEL=llama3.2:3b

# Verification model (verifier profile)
VERIFIER_MODEL=qwen2.5:32b

# =============================================================================
# LLM GENERATION SETTINGS
# =============================================================================

# Generation temperature (0.0-1.0, lower = more deterministic)
LLM_TEMPERATURE=0.2

# Maximum tokens to generate
LLM_NUM_PREDICT=4000

# LLM timeout (seconds)
LLM_TIMEOUT=120

# =============================================================================
# VERIFICATION SETTINGS (ADVERSARIAL RAG)
# =============================================================================

# Verification temperature (always 0.0 for deterministic fact-checking)
# (Not configurable - hardcoded to 0.0 in config.py)

# Verification token limit
VERIFIER_NUM_PREDICT=500

# Verification timeout (seconds)
VERIFIER_TIMEOUT=60

# Maximum retries for verification calls
VERIFIER_MAX_RETRIES=3

# Minimum fallback score when verification fails
MIN_SCORE_FALLBACK=75

# Confidence threshold (0.0-1.0) for Tier 1 -> Tier 2 escalation
# 0.9 means 90/100 verification score required to skip Tier 2
CONFIDENCE_THRESHOLD=0.9

# =============================================================================
# OPENAI CONFIGURATION (CLOUD FALLBACK - OPTIONAL)
# =============================================================================

# OpenAI API key (only needed if using 'cloud' profile)
OPENAI_API_KEY=

# OpenAI model (for cloud profile)
OPENAI_MODEL=gpt-4

# OpenAI timeout (seconds)
OPENAI_TIMEOUT=30

# =============================================================================
# LM STUDIO CONFIGURATION (OPTIONAL)
# =============================================================================

# LM Studio base URL (OpenAI-compatible API)
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# LM Studio timeout (seconds)
LMSTUDIO_TIMEOUT=120

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Debug mode (1=enabled, 0=disabled)
# Enables verbose logging and debug output
DEBUG_MODE=1

# Flask debug mode (1=enabled, 0=disabled)
# Enables Flask auto-reload and detailed error pages
FLASK_DEBUG=0

# Secret key for Flask sessions
# IMPORTANT: Change this in production!
SECRET_KEY=change-me-in-production

# Session file directory (optional)
# If not set, defaults to system temp directory
SESSION_FILE_DIR=/app/flask_session

# =============================================================================
# TIER 0 (CORPUS EXPLORATION) CONFIGURATION
# =============================================================================

# Total documents to read in Tier 0 exploration
TIER0_EXPLORATION_BUDGET=2000

# Strategy: temporal | genre | biographical | balanced
TIER0_EXPLORATION_STRATEGY=balanced

# Read full corpus (ignore sampling)
TIER0_FULL_CORPUS=0

# Maximum characters sent per batch to the LLM
TIER0_BATCH_MAX_CHARS=60000

# Max docs per sub-batch within a stratum (prevents timeouts)
TIER0_SUB_BATCH_DOCS=10

# LLM timeout for Tier 0 calls (seconds)
TIER0_LLM_TIMEOUT=300
TIER0_HEARTBEAT_SECONDS=60
TIER0_LLM_CACHE_ENABLED=1
TIER0_LLM_CACHE_DIR=/app/logs/llm_cache
TIER0_SYNTHESIS_CHECKPOINT_DIR=/app/logs/synthesis_checkpoints
TIER0_EXTRACT_DATES_STRICT=1
TIER0_GROUP_INDICATOR_MIN_DOCS=3
TIER0_QUESTION_MIN_EVIDENCE_DOCS=5
TIER0_ANSWERABILITY_MIN_DOCS=5
TIER0_ANSWERABILITY_MAX_DOCS=200
TIER0_ANSWERABILITY_TOP_K=50
TIER0_SYNTHESIS_ENABLED=1
TIER0_SYNTHESIS_DYNAMIC=1
TIER0_SYNTHESIS_SEMANTIC_ASSIGNMENT=1
TIER0_SYNTHESIS_EMBED_PROVIDER=ollama
TIER0_SYNTHESIS_EMBED_MODEL=qwen3-embedding:0.6b
TIER0_SYNTHESIS_EMBED_CACHE=/app/logs/embedding_cache.pkl
TIER0_SYNTHESIS_EMBED_TIMEOUT=120
TIER0_SYNTHESIS_ASSIGN_MIN_SIM=0.2
TIER0_SYNTHESIS_DEDUPE_THRESHOLD=0.86
TIER0_SYNTHESIS_THEME_MERGE_THRESHOLD=0.84
TIER0_SYNTHESIS_THEME_COUNT=5
TIER0_SYNTHESIS_MAX_QUESTION_SAMPLE=24
TIER0_SYNTHESIS_MAX_PATTERN_SAMPLE=12

# LLM profile for Tier 0 batch analysis (fast|quality)
TIER0_BATCH_PROFILE=fast

# Enable semantic chunking (Notebook-LLM style)
TIER0_SEMANTIC_CHUNKING=1

# Maximum characters per semantic block
TIER0_BLOCK_MAX_CHARS=2000

# Maximum blocks per document
TIER0_MAX_BLOCKS_PER_DOC=12

# Minimum findings required before repair attempt
TIER0_MIN_ENTITIES_PER_BATCH=5
TIER0_MIN_PATTERNS_PER_BATCH=2

# Repair attempts for evidence alignment
TIER0_REPAIR_ATTEMPTS=1

# Enforce closed-world evidence references
TIER0_STRICT_CLOSED_WORLD=1

# Notebook persistence
NOTEBOOK_SAVE_DIR=/app/logs/corpus_exploration
NOTEBOOK_AUTO_SAVE=1

# Tier 0 logging
TIER0_LOG_DIR=/app/logs/tier0
TIER0_DEBUG_MODE=0

# =============================================================================
# TIER 0 QUESTION GENERATION (TYPOLOGY + VALIDATION)
# =============================================================================

# How many questions to generate per type before validation
TIER0_QUESTION_PER_TYPE=4

# Validation thresholds
TIER0_QUESTION_MIN_SCORE=60
TIER0_QUESTION_MIN_SCORE_REFINE=50
TIER0_QUESTION_MAX_REFINEMENTS=2

# Final output targets
TIER0_QUESTION_TARGET_COUNT=12
TIER0_QUESTION_MIN_COUNT=8

# Diversity enforcement
TIER0_QUESTION_ENFORCE_TYPE_DIVERSITY=1
TIER0_QUESTION_MIN_TYPES=3

# =============================================================================
# PROFILE SUMMARY (For Reference)
# =============================================================================

# The system supports 4 LLM profiles configured in config.py:
#
# 1. "fast" profile:
#    - Provider: ollama
#    - Model: $LLM_FAST_MODEL (llama3.2:3b)
#    - Temperature: 0.3
#    - Use case: Multi-query generation, quick operations
#
# 2. "quality" profile:
#    - Provider: ollama
#    - Model: $LLM_MODEL (qwen2.5:32b)
#    - Temperature: 0.2
#    - Use case: Main answer generation
#
# 3. "verifier" profile:
#    - Provider: ollama
#    - Model: $VERIFIER_MODEL (qwen2.5:32b)
#    - Temperature: 0.0 (deterministic)
#    - Use case: Adversarial fact-checking
#
# 4. "cloud" profile:
#    - Provider: openai
#    - Model: $OPENAI_MODEL (gpt-4)
#    - Temperature: 0.2
#    - Use case: Fallback when Ollama is unavailable
#
# =============================================================================

# =============================================================================
# DOCKER-SPECIFIC (Usually set in docker-compose.yml)
# =============================================================================

# Flask host (usually 0.0.0.0 in Docker)
# FLASK_HOST=0.0.0.0

# Flask port
# FLASK_PORT=5000

# =============================================================================
# VALIDATION CHECKLIST
# =============================================================================

# Required variables (must be set):
# ✓ MONGO_URI or APP_MONGO_URI
# ✓ OLLAMA_BASE_URL (if using Ollama)
# ✓ LLM_MODEL
# ✓ VERIFIER_MODEL
# ✓ SECRET_KEY (change in production!)

# Optional variables (have sensible defaults):
# - All timeout values
# - All numeric parameters (weights, pool sizes, etc.)
# - Provider-specific options

# Cloud provider variables (only if using):
# - OPENAI_API_KEY (if using 'cloud' profile)
# - LMSTUDIO_BASE_URL (if using LM Studio)

# --- 2. OLLAMA & LLM MASTER SETTINGS ---
OLLAMA_BASE_URL=http://host.docker.internal:11434
HISTORIAN_AGENT_OLLAMA_BASE_URL=http://host.docker.internal:11434
LLM_MODEL=gpt-oss:20b 
HISTORIAN_AGENT_MODEL=gpt-oss:20b 
HISTORIAN_AGENT_MODEL_PROVIDER=ollama
LLM_TEMPERATURE=0.2
HISTORIAN_AGENT_TEMPERATURE=0.2
LLM_MAX_TOKENS=40000
MAX_CONTEXT_TOKENS=80000

# --- 3. RAG & RETRIEVAL PARAMETERS ---
HISTORIAN_AGENT_TOP_K=20
VECTOR_WEIGHT=0.7
KEYWORD_WEIGHT=0.3
HISTORIAN_AGENT_HYBRID_ALPHA=0.5
HISTORIAN_AGENT_EMBEDDING_PROVIDER=ollama
HISTORIAN_AGENT_EMBEDDING_MODEL=qwen3-embedding:0.6b
HISTORIAN_AGENT_VECTOR_STORE=chroma
HISTORIAN_AGENT_USE_VECTOR_RETRIEVAL=true
CHROMA_PERSIST_DIRECTORY=/data/chroma_db/persist
RETRIEVAL_POOL_SIZE=100 # Number of documents to retrieve in the first step

# --- 4. RERANKING & ADVERSARIAL PIPELINE ---
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
CROSS_ENCODER_WEIGHT=0.85
TEMPORAL_WEIGHT=0.10
ENTITY_WEIGHT=0.05
FINAL_TOP_K=10
CRITIQUE_TEMPERATURE=0.1
USE_RERANKING=1
CONFIDENCE_THRESHOLD=0.85
VERIFIER_MODEL=qwen2.5:32b  # More reliable for JSON

# --- 5. FLASK & SYSTEM CONFIGURATION ---
SECRET_KEY=super-secret-key
FLASK_ENV=development
FLASK_DEBUG=1
HISTORIAN_AGENT_ENABLED=1
HISTORIAN_AGENT_CONTEXT_K=4
HISTORIAN_AGENT_CONTEXT_FIELDS=title,content
HISTORIAN_AGENT_SUMMARY_FIELD=content
HISTORIAN_AGENT_FALLBACK=1
HISTORIAN_AGENT_CHUNK_SIZE=1000
HISTORIAN_AGENT_CHUNK_OVERLAP=200
RUN_BOOTSTRAP=0
OPENAI_API_KEY=
DEBUG_MODE=1

# How many FULL documents to retrieve in Tier 2 expansion (Small-to-Big)
PARENT_RETRIEVAL_CAP=15

# --- 6. SYSTEM PATHS (CONTAINER & HOST) ---
ARCHIVES_PATH=/data/archives/
ARCHIVES_HOST_PATH=../archives/borr_data/
MONGO_DATA_PATH=/data/db
MONGO_DATA_HOST_PATH=../mongo_data
SESSION_PATH=/app/flask_session
SESSION_HOST_PATH=../flask_session
CHROMA_HOST_PATH=../chroma_db
