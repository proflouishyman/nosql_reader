# Example Environment Configuration for Image Ingestion CLI
# Copy this file to .env and customize for your setup

# ============================================================
# PROVIDER CONFIGURATION
# ============================================================

# Default AI provider: "ollama" or "openai"
HISTORIAN_AGENT_MODEL_PROVIDER=ollama

# Default model name (used by both providers if not specified)
HISTORIAN_AGENT_MODEL=llama3.2-vision:latest

# ============================================================
# OLLAMA CONFIGURATION
# ============================================================

# Ollama server URL
OLLAMA_BASE_URL=http://localhost:11434

# Alternative: use for multi-host setups
# HISTORIAN_AGENT_OLLAMA_BASE_URL=http://gpu-server-01:11434

# Fallback Ollama model (if HISTORIAN_AGENT_MODEL not set)
OLLAMA_DEFAULT_MODEL=llama3.2-vision:latest

# ============================================================
# OPENAI CONFIGURATION
# ============================================================

# OpenAI API key (or use --api-key flag)
# OPENAI_API_KEY=sk-...

# Path to file containing API key (more secure than env var)
# OPENAI_API_KEY_FILE=~/.config/nosql_reader/openai_api_key.txt

# Default OpenAI model
OPENAI_DEFAULT_MODEL=gpt-4o-mini

# ============================================================
# CUSTOM PROMPT (OPTIONAL)
# ============================================================

# Path to custom prompt file (or use --prompt-file flag)
# HISTORIAN_AGENT_PROMPT_FILE=./prompts/my_custom_prompt.txt

# Or set prompt directly (multiline):
# HISTORIAN_AGENT_PROMPT='Extract and structure document data as JSON...'

# ============================================================
# MONGODB CONFIGURATION (if using database)
# ============================================================

# MongoDB connection string
MONGODB_URI=mongodb://localhost:27017/

# Database name
MONGODB_DB_NAME=document_archive

# ============================================================
# LOGGING
# ============================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (optional)
# LOG_FILE=logs/ingestion.log

# ============================================================
# PROCESSING OPTIONS (Defaults)
# ============================================================

# Maximum image dimension (pixels) for preprocessing
# MAX_IMAGE_SIDE=1600

# JPEG quality for preprocessing (1-100)
# JPEG_QUALITY=85

# Minimum OCR text length to consider valid (characters)
# MIN_OCR_TEXT_LENGTH=50

# Context window size for Ollama stage 2 (tokens)
# OLLAMA_STAGE2_CONTEXT_SIZE=8192

# ============================================================
# HPC CONFIGURATION
# ============================================================

# For SLURM scripts - base directory for batch processing
# HPC_BASE_DIR=/scratch/username/archives

# For SLURM scripts - Conda environment name
# HPC_CONDA_ENV=nosql_reader

# For distributed Ollama setup
# HPC_OLLAMA_URL=http://gpu-node-01:11434

# ============================================================
# USAGE NOTES
# ============================================================
#
# Load this file with:
#   export $(cat .env | grep -v '^#' | xargs)
#
# Or use in Python:
#   from dotenv import load_dotenv
#   load_dotenv()
#
# Or source in bash:
#   source .env
#
# Command-line flags override these environment variables.
